file_name,comments,num_comm
1-nearest-nbor-naive-classifier-for-home-credit.py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Run classifier.  Since outputs are 0 and 1, just test counts', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",22
15-lines-plb-0-712-ext-source-x-lgbm.py,"['# coding: utf-8', '# # A quick and simple GB model optimisation on EXT\\_SOURCE\\_\\* variables', '# This kernel has started from the simple and clear [15 lines: Just EXT_SOURCE_x](https://www.kaggle.com/lemonkoala/15-lines-just-ext-source-x) by [Lem Lordje Ko](https://www.kaggle.com/lemonkoala). Goal goal is to see what performance can one reach in short piece of code. What has been added on top on the original kernel is optimisation of LightGBM hyper-parameters. The final reported precision is 0.723 locally and 0.712 on the public leaderboard', '# In[ ]:', '# Define parameter range in which optimisation will be performed.', '# In[ ]:', '# Define the hyper-parameter optimiser, it will test `n_HP_points_to_test` points sampled randomly. Beware: 3x20 (`CV_folds x n_HP_points_to_test`)  will run for approx 3 min on 4 CPU cores on kaggle', '# In[ ]:', '# Do actual parameter tune', '# In[ ]:', ""# Let's print the 5 best parameter sets based on the average roc auc on the testing fold in CV"", '# In[ ]:', '# Prepare a submission (note that you can directly submit it from the `Output` tab of the kernel, when you fork it)', '# In[ ]:', '# In[ ]:']",15
2019-06-05.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", '# sklearn preprocessing for dealing with categorical variables', '# File system manangement', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# Any results you write to the current directory are saved as output.', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[24]:', '# In[25]:', '# In[26]:', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# In[44]:', '# In[45]:', '# In[46]:', '# In[47]:', '# In[48]:', '# In[49]:', '# In[50]:', '# In[51]:']",60
792_lb_lgbm_with_smile_feats.py,"['# HOME CREDIT DEFAULT RISK COMPETITION', '# Most features are created by applying min, max, mean, sum and var functions to grouped tables. ', '# Little feature selection is done and overfitting might be a problem since many features are related.', '# The following key ideas were used:', '# - Divide or subtract important features to get rates (like annuity and income)', '# - In Bureau Data: create specific features for Active credits and Closed credits', '# - In Previous Applications: create specific features for Approved and Refused applications', '# - Modularity: one function for each table (except bureau_balance and application_test)', '# - One-hot encoding for categorical features', '# All tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).', '# You can use LightGBM with KFold or Stratified KFold. Please upvote if you find usefull, thanks!', '# Update 16/06/2018:', '# - Added Payment Rate feature', '# - Removed index from features', '# - Set early stopping to 200 rounds', '# - Use standard KFold CV (not stratified)', '# Public LB increased to 0.792', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Some simple new features (percentages)', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",57
793_new_app_feats.py,"['# HOME CREDIT DEFAULT RISK COMPETITION', '# Most features are created by applying min, max, mean, sum and var functions to grouped tables. ', '# Little feature selection is done and overfitting might be a problem since many features are related.', '# The following key ideas were used:', '# - Divide or subtract important features to get rates (like annuity and income)', '# - In Bureau Data: create specific features for Active credits and Closed credits', '# - In Previous Applications: create specific features for Approved and Refused applications', '# - Modularity: one function for each table (except bureau_balance and application_test)', '# - One-hot encoding for categorical features', '# All tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).', '# You can use LightGBM with KFold or Stratified KFold. Please upvote if you find usefull, thanks!', '# Update 16/06/2018:', '# - Added Payment Rate feature', '# - Removed index from features', '# - Set early stopping to 200 rounds', '# - Use standard KFold CV (not stratified)', '# Public LB increased to 0.792', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Some simple new features (percentages)', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",57
a2-home-credit-default-risk.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:']",3
addnewfeature_lightgbm_with_simple_features.py,"['# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features', '# From Kaggler : https://www.kaggle.com/jsaguiar', '# Just removed a few min, max features. U can see the CV is not good. Dont believe in LB.', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # add features', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",43
alim-copy-eda-basic-fe-and-lgb.py,"['# coding: utf-8', '# # Home Credit Default Risk', '# ', '# This kernel will contain EDA, visualization, feature engineering and some modelling. Work currently in progress.', '# In[ ]:', ""# There are several files with data, let's go through them step by step."", '# In[ ]:', '# ## Data Exploration', '# ### application_train and application_test', '# These are main files with data and technically we can use only them to make predictions. Obviously using additional data is necessary to improve score.', '# In[ ]:', ""# We have 122 columns in just main file! Let's take a look on some of them."", '# #### Categorical features', '# ##### Target', '# In[ ]:', ""# We have disbalanced target, though disbalance isn't really serious."", '# ##### NAME_CONTRACT_TYPE', '# In[ ]:', '# We can see that there are two types of contract - cash loans and revolving loans. Most of the loans are cash loans which are defaulted.', '# ##### CODE_GENDER', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# We can see that women take more loans and higher percentage of them repays the loans. And there are 4 people with unindentified gender, who repayed their loans :)', '# ##### FLAG_OWN_CAR and FLAG_OWN_REALTY', '# In[ ]:', '# ##### CNT_CHILDREN and NAME_FAMILY_STATUS', '# In[ ]:', '# We can see that most of the people are married and have zero children. In face we can divide people into two group based on their family status - living together with their partner or single.', '# In[ ]:', ""# It isn't surprising that there are a lot of families consisting of two or one adults. Also there are families with two adults and 1-2 children."", '# ##### NAME_TYPE_SUITE', '# This feature shows who was accompanying client when he was applying for the loan.', '# In[ ]:', '# In[ ]:', '# It is interesting to see that these two variables sometimes contradict each other. For example, separated, single or widowed applicants were sometimes accompanied by their partner. I suppose this means unofficial relationships? Also sometimes children accompanied the applicant. Maybe these were adult childred?', '# ##### NAME_INCOME_TYPE', '# In[ ]:', '# In[ ]:', '# We can see that there are 4 categories with little amount of people in them: several high-income businessmen, 4 women and 1 man on maternity leave, and some unemployed/students. It is quite interesting that unemployed/students have quite a high income.', '# And of course, most of the people work.', '# In[ ]:', '# ##### AMT_GOODS_PRICE', '# For consumer loans it is the price of the goods for which the loan is given', '# In[ ]:', ""# So this means that only 278 loans have some other type. Let's go deeper."", '# In[ ]:', '# We can see that most of the loans have the amount which is similar to the goods price, but there are some outliers.', '# ##### NAME_HOUSING_TYPE', '# In[ ]:', '# ##### Contact information', ""# There are 6 features showing that client provided some contact information, let's see how many ways of contact clients usually provide."", '# In[ ]:', ""# Most clients provide 3 ways to contact them and usually minimus is 2, if we don't consider several people who left only 1."", '# # deliquencies', '# ', ""# It is very important to see how many times clients was late with payments or defaulted his loans. I suppose info about his social circle is also important. I'll divide values into 2 groups: 0, 1 and more than 1."", '# In[ ]:', '# In[ ]:', '# #### Continuous variables', '# ##### AMT_INCOME_TOTAL', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# We can see following things from the information above:', '# - income feature has some huge outliers. This could be due to rich individuals or due to errors in data;', ""# - average income is almost similar for those who repay the loans and those who don't;"", '# - if we leave only data within 90 percentile, it is almost normally distributed;', '# - log transformation also helps;', '# ##### AMT_CREDIT', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# This feature shows the amount of the loan in question.', '# We can see following things from the information above:', '# - income feature has some outliers. Maybe mortgage?;', ""# - average credit amoint is almost similar for those who repay the loans and those who don't;"", '# - if we leave only data within 95 percentile, it is almost normally distributed;', '# - log transformation also helps;', '# ##### DAYS_BIRTH', '# In[ ]:', '# We can see that age distribution is almost normal and most of the people are between 30 and 40 years.', '# ##### DAYS_EMPLOYED', '# In[ ]:', '# Ther was a strange value - 365243, it could mean empty values or some errors, so I replace it with zero.', ""# A lot of people don't work, but let's look deeper into this."", '# In[ ]:', '# Well, it seems that a lot of non-working people are pensioners, which is normal. As for working people - they seem to work for several years at one place.', ""# Ther are so many features and so many possible angles from which we can analyze them. Let's see this for example:"", '# In[ ]:', '# We can see that most of the loans are taken by working people with secondary education.', '# ## Transforming and merging data', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Basic modelling, LGB', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# This was EDA and basic feature engineering. I know that feature engineering and modelling could be much better, but decided to make EDA the main focus of this kernel. I'll do better feature engineering and modelling in the next one."", '# In[ ]:']",115
app-train-feat.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[2]:', '# In[3]:', '# In[4]:', '# Check missing value', '# Process features ', '    # Combine these (average)', '    # Transform to years', '    # round counting features', '    # extract columns that have more than 2 unique values and the maximum large than 1 and normalized', '# # Split training set and test set', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[24]:', '# In[ ]:', '# CV for logistic regression', '# In[ ]:', '# CV for XGB', '# In[10]:', '    # Hyperparam', '    LR = 0.0001 # learning rate', '    # build graph', '    nn_inputs = tf.layers.dense(x, units = round(2*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 2', '    nn_inputs = tf.layers.dense(x, units = round(1*NUM_FEAT), kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 3', '    nn_inputs = tf.layers.dense(x, units = 0.5*NUM_FEAT, kernel_initializer = tf.truncated_normal_initializer(),activation = tf.nn.sigmoid) # hidden layer 4', '    # session', '# In[11]:', '# In[12]:', '# In[ ]:']",41
automated-model-tuning.py,"['# coding: utf-8', '# # Introduction: Automated Hyperparameter Tuning', '# ', '# In this notebook, we will talk through a complete example of using automated hyperparameter tuning to optimize a machine learning model. In particular, we will use Bayesian Optimization and the Hyperopt library to tune the hyperparameters of a gradient boosting machine. ', '# ', '# __Additional Notebooks__ ', '# ', ""# If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:"", '# ', '# * [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)', '# * [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)', '# * [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)', '# * [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)', '# * [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)', '# * [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)', '# * [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)', '# * [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)', '# ', '# There are four approaches to tuning the hyperparameters of a machine learning model', '# ', '# 1. __Manual__: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.', '# 2. __Grid Search__: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!', '# 3. __Random search__: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.', '# 4. __Automated Hyperparameter Tuning__: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.', '# ', ""# These are listed in general order of least to most efficient. While we already conquered 2 and 3 [in this notebook](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search) (we didn't even try method 1), we have yet to take on automated hyperparameter tuning. There are a number of methods to do this including genetic programming, Bayesian optimization, and gradient based methods. Here we will focus only on Bayesian optimization, using the Tree Parzen Esimator (don't worry, you don't need to understand this in detail) in the [Hyperopt open-source Python library](https://hyperopt.github.io/hyperopt/)."", '# ', ""# For a little more background (we'll cover everything you need below), [here is an introductory article](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0) on Bayesian optimization, and [here is an article on automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a) using Bayesian optimization. Here we'll get right into automated hyperparameter tuning, so for the necessary background on model tuning, refer to [this kernel](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)"", '# ', '# ## Bayesian Optimization Primer', '# ', '# The problem with grid and random search is that these are __uninformed methods__ because they do not use the past results from different values of hyperparameters in the objective function (remember the objective function takes in the hyperparameters and returns the model cross validation score). We record the results of the objective function for each set of hyperparameters, but the algorithms do not select the next hyperparameter values from this information. Intuitively, if we have the past results, we should  use them to reason about what hyperparameter values work the best and choose the next values wisely to try and spend more iterations evaluating promising values. Evaluating hyperparameters in the objective function is very time-consuming, and the __concept of Bayesian optimization is to limit calls to the evaluation function by choosing the next hyperparameter values based on the previous results.__ This allows the algorithm to spend __more time evaluating promising hyperparameter values and less time in low-scoring regions of the hyperparameter space__. For example, consider the image below:', '# ', '# ![](https://raw.githubusercontent.com/WillKoehrsen/hyperparameter-optimization/master/images/random_forest_hypothetical.png)', '# ', '# If you were choosing the next number of trees to try for the random forest, where would you concentrate your search? Probably around 100 trees because that is where the lowest errors have tended to occur (imagine this is a problem where we want to minimize the error). In effect, you have just done Bayesian hyperparameter optimization in your head! You formed a probability model of the error as a function of the hyperparameters and then selected the next hyperparameter values by maximizing the probability of a low error. Bayesian optimization works by building a surrogate function (in the form of a probability model) of the objective function $P(\\text{score} | \\text{hyperparameters}$. The surrogate function is much cheaper to evaluate than the objective, so the algorithm chooses the next values to try in the objective based on maximizing a criterion on the surrogate (usually expected improvement), exactly what you would have done with respect to the image above. ', '# ', ""# The surrogate function is based on past evaluation results - pairs of (score, hyperparameter) records - and is continually updated with each objective function evaluation. Bayesian optimization therefore uses Bayesian reasoning: form an initial model (called a prior) and then update it with more evidence. The idea is that as the data accumulates, the surrogate function gets closer and closer to the objective function, and the hyperparameter values that are the best in the surrogate function will also do the best in the objective function. Bayesian optimization methods differ in the algorithm used to build the surrogate function and choose the next hyperparameter values to try. Some of the common choices are Gaussian Process (implemented in Spearmint), Random Forest Regression (in SMAC), and the Tree Parzen Estimator (TPE) in Hyperopt (technical details can be found in this article, although they won't be necessary to use the methods)."", '# ', '# ### Four Part of Bayesian Optimization', '# ', '# Bayesian hyperparameter optimization requires the same four parts as we implemented in grid and random search:', '# ', '# 1. __Objective Function__: takes in an input (hyperparameters) and returns a score to minimize or maximize (the cross validation score)', '# 2. __Domain space__: the range of input values (hyperparameters) to evaluate', '# 3. __Optimization Algorithm__: the method used to construct the surrogate function and choose the next values to evaluate', '# 4. __Results__: score, value pairs that the algorithm uses to build the surrogate function', '# ', '# The only differences are that now our objective function will return a score to minimize (this is just convention in the field of optimization), our domain space will be probability distributions rather than a hyperparameter grid, and the optimization algorithm will be an __informed method__ that uses past results to choose the next hyperparameter values to evaluate. ', '# ', '# ## Hyperopt', '# ', '# Hyperopt is an open-source Python library the implements Bayesian Optimization using the Tree Parzen Estimator algorithm to construct the surrogate function and select the next hyperparameter values to evaluate in the objective function. There are a number of other libraries such as Spearmint (Guassian process surrogate function) and SMAC (random forest regression surrogate function) sharing the same problem structure. The four parts of an optimization problem that we develop here will apply to all the libraries with only a change in syntax. Morevoer, the optimization methods as applied to the Gradient Boosting Machine will translate to other machine learning models or any problem where we have to minimize a function.', '# ', '# ### Gradient Boosting Machine', '# ', '# We will use the gradient booosting machine (GBM) as our model to tune in the LightGBM library. The GBM is our choice of model because it performs extremely well for these types of problems (as shown on the leaderboard) and because the performance is heavily dependent on the choice of hyperparameter values. ', '# ', '# For more details of the Gradient Boosting Machine (GBM), check out this high-level blog post, or this in depth technical article.', '# ', '# ### Cross Validation with Early Stopping', '# ', '# As with random and grid search, we will evaluate each set of hyperparameters using 5 fold cross validation on the training data. The GBM model will be trained with early stopping, where estimators are added to the ensemble until the validation score has not decrease for 100 iterations (estimators added). ', '# ', '# Cross validation and early stopping will be implemented using the LightGBM `cv` function. We will use 5 folds and 100 early stopping rounds. ', '# ', '# #### Dataset and Approach', '# ', ""# As before, we will work with a limited section of the data - 10000 observations for training and 6000 observations for testing. This will allow the optimization within the notebook to finish in a reasonable amount of time. Later in the notebook, I'll present results from 1000 iterations of Bayesian hyperparameter optimization on the reduced dataset and we then will see if these results translate to a full dataset (from [this kernel](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features)). The functions developed here can be taken and run on any dataset, or used with any machine learning model (just with minor changes in the details) and working with a smaller dataset will allow us to learn all of the concepts. I am currently running 500 iterations of Bayesian hyperparameter optimization on a complete dataset and will make the results available when the search is completed. "", ""# With the background details out of the way, let's get started with Bayesian optimization applied to automated hyperparameter tuning! "", '# In[ ]:', '# Data manipulation', '# Modeling', '# Evaluation of the model', '# Visualization', '# Governing choices for search', '# The code below reads in the data and creates a smaller version for training and a set for testing. We can only use the training data __a single time__ when we evaluate the final model. Hyperparameter tuning must be done on the training data using cross validation!', '# In[ ]:', '# Sample 16000 rows (10000 for training, 6000 for testing)', '# Only numeric features', '# Extract the labels', '# Split into training and testing data', '# ### Baseline Model ', '# ', '# First we can create a model with the default value of hyperparameters and score it using cross validation with early stopping. Using the `cv` LightGBM function requires creating a `Dataset`.', '# In[ ]:', '# Training set', '# In[ ]:', '# Default hyperparamters', '# Using early stopping to determine number of estimators.', '# Perform cross validation with early stopping', '# Highest score', '# Standard deviation of best score', '# Now we can evaluate the baseline model on the testing data.', '# In[ ]:', '# Optimal number of esimators found in cv', '# Train and make predicions with model', '# # Objective Function', '# ', '# The first part to write is the objective function which takes in a set of hyperparameter values and returns the cross validation score on the training data. An objective function in Hyperopt must return either a single real value to minimize, or a dictionary with a key ""loss"" with the score to minimize (and a key ""status"" indicating if the run was successful or not). ', '# ', '# Optimization is typically about minimizing a value, and because our metric is Receiver Operating Characteristic Area Under the Curve (ROC AUC) where higher is better, the objective function will return $1 - \\text{ROC AUC Cross Validation}$. The algorithm will try to drive this value as low as possible (raising the ROC AUC) by choosing the next hyperparameters based on the past results. ', '# ', '# The complete objective function is shown below. As with random and grid search, we write to a `csv` file on each call of the function in order to track results as the search progress and so we have a saved record of the search. (The `subsample` and `boosting_type` logic will be explained when we get to the domain). ', '# In[ ]:', '    # Keep track of evals', '    # Using early stopping to find number of trees trained', '    # Retrieve the subsample', '    # Extract the boosting type and subsample to top level keys', '    # Make sure parameters that need to be integers are integers', '    # Perform n_folds cross validation', '    # Extract the best score', '    # Loss must be minimized', '    # Boosting rounds that returned the highest cv score', '    # Add the number of estimators to the hyperparameters', ""    # Write to the csv file ('a' means append)"", '    # Dictionary with information for evaluation', '# # Domain ', '# ', '# Specifying the domain (called the space in Hyperopt) is a little trickier than in grid search. In Hyperopt, and other Bayesian optimization frameworks, the domian is not a discrete grid but instead has probability distributions for each hyperparameter. For each hyperparameter, we will use the same limits as with the grid, but instead of being defined at each point, the domain represents probabilities for each hyperparameter. This will probably become clearer in the code and the images!', '# In[ ]:', '# First we will go through an example of the learning rate. We are using a log-uniform space for the learning rate defined from 0.005 to 0.5. The log - uniform distribution has the values evenly placed in logarithmic space rather than linear space. This is useful for variables that differ over several orders of magnitude such as the learning rate. For example, with a log-uniform distribution, there will be an equal chance of drawing a value from 0.005 to 0.05 and from 0.05 to 0.5 (in linear space far more values would be drawn from the later since the linear distance is much larger. The logarithmic space is exactly the same - a factor of 10).', '# In[ ]:', '# Create the learning rate', '# We can visualize the learning rate by drawing 10000 samples from the distribution.', '# In[ ]:', '# Draw 10000 samples from the learning rate domain', '# The number of leaves on the other hand is a discrete uniform distribution.', '# In[ ]:', '# Discrete uniform distribution', '# Sample 10000 times from the number of leaves distribution', '# kdeplot', '# ### Conditional Domain', '# ', '# In Hyperopt, we can use nested conditional statements to indicate hyperparameters that depend on other hyperparameters. For example, the ""goss"" `boosting_type` cannot use subsampling, so when we set up the `boosting_type` categorical variable, we have to set the subsample to 1.0 while for the other boosting types it\'s a float between 0.5 and 1.0.', '# In[ ]:', '# boosting type domain ', '# Draw a sample', '# We need to set both the boosting_type and subsample as top-level keys in the parameter dictionary. We can use the Python dict.get method with a default value of 1.0. This means that if the key is not present in the dictionary, the value returned will be the default (1.0).', '# In[ ]:', '# Retrieve the subsample if present otherwise set to 1.0', '# Extract the boosting type', '# The gbm cannot use the nested dictionary so we need to set the `boosting_type` and `subsample` as top level keys. ', '# ', '# Nested conditionals allow us to use a different set of hyperparameters depending on other hyperparameters. For example, we can explore different models with completely different sets of hyperparameters by using nested conditionals. The only requirement is that the first nested statement must be based on a choice hyperparameter (the choice could be the type of model).', '# ## Complete Bayesian Domain', '# ', '# Now we can define the entire domain. Each variable needs to have a label and a few parameters specifying the type and extent of the distribution. For the variables such as boosting type that are categorical, we use the choice variable. Other variables types include quniform, loguniform, and uniform. For the complete list, check out the documentation for Hyperopt. Altogether there are 10 hyperparameters to optimize. ', '# In[ ]:', '# Define the search space', '# ### Example of Sampling from the Domain', '# ', ""# Let's sample from the domain (using the conditional logic) to see the result of each draw. Every time we run this code, the results will change. (Again notice that we need to assign the top level keys to the keywords understood by the GBM)."", '# In[ ]:', '# Sample from the full space', '# Conditional logic to assign top-level keys', '# In[ ]:', ""# Let's test the objective function with the domain to make sure it works. (Every time the `of_connection` line is run, the `outfile` will be overwritten, so use a different name for each trial to save the results.)"", '# In[ ]:', '# Create a new file and open a connection', '# Write column names', '# Test the objective function', '# # Optimization Algorithm', '# ', '# The optimization algorithm is the method for constructing the surrogate function (probability model) and selecting the next set of hyperparameters to evaluate in the objective function. Hyperopt has two choices: random search and Tree Parzen Estimator. ', '# ', '# The technical details of TPE can be found in [this article](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) and a conceptual explanation is in [this article](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f). Although this is the most technical part of Bayesian hyperparameter optimization, defining the algorithm in Hyperopt is simple. ', '# In[ ]:', '# Create the algorithm', '# # Results History', '# The final part is the history of objective function evaluations. Although Hyperopt internally keeps track of the results for the algorithm to use, if we want to monitor the results and have a saved copy of the search, we need to store the results ourselves. Here, we are using two methods to make sure we capture all the results:', '# ', '# 1. A `Trials` object that stores the dictionary returned from the objective function', '# 2. Adding a line to a csv file every iteration.', '# ', '# The csv file option also lets us monitor the results of an on-going experiment. Although do not use Excel to open the file while training is on-going. Instead check the results using `tail results/out_file.csv` from bash or open the file in Sublime Text or Notepad.', '# In[ ]:', '# Record results', '# The `Trials` object will hold everything returned from the objective function in the `.results` attribute. We can use this after the search is complete to inspect the results, but an easier method is to read in the `csv` file because that will already be in a dataframe.', '# In[ ]:', '# Create a file and open a connection', '# Write column names', '# # Automated Hyperparameter Optimization in Practice', '# ', '# We have all four parts we need to run the optimization. To run Bayesian optimization we use the `fmin` function (a good reminder that we need a metric to minimize!) ', '# In[ ]:', '# `fmin` takes the four parts defined above as well as the maximum number of iterations `max_evals`. ', '# In[ ]:', '# Global variable', '# Run optimization', '# The `best` object holds only the hyperparameters that returned the lowest loss in the objective function. Although this is ultimately what we are after, if we want to understand how the search progresses, we need to inspect the `Trials` object or the `csv` file. For example, we can sort the `results` returned from the objective function by the lowest loss:', '# In[ ]:', '# Sort the trials with lowest loss (highest AUC) first', '# An easier method is to read in the csv file since this will be a dataframe. ', '# In[ ]:', '# The function below takes in the results, trains a model on the training data, and evalutes on the testing data. It returns a dataframe of hyperparameters from the search. ', '# ', '# Saving the results to a csv file converts the dictionary of hyperparameters to a string. We need to map this back to a dictionary using `ast.literal_eval`. ', '# In[ ]:', '    # String to dictionary', '    # Sort with best values on top', '    # Print out cross validation high score', '    # Use best hyperparameters to create a model', '    # Train and make predictions', '    # Create dataframe of hyperparameters', '    # Iterate through each set of hyperparameters that were evaluated', '    # Put the iteration and score in the hyperparameter dataframe', '# In[ ]:', '# ## Continue Optimization', '# ', '# Hyperopt can continue searching where a previous search left off if we pass in a `Trials` object that already has results. The algorithms used in Bayesian optimization are black-box optimizers because they have no internal state. All they need is the previous results of objective function evaluations (the input values and loss) and they can build up the surrogate function and select the next values to evaluate in the objective function. This means that any search can be continued as long as we have the history in a `Trials` object. ', '# In[ ]:', '# Continue training', '# To save the `Trials` object so it can be read in later for more training, we can use the `json` format. ', '# In[ ]:', '# Save the trial results', '# To start the training from where it left off, simply load in the `Trials` object and pass it to an instance of `fmin`. (You might even be able to tweak the hyperparameter distribution and continue searching with the `Trials` object because the algorithm does not maintain an internal state. Someone should check this and let me know in the comments!).', '# ## Next Steps', '# ', '# Now that we have developed all the necessary parts for automated hyperparameter tuning using Bayesian optimization, we can apply these to any dataset or any machine learning method. The functions taken here can be put in a script and run a full dataset. Next, we will go through results from 1000 evaluations on a reduced size dataset to see how the search progresses. We can then compare these results to random search to see how a method that uses __reasoning__ about past results differs from a method that does not. ', '# ', ""# After examining the tuning results from the reduced dataset, we will take the best performing hyperparameters and see if these translate to a full dataset, the features from the `[Updated 0.792 LB] LightGBM with Simple Features`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel (I did not develop these features and want to give credit to the numerous people, including [Aguiar](https://www.kaggle.com/jsaguiar) and [olivier](https://www.kaggle.com/ogrellier),  who have worked on these features. Please check out their [kernels](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features)!). We saw in the random and grid search notebook that the best hyperparameter values from the small datasets do not necessarily perform well on the full datasets. I am currently running the Bayesian Hyperparameter optimization for 500 iterations on the features referenced above and will make the results publicly available when the search is finished. For now, we will turn to the 1000 trials from the smaller dataset. These results can be generated by running the cell below, but I can't guarantee if this will finish within the kernel time limit! "", '# In[ ]:', '# MAX_EVALS = 1000', '# # Create a new file and open a connection', ""# OUT_FILE = 'bayesian_trials_1000.csv'"", ""# of_connection = open(OUT_FILE, 'w')"", '# writer = csv.writer(of_connection)', '# # Write column names', ""# headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']"", '# writer.writerow(headers)', '# of_connection.close()', '# # Record results', '# trials = Trials()', '# global ITERATION', '# ITERATION = 0 ', '# best = fmin(fn = objective, space = space, algo = tpe.suggest,', '#             trials = trials, max_evals = MAX_EVALS)', '# # Sort the trials with lowest loss (highest AUC) first', ""# trials_dict = sorted(trials.results, key = lambda x: x['loss'])"", ""# print('Finished, best results')"", '# print(trials_dict[:1])', '# # Save the trial results', ""# with open('trials.json', 'w') as f:"", '#     f.write(json.dumps(trials_dict))', '# # Search Results ', '# ', '# Next we will go through the results from 1000 search iterations on the reduced dataset. We will look at the scores, the distribution of hyperparameter values tried, the evolution of values over time, and compare the hyperparameters values to those from random search.', '# ', '# After examining the search results, we will use the optimized hyperparameters (at least optimized for the smaller dataset) to make predictions on a full dataset. These can then be submitted to the competition to see how well the methods do on a small sample of the data.', '# ## Learning Rate Distribution ', '# In[ ]:', '# We can see that the Bayesian search did worse in cross validation but then found hyperparameter values that did better on the test set! We will have to see if these results translate to the acutal competition data. First though, we can get all the scores in a dataframe in order to plot them over the course of training.', '# In[ ]:', '# Dataframe of just scores', '# We can also find the best scores for plotting the best hyperparameter values.', '# In[ ]:', '# Below is the code showing the progress of scores versus the iteration. For random search we do not expect to see a pattern, but for Bayesian optimization, we expect to see the scores increasing with the search as more promising hyperparameter values are tried.', '# In[ ]:', '# Plot of scores over the course of searching', '# Sure enough, we see that the Bayesian hyperparameter optimization scores increase as the search continues. This shows that more promising values (at least on the cross validation reduced dataset) were tried as the search progressed. Random search does record a better score, but the results do not improve over the course of the search. In this case, it looks like if we were to continue searching with Bayesian optimization, we would eventually reach higher scores on the cross vadidation data. ', '# ', '# For fun, we can make the same plot in Altair.', '# In[ ]:', '# Same chart, just in a different library for practice! ', '# ', '# ## Learning Rate Distribution', '# ', '# Next we can start plotting the distributions of hyperparameter values searched. We expect random search to align with the search domain, while the Bayesian hyperparameter optimization should tend to focus on more promising values, wherever those happen to be in the search domain.', '# ', '# The dashed vertical lines indicate the ""optimal"" value of the hyperparameter.', '# In[ ]:', '# Density plots of the learning rate distributions ', '# ## Distribution of all Numeric Hyperparameters', '# ', '# We can make the same chart now for all of the hyperparameters. For each setting, we plot the values tried by random search and bayesian optimization, as well as the sampling distirbution.', '# In[ ]:', '# Iterate through each hyperparameter', '        # Plot the random search distribution and the bayes search distribution', '# ## Evolution of Search', '# ', '# An interesting series of plots to make is the evolution of the hyperparameters over the search. This can show us what values the Bayesian optimization tended to focus on. The average cross validation score continued to improve throughout Bayesian optimization, indicating that ""more promising"" values of the hyperparameters were being evaluated and maybe a longer search would prove useful (or there could be a plateau in the validation scores with a longer search).', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '# In[ ]:', '# Scatterplot of next three hyperparameters', '# The final plot is just a bar chart of the `boosting_type`. ', '# In[ ]:', '# Bar plots of boosting type', '# The Bayes optimization spent many more iterations using the `dart` boosting type than would be expected from a uniform distribution. We can use information such as this in further hyperparameter tuning. For example, we could use the distributions from Bayesian hyperparameter optimization to make a more focused hyperparameter grid for grid or even random search. ', '# ', '# ![](http://)For this chart, we can also make it in Altair for the practice.', '# In[ ]:', '# In[ ]:', '# ## Applied to Full Dataset', '# ', '# Now, we can take the best hyperparameters found from 1000 iterations of Bayesian hyperparameter optimization on the smaller dataset and apply these to a full dataset of features from the `[Updated 0.792 LB] LightGBM with Simple Features`(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel. The best hyperparameters from the smaller dataset will not necessarily be the best on the full dataset (because the small dataset does nto perfectly represent the entire data), but we can at least try them out. We will train a model using the optimal hyperparameters from Bayesian optimization using early stopping to determine the number of estimators. ', '# In[ ]:', '# Read in full dataset', '# Extract the test ids and train labels', '# In[ ]:', '# ### Random Search on the Full Dataset', '# In[ ]:', '# Cross validation with n_folds and early stopping', '# Then we can make predictions on the test data. The predictions are saved to a csv file that can be submitted to the competition.', '# In[ ]:', '# Submitting these to the competition results in a score of __0.787__ which compares to the original score from the kernel of __0.792__', '# ### Bayesian Optimization on the Full Dataset', '# In[ ]:', '# Cross validation with n_folds and early stopping', '# In[ ]:', ""# Submitting these to the competition results in a score of __0.792__. So the Bayesian optimization outperforms the random search based on 1000 iterations on a reduced sized dataset. I wouldn't put too much weight into these results because we saw that random search actually yielded a higher cross validation score. Nonetheless, Bayesian hyperparameter optimization can be an effective technique for automated machine learning model tuning."", '# # Conclusions', '# ', '# Bayesian optimization is one method for automated hyperparameter tuning. Automated hyperparameter tuning aims to find the best hyperparameter values for a machine learning model on a given dataset with no input from the data scientist beyond initial set-up required. Bayesian optimization uses Bayesian reasoning to build a probability model of the objecitve function $P(\\text{score} | \\text{hyperparameters})$ which is then used to select the next hyperparameter values to evaluate. The concept is to use more search iterations evaluating promising hyperparameter values by reasoning from the past results. This is an intuitive method of hyperparamter optimization that works in much the same way a human does to get better at any situation: learn from past experiences! If everything works as expected, Bayesian hyperparameter optimization can result in:', '# ', '# * Better generalization performance on the test set', '# * Fewer iterations than random or grid search require', '# ', '# Even if Bayesian optimization (or other automated hyperparameter tuning methods) does not deliver on the above points in all situations, it is a useful skill to master as a data scientist. In the future, data scientists are not going to spend valuable time tweaking model hyperparameters, and knowing methods for accomplishing this automatically will go a long way in your career or studies! Feel free to take this code and apply it to any dataset or to a different machine learning model. I am currently running these methods on a full dataset and will share the results when they are completed. I look forward to the next notebook! ', '# ', '# As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will. Thanks for reading to the end and keep making the data science community such a wonderful place for learning and sharing, ', '# ', '# Will', '# In[ ]:']",326
base-model-with-0-804-auc-on-home-credit (1).py,"['# coding: utf-8', '# ## Base Model Study on Home Credit', '# ', '# The Home Credit Default Risk dataset on the Kaggle is subjected as a final project of my DS/ML bootcamp, and I have spent a period of three weeks on this project. I developed various models and quite a large number of them having AUC scores better than 0.8 ( highest one +0.804). Unfortunately, I could not run any full version of my models on Kaggle because of insufficient RAM issue even though datasets are zipped to almost 4 times by integer/float dtype conversion on my datasets. Moreover, I made a bleend boosting study to acheive highest AUC score (0.81128, much highers possible) on Kaggle (https://www.kaggle.com/hikmetsezen/blend-boosting-for-home-credit-default-risk). Furthermore, I developed also a micro model having only 174 features with a better than  0.8 AUC score (https://www.kaggle.com/hikmetsezen/micro-model-174-features-0-8-auc-on-home-credit)', '# ', '# ', '# Mostly I use Colab Pro to compute LigthGBM calculations with 5-fold CV on GPUs. My models have 900-1800 features. ', '# ', '# I have a limited knowledge about the credit finance, therefore, I combined many Kaggle notebooks for expending number of features as much as I desire and/or acceptance of my LigthGBM models harvesting further enhance scores. I would like to thank these contributors. Some of them are listed here:', '# * https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features <=-- my models are based on this study', '# * https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution', '# * https://www.kaggle.com/sangseoseo/oof-all-home-credit-default-risk <=-- in most cases these hyperparameters are used', '# * https://www.kaggle.com/ashishpatel26/different-basic-blends-possible <=-- thank for blending idea', '# * https://www.kaggle.com/mathchi/home-credit-risk-with-detailed-feature-engineering', '# * https://www.kaggle.com/windofdl/kernelf68f763785', '# * https://www.kaggle.com/meraxes10/lgbm-credit-default-prediction', '# * https://www.kaggle.com/luudactam/hc-v500', '# * https://www.kaggle.com/aantonova/aggregating-all-tables-in-one-dataset', '# * https://www.kaggle.com/wanakon/kernel24647bb75c', '# In[1]:', '# !pip install lightgbm==2.3.1', '# import lightgbm', '# lightgbm.__version__', '# In[2]:', '# load libraries', '# In[3]:', '# run functions and pre_settings', '    # one-hot encoder killer :-)', '# In[4]:', '    # general cleaning procedures', ""    df = df[df['AMT_INCOME_TOTAL'] < 20000000] # remove a outlier 117M"", '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', ""    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True) # set null value"", ""    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True) # set null value"", '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '    # Flag_document features - count and kurtosis', '    # Categorical age - based on target=1 plot', '    # New features based on External sources', '    # Some simple new features (percentages)', '    # Credit ratios', '    # Income ratios', '    # Time ratios', '    # EXT_SOURCE_X FEATURE', '    # AMT_INCOME_TOTAL : income', '    # CNT_FAM_MEMBERS  : the number of family members', ""    # DAYS_BIRTH : Client's age in days at the time of application"", '    # DAYS_EMPLOYED : How many days before the application the person started current employment', '    # other feature from better than 0.8', '# In[5]:', '    # Credit duration and credit/account end date difference', '    # Credit to debt ratio and difference', '    # CREDIT_DAY_OVERDUE :', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[6]:', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Feature engineering: ratios and difference', '    # Interest ratio on previous application (simplified)', '    # Days last due difference (scheduled x done)', '    # from off', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# In[7]:', '    # Flag months with late payment', ""    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) #  0) & (x < 120) else 0)"", '    # Features', '    # Count pos cash accounts', '    # Percentage of previous loans completed and completed before initial term', '    # Number of remaining installments (future installments) and percentage from total', '    # Group by SK_ID_CURR and merge', '    # Percentage of late payments for the 3 most recent applications', '    # Last month of each application', '    # Most recent applications (last 3)', '# In[8]:', '    # Group payments and get Payment difference', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Flag late payment', '    # Flag late payments that have a significant amount', '    # Flag k threshold late payments', '    # Features: Perform aggregations', '    # Count installments accounts', '    # from oof (DAYS_ENTRY_PAYMENT)', '# In[9]:', '    # Amount used from limit', '    # Current payment / Min payment', '    # Late payment  0 else 0)', '    # How much drawing of limit', '    # General aggregations', '    # Count credit card lines', '    # Last month balance of each credit card application', '# In[10]:', '    # keep index related columns', '    # Reduced memory usage', '    # Remove non-informative columns', '    # Removing features not interesting for classifier', '    # dataframe = ligthgbm_feature_selection(dataframe, index_cols, auc_limit=auc_limit)', '    # generate new columns with risk_groupanizer', '    # ending message of DATA POST-PROCESSING', '# In[11]:', '    # loading predicted result ', '    # split train, and test datasets', '    # delete main dataframe for saving memory', '        # Expand train dataset with two times of test dataset including predicted results', '    # Cross validation model', '    # Create arrays and dataframes to store results', '    # limit number of feature to only 174!!!', '    # print final shape of dataset to evaluate by LightGBM', '    # create submission file', '# In[12]:']",117
baseline_lgbm_in_r.R,"['# Forked from https://www.kaggle.com/kailex/tidy-xgb-all-tables-0-796', '# Changed to LGBM model', '# Changed to one-hot encoding', '  group_by(SK_ID_BUREAU) %>% # Aggregate by SK_ID_BUREAU', '  group_by(SK_ID_CURR) %>% # Aggregate by SK_ID_CURR', '# Make prediction and submission']",6
basic-baseline-with-lgb-v3-one-hot-encoder.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[2]:', ""# Now let's look at the shape of this dataset"", '# In[3]:', ""# **Let's look at the data**"", ""# Number of rows of most files are larger than numbers of rows of test / train data. Therefore, if all if entities (IDs) from train / test dataset are included in other files, we can merge other files' features into applicaion_test / application_train dataframe. If not all, but if most of IDs from applicaion_test / application_train are included in other files, we could merge them by estimating empty values. "", '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# **Ideas on hot to merge / concatenate data**', '# ', '# ![](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)', '# According to the image on dataset explanation, there are some keys to merge / concatenate sparse data into some organized form. ', '# ', '# 1. First of all, with SK_ID CURR, we can find linkage between ***train / test dataset , bureau, POS_CASH_balance, credit_card_balance, installment_payment, and previous_application*** together. Here, since train / test dataset is our main target to work on, train/test dataset could be the hub of these data. ', '# ', ""# 2. With SK_ID_PREV, we cand find linkage between ***previous_application, POS_CASH_balance,  credit_card_balance, installment_payment*** and here, previoud_application may be the hub. (Since the linkage is 'SK_ID_PREV' and the infomation is mainly about previous information."", '# ', '# 3. Lastly, with SK_ID_BUREAU, we can link ***bureau and bureau_balance*** thereby linking these to the first group - to train / test dataset. ', '# ', ""# When we merge or link these separate data, we should consider** how to deal with Nan values. **If most of the files in each group shares most of the SK_IDs within there group, we can merge them on SK_ID and fill Nans by rather **1) putting estimated values, or 2) putting 0 or negative values as a sign of 'unidentified''** If 'unable to identify value' itself have significant implication (ex. if, say,  applicants with certain range of risk probability have tendency to have more missing values), putting 0 or negative value as sign of missing value would be more effective. "", ""# First let's see to what extent each group shares SK_IDs"", '# In[12]:', '# **Group 1 - train / test dataset , bureau, POS_CASH_balance, credit_card_balance, installment_payment, and previous_application**', '# In[13]:', '# **credit_card_balance**shows only little amount of IDS shared with training / test dataset. Others seems like including most of ID from training / test data. Seems like only if we handle them well, we could link them and work on it together. ', '# **Group 2 - previous_application, POS_CASH_balance,  credit_card_balance, installment_payment**', '# In[14]:', '# **Group 3 - bureau and bureau_balance**', '# In[15]:', ""# However, here, I'm thinking of linking Bureau data train / test dataset. Since Bureau shares 85% of it's id with train / test data, I'll check how much of those shared IDs are also shared with bureau_balance. "", '# In[16]:', ""# There are many redundant IDs. There are some value differences within the identical IDs, so later we'll think of ways to work on them - say, merge their values into the average, etc. For now, let's just check how many of independent IDs are shared. "", '# In[17]:', '# **To sum up, In group 1, train / test data as hub, we can merge : **', '# ', '# * POS_CASH_balance : 94.66589942597297 %', '# * bureau : 85.84047943186762 %', '# * credit_card_balance : 29.06850430169401 %', '# * installments_payments : 95.3213288234551 %', '# * previous_application : 95.11641941867482%', '# ', '# ** In group 2, previous_application as a hub, we can merge: **', '# * POS_CASH_balance : 53.81963029887188 %', '# * credit_card_balance : 5.564257035326012  %', '# * installments_payments : 57.41210407768106 %', '# ', '# ** Linking train / test data with bureau_balance, using bureau as a link: **', '# * 37.7656453944506%', '# ', '# of entire rows (IDs) could be merged', ""# **Now, let's merge group 1**"", '# ', '# ', '# Processing application_train/test', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# **Processing and merging *POS_CASH_balance***', '# In[22]:', '# ', ""# let's look at how it is distribute."", '#  I refered to the function of plotting from [this kernal](http://https://www.kaggle.com/gpreda/home-credit-default-risk-extensive-eda)', '# In[23]:', ""# Let's merge numerical values by the means within same SK_ID"", '# In[24]:', '# In[25]:', '# **Processing and merging *bureau***', '# In[26]:', '# In[27]:', '# In[28]:', '# In[29]:', '# * **Processing and merging* credit_card_balance***', '# In[30]:', '# In[31]:', '# In[32]:', '# **Processing and mergins *installments_payments***', '# In[33]:', '# In[34]:', '# In[35]:', '# **Processing and merging *previous_application***', '# In[36]:', '# In[37]:', '# In[38]:', '# **Try LGBM with merged data**', '# ', ""# Yet, I have much more things to be done (I noted them at the bottom of this notebook). However, I'll try LGBM with what I've got so far (Group 1)"", '# In[51]:', '# In[52]:', '# In[53]:', '# More things to do:', ""# - Adding categorical features when merging files (I only added numerical features for now. This was because each file have so many data on redundant SK_ID, so in case of numerical data I just merged average value within identical IDs, but in case of categorical data I'm not sure what would be the best way to merge it. For now I'm trying to get mode of categorical values** (But identifying mode of each IDs and merging them takes too much time and memory since there exists too much different IDs and categories.) I'll be glad to share any ideas on how to merge categorical values here!**"", '# ', ""# - Do log processing for numerical values (Haven't processed yet due to irregularity of numerical features, and some of files having too much features)"", '# ', ""# - More carefully handle Nan values : I just put -1 to all Nan values, but in some cases, say, if there exists value '-1' (or near -1) within the columns -1 would not appropriately identify 'Non existing value' for the column."", '# ', '# - Merge group 2 (previous payment as a hub, SK_ID_PREV as keys) to find out information on previous application. ', '# ', '# - Merge Bureau balance to df (Group 3)']",115
basic-jeepy.py,"['# coding: utf-8', '# In[3]:', '# In[4]:', '    # Compute target mean ', '    # Compute smoothing', '    # Apply average function to all target data', '    # The bigger the count the less full_avg is taken into account', '    # Apply averages to trn and tst series', '    # pd.merge does not keep the index so restore it', '    # pd.merge does not keep the index so restore it', '# In[5]:', ""# buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '# In[6]:', '# In[ ]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[24]:', '# In[25]:', '# In[ ]:']",34
bayesian-opt-lgbmclassifier.py,"['# coding: utf-8', '# Table of Contents', '# ', '# 1. [Predictive model LightGBM](#section-1)', '#   1. [Bayesian Optimization](#subsection-11)', '#   1. [LightGBM](#subsection-12)', '# 1. [Results](#section-2)', '#   1. [Errors ROC](#subsection-12)', '#   1. [Feature importance](#subsection-13)', '#   1. [Feature correlations](#subsection-14)', '# 1. [Last run for submission](#section-3)', '# In[1]:', '# ', '# # Predictive model LightGBM', '# In[2]:', '# In[3]:', '# In[4]:', '# ', '# ## Bayesian Optimization', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# ', '# ## LightGBM', '# In[10]:', '# LightGBM parameters found by Bayesian optimization', '# ', '# # Results', '# ', '# ## Errors ROC', '# In[11]:', '# In[12]:', '# ', '# ## Feature importance', '# In[13]:', '# In[14]:', '# In[15]:', '# ', '# ## Feature correlations', '# In[16]:', '# In[17]:', '# ', '# # Last run for submission', '# In[18]:', '# In[19]:', '# LightGBM parameters found by Bayesian optimization', '# In[20]:', '# In[21]:', '# In[22]:']",51
bigdata-project-eda-fe-choi-yunna.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# # ****application_train, application_test  ****', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# **TARGET   AMT_CREDIT( )  **', '# ', '# ', '# plt.subplots()  AMT_CREDIT   ', '# In[12]:', '#   (  True  )', '# **app_train  app_test   preprocessing **', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# ****', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[24]:', '# In[25]:', '# \xa0   \xa0  ', '# In[26]:', '# \xa0   \xa0  plot (TARGET=0)', '# \xa0   \xa0  plot (TARGET=1)', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# iterate through the sources', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# ** ,    **', '# In[37]:', '# In[38]:', '# In[39]:', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# In[44]:', '# AMT_CREDIT  Feature ', '# In[45]:', '# AMT_INCOME_TOTAL  Feature ', '# AMT_INCOME_TOTAL    \xa0  ', '#  \xa0\xa0    . ', '# In[46]:', '# DAYS_BIRTH, DAYS_EMPLOYED  Feature ', '# DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature . ', '# In[47]:', '#  \xa0 , NULL LightGBM     ', '# **    **', '# In[48]:', '# In[49]:', '# In[50]:', '# In[51]:', '# In[52]:', '# In[53]:', '# In[54]:', '# In[ ]:']",88
bigdata-project-eda-fe-ella.py,"['# coding: utf-8', '# ## **Imports**', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# ## EDA : Application data', '# In[3]:', '# In[4]:', '# training   ', '# Training data  307511  \xa0 TARGET  122  \xa0 .', '# In[5]:', '# testing   ', '# Testing data 48744   TARGET  \xa0 121  \xa0 .', '# In[6]:', ""# 'TARGET'  "", '# In[7]:', '# TARGET    \xa0 ', '# In[8]:', '# target   2', '# 0 1   \xa0 1  8% .     \xa0   \xa0 . ', '# In[9]:', '# training   ', '# In[10]:', '# column type  ', '# int64  float64   \xa0 object string   .  encoding \xa0  . ', '# In[11]:', '# target     . \xa0 sorting . ', '#     15,     15 ', ""# ### EDA1 : 'DAYS_BIRTH'"", ""# 'DAYS BIRTH'  \xa0 \xa0  \xa0\xad    .  \xa0, (days) . "", '# In[12]:', '# DAYS_BIRTH  ', '# In[13]:', ""# 'DAYS_BIRTH'  \xa0     -365 . "", '# In[14]:', '# \xa0   \xa0  . ', '#    \xa0  . ', '# In[15]:', '# \xa0   \xa0  plot (TARGET=0)', '# \xa0   \xa0  plot (TARGET=1)', '# target==1()    20-30     .  \xa0    \xa0 \xa0 \xa0 \xa0\xa0  .', '# target==0 1 TARGET      \xa0  \xa0 \xa0  .', '# In[16]:', '#     \xa0 target=1(  \xa0)  \xa0 .', '#  20  70  10 . ', ""#  '~' '~' ."", '# In[17]:', '# cut()   5    \xa0\xa0,       .', '# In[18]:', '# bin groupby \xa0 \xa0 ', '# In[19]:', '#  target \xa0 bar plot  . ', '# \xa0     . ', '# 20-25, 25-30 30-35  10%    \xa0, 55-60, 60-65, 65-70 5%   .', ""# ### EDA2 : 'DAYS_EMPLOYED'"", ""# 'DAYS_EMPLOYED' \xa0  \xa0 ,  \xa0\xad\xa0   \xa0 .     (days)  ."", '# In[20]:', '#   ', '# In[21]:', '#  ', '#  365243 \xa0, std      \xa0   . ', '# In[22]:', '# 365243   \xa0 . \xa0     \xa0   \xa0    .', '# In[23]:', '# 365243   \xa0, 365243     target  .', '# In[24]:', '# (365243)   True , False ', '#  nan ', '# \xa0 ', '# 365243  nan .', '# In[25]:', '# test  train   .', '# True, False   sum True  .', ""# ### EDA3 : 'REGION_RATING_CLIENT_W_CITY'"", ""# 'REGION_RATING_CLIENT_W_CITY' \xa0   , (1,2,3)   \xad   . "", '# In[26]:', '#  ', '# In[27]:', '# catplot  target     ', '# REGION_RATING_CLIENT_W_CITY 2,3      .    \xa0.', '# In[28]:', '# REGION_RATING_CLIENT_W_CITY   TARGET 1  ', ""# ### EDA 4 : 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'"", '# EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3     \xa0  \xa0  . ', '#         \xa0 EDA .', '# TARGET  EXT_SOURCE  EXT_SOURCE   .', '# In[29]:', ""# 'DAYS_BIRTH'     . 'DAYS_BIRTH'  app_train  app_test     ."", '# In[30]:', '#   \xa0.', '# In[31]:', '# EXT_SOURCE1,2,3 TARGET  \xa0 kdeplot   .', '    # subplot   ', '    # target  kdeplot ', '    # title x, y \xa0', '#  \xa0', '# ### EDA 5 :  ', '# column type    , string    .   target   \xa0 encoding\xa0  .', '# In[32]:', '# column type  ', '# int64  float64   \xa0 object string   .  encoding \xa0  . ', '# In[33]:', '# object  list  ', ""# - 'NAME_CONTRACT_TYPE' : \xa0"", ""# - 'CODE_GENDER': "", ""# - 'FLAG_OWN_CAR':  \xa0 "", ""# - 'FLAG_OWN_REALTY':  \xa0 "", ""# - 'NAME_TYPE_SUITE': \xa0"", ""# - 'NAME_INCOME_TYPE': \xa0"", ""# - 'NAME_EDUCATION_TYPE': \xa0  \xa0"", ""# - 'NAME_FAMILY_STATUS': \xa0  \xa0"", ""# - 'NAME_HOUSING_TYPE':  \xa0"", ""# - 'OCCUPATION_TYPE':  \xa0"", ""# - 'WEEKDAY_APPR_PROCESS_START': \xa0\xad "", ""# - 'ORGANIZATION_TYPE': \xa0  \xa0"", ""# - 'FONDKAPREMONT_MODE','HOUSETYPE_MODE','WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE' : \xa0 \xad \xa0"", '# ', ""#      \xa0\xa0   'CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_INCOME_TYPE','NAME_HOUSING_TYPE','OCCUPATION_TYPE'  \xa0."", '# In[34]:', '#   target  .', '# In[35]:', '# target    .', '# ****', ""# - 'CODE_GENDER':          . "", ""# - 'FLAG_OWN_CAR':        ."", ""# - 'FLAG_OWN_REALTY':        . "", ""# - 'NAME_INCOME_TYPE': State servant, working, unemployed      . "", ""# - 'NAME_HOUSING_TYPE':            ."", ""# - 'OCCUPATION_TYPE': , ,        . "", ""# ### EDA 6 : 'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_ANNUITY','AMT_GOODS_PRICE'"", ""# - 'AMT_CREDIT': "", ""# - 'AMT_INCOME_TOTAL' : \xa0 "", ""# - 'AMT_ANNUITY' :   "", ""# - 'AMT_GOODS_PRICE' :   "", '# In[36]:', '#    .', '# In[37]:', '#   \xa0.', '# In[38]:', ""# 'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_ANNUITY' TARGET  \xa0 kdeplot   ."", '    # subplot   ', '    # target  kdeplot ', '    # title x, y \xa0', '#  \xa0', '# ###  ', '# In[39]:', '#    \xa0 . ', '        #  ', '        #  ', '        #  ', '        # column   \xa0', '        #      ', '        # \xa0 ', '# In[40]:', '#  ', '# ###   \xa0 training data, testing data ', '# In[41]:', '# In[42]:', '# In[43]:', '# ## Feature Engineering : Application data', '# ###   Label Encoding', '# In[44]:', '#    \xa0   ', '# In[45]:', ""# ### FE1 : 'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_ANNUITY','AMT_GOODS_PRICE'"", '#     .', '# - CREDIT_INCOME_PERCENT: \xa0      ', '# - ANNUITY_INCOME_PERCENT: \xa0      ', '# - CREDIT_TERM:    ( )', '# - GOODS_CREDIT_RATIO:        ', '# - CREDIT_GOODS_DIFF:    -   ', '# - GOODS_INCOME_RATIO : \xa0      ', '# In[46]:', '#   ', ""# ### FE2 : 'EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3'"", '#      .', ""# - 'APPS_EXT_SOURCE_MEAN': EXT_SOURCE  \xa0"", ""# - 'APPS_EXT_SOURCE_STD' : EXT_SOURCE  "", '# In[47]:', '# In[48]:', '# column  \xa0 ', '# In[49]:', '#        \xa0    mean std  .', '# In[50]:', '# In[51]:', ""# 'APPS_EXT_SOURCE_STD' NULL  \xa0 "", '# In[52]:', ""# ### FE3 : 'DAYS_BIRTH', 'DAYS_EMPLOYED'"", '#     .', ""# - 'EMPLOYED_BIRTH_RATIO' : \xa0    \xa0 "", ""# - 'INCOME_EMPLOYED_RATIO' :  \xa0     "", ""# - 'INCOME_BIRTH_RATIO' : \xa0     "", ""# - 'CAR_BIRTH_RATIO' : \xa0   \xa0  "", ""# - 'CAR_EMPLOYED_RATIO' :  \xa0   \xa0  "", '# In[53]:', '# DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature ', '# ### Null   ', '# In[54]:', '# ## EDA : CREDIT CARD BALANCE data', '# In[55]:', '# In[56]:', '# In[57]:', '# In[58]:', '# application_train  credit_card_balance  merge', '# ### Null  ', '# In[59]:', '# ### SK_ID_CURR \xa0 SK_ID_PREV  ', '# In[60]:', '# In[61]:', '# In[62]:', '# boxplot  , \xa0', '#   \xa0 \xa0 37 \xa0. ', '# ### EDA1 :  ', '# In[63]:', '# In[64]:', ""# 'NAME_CONTRACT_STATUS'  target  "", '    # Rotate x-labels', '# In[65]:', '# crosstab  , 0 completed  1      .', '# ### EDA2 :  ', '# In[66]:', '#     ', '# In[67]:', ""# app_train[['SK_ID_CURR', 'TARGET']]   \xa0 "", ""# on='SK_ID_CURR'   "", '# In[68]:', '#    ', '# In[69]:', ""# 'SK_ID_PREV','SK_ID_CURR','TARGET'  "", '# In[70]:', '#   show_hist_by_target  ', '# ### EDA3 :   ', '# In[71]:', '# target     . \xa0 sorting . ', '#     15,     15 ', '# ### EDA4 : AMT_BALANCE, AMT_CREDIT_LIMIT_ACTUAL, AMT_INST_MIN_REGULARITY,AMT_RECIVABLE, AMT_TOTAL_RECEIVABLE,CNT_INSTALMENT_MATURE_CUM', '# target    \xa0 \xa0 \xa0 6    \xa0.', '# (AMT_BALANCE, AMT_CREDIT_LIMIT_ACTUAL, AMT_INST_MIN_REGULARITY,AMT_RECIVABLE,', '# AMT_TOTAL_RECEIVABLE,CNT_INSTALMENT_MATURE_CUM )', '# ', '# - AMT_BALANCE :   ', '# - AMT_CREDIT_LIMIT_ACTUAL :    ', '# - AMT_INST_MIN_REGULARITY :  \xa0  ', '# - AMT_TOTAL_RECEIVABLE :    ', '# - AMT_RECIVABLE :    ', '# - CNT_INSTALMENT_MATURE_CUM :   ', '# ', '# In[72]:', '# AMT_BALANCE', '# AMT_CREDIT_LIMIT_ACTUAL', '# AMT_INST_MIN_REGULARITY', '# CNT_INSTALMENT_MATURE_CUM', '# AMT_INST_MIN_REGULARITY', '# AMT_CREDIT_LIMIT_ACTUAL', '# In[73]:', '# mean, median, count  ', '# ## Feature Engineering : CREDIT CARD BALANCE data', '# In[74]:', '#   ccb_group \xa0', '# In[75]:', '# In[76]:', '# MultiIndex    ', '# In[77]:', '#  ', '# In[78]:', '# In[79]:', '# In[80]:', '# ###   ', '# In[81]:', '#    \xa0   ', '# ## Modeling', '# In[82]:', '# In[83]:', '#  train, test  ', '# In[84]:', '# In[85]:', '# In[86]:', '# In[87]:', '#  Classifier    \xa0  Kaggle Submit ', '# In[88]:']",290
bigdata-project-eda-fe-hr.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# ## 1.   app   ', '# In[2]:', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# ## 2. Application_train  EDA ', '# In[4]:', '# In[5]:', '# In[6]:', '# ## 3. Feature  TATGET    ', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# ## 4.   TARGET  ', '# In[14]:', '# In[15]:', '# In[16]:', '# ## 4-1. EXT_SORCE_X  TARGET     ', '# In[17]:', '# In[18]:', '# In[19]:', '## #  EXT_SOURCE  TARGET  \xa0  ', '# iterate through the sources', '# ## 5. Feature Engineering', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[24]:', '# In[25]:', '# In[26]:', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# ## 6.     \xa0 LGBM Classifier  .', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# ## submit ', '# ![image.png](attachment:92196adc-df60-454c-8af6-4d9103f1765b.png)', '# ## 7. prev_application    EDA Feature Engineering     /', '# ', '# -> PREVIOUS_APPLICATION.CSV -   \xa0 (\xa0   \xa0   \xa0 \xa0)', '# In[40]:', '    # EXT_SOURCE_X FEATURE ', '    # AMT_CREDIT  Feature ', '    # AMT_INCOME_TOTAL  Feature ', '    # DAYS_BIRTH, DAYS_EMPLOYED  Feature ', '# In[41]:', '# In[42]:', '# In[43]:', '# In[44]:', '# In[45]:', ""# app_train[['SK_ID_CURR', 'TARGET']]   \xa0 on='SK_ID_CURR' "", '# In[46]:', '#  dtype \xa0   ', '# In[47]:', '# In[48]:', '# In[49]:', '# In[50]:', '# \xa0\xad \xa0 ,   ,   ,      . ', '    #  \xa0\xad  \xa0 /    ', '    # DAYS_XXX  365243()  NULL ', '    #     ', '    #       \xa0   . ', '    #   \xa0  RATE_INTEREST_PRIVILEGED  Null  \xad   ', '    # \xa0    AMT_CREDIT  \xa0      . ', '# In[51]:', '#     \xa0   SK_ID_CURR \xa0 Aggregation .  ', '         #    SK_ID_CURR \xa0 Aggregation ', '        # \xa0   SK_ID_CURR \xa0 Aggregation ', ""    # multi index  '_'   "", ""    # 'SK_ID_CURR'   SK_ID_CURR    "", '# In[52]:', '# NAME_CONTRACT_STATUS SK_ID_CURR Approved, Refused  .  ', '    #  groupby  +    groupby .  \xa0 aggregation    unstack() \xa0 . ', '    # NAME_CONTRACT_STATUS group  Approved Refused  ', '    #  . ', '    # NaN  0 . ', ""    # 'SK_ID_CURR'   SK_ID_CURR    "", '# In[53]:', '#   prev_amt_agg prev_refused_appr_agg \xa0 SK_ID_CURR APPROVED_COUNT  REFUSED_COUNT  ', '    # prev_amt_agg . ', '    # SK_ID_CURR    APPROVED_COUNT  REFUSED_COUNT  . ', ""    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'  drop "", '# In[54]:', '# apps previous   SK_ID_CURR\xa0   aggregation  prev_agg ', '#     ', '# In[55]:', '# Label  . ', '# In[56]:', '#      . ', '# In[57]:', '#  . ', '    #     ', '# In[58]:', '# In[59]:', '# In[60]:', '# In[61]:', '# ## submit ', '# ![image.png](attachment:f258d504-8c68-4fc9-bbdd-820ac54035e2.png)', '# ## 8. bureau bureau_bal    EDA Feature Engineering     /', '# ->bureau.csv :    \xa0  \xa0 \xa0 \xa0  \xa0', '# In[62]:', '# In[63]:', '# TARGET  \xa0  bureau apps ', '# In[64]:', '# In[65]:', '#   ID TARGET \xa0', '# In[66]:', '# In[67]:', '# In[68]:', '# In[69]:', '# correlated features', '# In[70]:', '# bureau   \xa0       \xa0  .', '    # \xa0     \xa0     \xa0  .  ', '    #   /     ', '# In[71]:', '# bureau        \xa0  SK_ID_CURR \xa0 aggregation  . ', '    #   ', '    #   SK_ID_CURR reset_index()  ', '# In[72]:', ""# Bureau CREDIT_ACTIVE='Active'   filtering         \xa0  SK_ID_CURR \xa0 aggregation  "", ""    # CREDIT_ACTIVE='Active'   filtering"", '        #   ', '    #   SK_ID_CURR reset_index()  ', '# In[73]:', '# bureau_bal SK_ID_CURR \xa0  MONTHS_BALANCE aggregation  ', '    # SK_ID_CURR \xa0  MONTHS_BALANCE aggregation  ', '    #   SK_ID_CURR reset_index()  ', '# In[74]:', '#  bureau\xa0 aggregation      ', '    #  bureau\xa0 aggregation      return  ', '# In[75]:', '# In[76]:', '# application, previous, bureau, bureau_bal \xa0    . ', '# In[77]:', '# Category   Label   ', '#    ', '# In[78]:', '# In[79]:', '# ## submit ', '# ![image.png](attachment:9230e333-0c7e-4e82-bad6-e3ef9ceec005.png)', '# ## - bureau, application, previous        .']",167
bigdata-project-eda-fe-je2760sul.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# Prev_application    EDA Feature Engineering     /', '# In[2]:', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '#  Feature  application_train application_test ', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '    # EXT_SOURCE_X FEATURE ', '    # AMT_CREDIT  Feature ', '    # AMT_INCOME_TOTAL    \xa0 Feature ', '    # DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature ', '# In[8]:', '# Previous_application ', '# In[9]:', '# In[10]:', '# In[ ]:', '# appication previous \xa0 ', '# ', '# -  ', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# Null  \xa0 \xa0    ', '# In[15]:', '# In[16]:', '# In[ ]:', '# Target \xa0   feature ', '# In[17]:', '# prev_app app_train inner  ==> TARGET  application_train(app_train)  ', '# ==> prev_app TARGET \xa0  FEATURE  \xa0 \xa0  ', ""# app_train[['SK_ID_CURR', 'TARGET']]   \xa0 "", ""# on='SK_ID_CURR'   "", '# In[18]:', '# TARGET 1   ', '# TARGET 0   ', '# In[19]:', '# Numerical Features( )  Categorical Features(/object )  ', '# In[20]:', '# Numerical Features & Categorical Feature   ', '# In[21]:', '#  ', '    # Rotate x-labels', '# previous_application  EDA', '# In[22]:', '# In[23]:', '# In[24]:', '# In[25]:', '# numberic feature ', '# In[26]:', '# RATE_DOWN_PAYMENT : \xa0 (\xa0)', '#   0.0~0.1  ', '# RATE_INTEREST_PRIMARY : ', '# RATE_INTEREST_PRIVILEGED : ', '# In[27]:', '# CNT_PAYMENT : \xa0  \xa0\xad  \xa0 TERM', '#   , 0~20     ', '# DAYS_FIRST_DUE : \xa0\xad \xa0   ', '# DAYS_LAST_DUE : \xa0\xad \xa0   ', '# In[28]:', ""# 'AMT_ANNUITY' :   "", '#  \xa0          ', '# In[29]:', '# NAME_CASH_LOAN_PURPOSE :   \xa0', '# In[ ]:', '# APP_PREV   FEATURE ENGINEERING ', '# In[30]:', '# groupby SK_ID_CURR', '# ', '# In[31]:', '#   prev_group \xa0', ""prev_group = prev_app.groupby('SK_ID_CURR') # groupby  "", '# In[ ]:', '#       ?', '# In[ ]:', '# In[32]:', '#   AMT_CREDIT \xa0    CNT_PAYMENT ', '# In[33]:', '# AMT_ANNUITY   ', '# In[34]:', '# In[35]:', '# In[36]:', '# SK_IP_CURR     \xa0,    \xa0, ,   ', '# In[37]:', '#    \xa0, ,   ', '# In[38]:', '# In[ ]:', '# COUSTOMER   (, ) ', '# In[39]:', '# In[40]:', '# Refused   y 0, Approved  Y=1 ', '# In[41]:', '# In[42]:', '# COUSTOMER APPROVED(=1)  ', '# In[43]:', '# COUSTOMER  \xa0\xad  APPROVED  \xa0   ', '# In[44]:', '# In[45]:', '# COUSTOMER     ', '# In[46]:', '# PURPOSE     ', '# In[47]:', '# ', '# XNA(\xa0  X), XNP(CASH LOAN ) \xa0\xa0 ELECTONIC EQUIPMENT \xa0   ', '# In[ ]:', '# SK_ID_CURR  NAME_CONTRACT_STATUS REFUSED   \xa0\xad ', '# In[48]:', '# In[49]:', '# In[50]:', '# dataframe  \xa0  \xa0\xad \xa0\xad  max min  ', '# In[51]:', '# In[52]:', '# merge   dataframe reset_index', '# In[53]:', '# merge', '# In[54]:', '# Null 0 ', '# In[55]:', '#    ', '# In[56]:', '# In[57]:', '#  \xa0\xad   ', '# In[58]:', '# In[59]:', '#    \xa0\xad      ', '# In[60]:', '# \xa0\xad   %  ', '# In[61]:', '# SK_ID_CURR MAX,MIN DIFFERENCE \xa0', '# In[62]:', '# RESET_INDEX\xa0 prev_refused_agg MERGE', '# In[63]:', '# Null 0 ', '# In[64]:', '# In[65]:', '# In[66]:', '# In[67]:', '# In[68]:', '#  \xa0 , NULL LightGBM     ', '# In[69]:', '# In[70]:', '#      ', '# In[71]:', '# In[72]:', '#     \xa0 LGBM Classifier  ', '# In[73]:', '# In[74]:', '# In[75]:', '#  Classifier    \xa0  Kaggle Submit ', '# In[ ]:', '# In[76]:', '# In[ ]:']",166
bigdata-project-eda-fe-leejihyefinal.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '#     ', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# MERGE  previous apps ', '# In[10]:', '# In[11]:', '# # 1. EDA', '#    \xa0   \xa0   ', '#      \xa0      \xa0\xa0 \xa0  ', '#     \xa0   \xa0  \xa0,   \xa0      ', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', ""# #### 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'/ \xa0\xa0, 'DAYS_BIRTH'     TARGET      \xa0  ."", '# In[16]:', '# \xa0   \xa0  plot (TARGET=0)', '# \xa0   \xa0  plot (TARGET=1)', '# In[17]:', '# iterate through the sources', '# In[18]:', '# TARGET  EXT_SOURCE  EXT_SOURCE   ', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[24]:', '    # Rotate x-labels', '# In[25]:', '#    ', '#    Target', '# In[26]:', '# In[27]:', '# In[28]:', '# In[29]:', ""prev_group = prev.groupby('SK_ID_CURR') # groupby  "", '# In[30]:', '# In[31]:', '# # 2. FEATURE ENGINEERING ', '#     Target   \xa0 ', '# Target \xa0     \xa0  () , ,   ', '# ', '# In[ ]:', '# In[32]:', '# Null  \xa0 \xa0    ', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# AMT_CREDIT  Feature ', '# In[40]:', '# AMT_INCOME_TOTAL  Feature ', '# AMT_INCOME_TOTAL    \xa0  ', '#  \xa0\xa0    . ', '# In[41]:', '# DAYS_BIRTH, DAYS_EMPLOYED  Feature ', '# DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature . ', '# In[42]:', '# In[43]:', '# In[44]:', '# In[45]:', '# In[46]:', '# In[47]:', '#  \xa0  ', '# In[48]:', '# #   ', '# In[49]:', '#  \xa0 , NULL LightGBM     ', '# In[ ]:', '# In[50]:', '# In[51]:', '# In[ ]:', '#   \xa0 LGBM Classifier  ', '# ', '#     ', '# ', '# /   ', '# In[52]:', '# In[53]:', '# In[54]:', '# In[55]:', '# ### Feature importance ', '# In[56]:', '# ###  classfier    \xa0 kaggle submit', '# In[57]:', '# In[58]:', '# In[ ]:', '# In[ ]:', '# ', '# :-)', '# In[59]:', '#    learning_rate=0.02,', '#    num_le', '# In[ ]:']",117
bigdata-project-eda-fe-my.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# In[3]:', '# In[4]:', '# ### Application_train Application_test  read_csv', '# In[5]:', '# In[6]:', '# ## Target column  ', '# #### Target 0(\xa0   ), 1( \xa0)  ', '# In[7]:', '# In[8]:', '# #### app_train app_test    preprocessing ', '# #####    ', '# In[9]:', '# In[10]:', '# In[11]:', '# #### Object feature() ', '# #####    ->Label Encoding', '# #####      -> Onehot Encoding', '# #####  \xa0, Feature importance   Label Encoding \xa0,  Onehot Encoding  .', '# #####   \xa0 apps_train apps_test \xad', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# ## EDA', '# #### train data  EDA', '# #### Null  ', '# In[16]:', '# In[17]:', '# ####     ', '# In[18]:', '# In[19]:', '# #### train data test data \xa0 LGBM Classifier ', '# ##### ftr_app= , target_app= ', '# In[20]:', '# #### Feature importance ', '# #####     ', '# In[21]:', '# In[22]:', '# #### feature  TARGET  ', '# In[23]:', '# In[24]:', '# Feature Importances     20 columns', '# ####   3  EXT_SOURCE_1,EXT_SOURCE_2,EXT_SOURCE_3 ', '# In[25]:', '# TARGET  EXT_SOURCE  EXT_SOURCE   ', '# In[26]:', '# In[27]:', '# iterate through the sources', '# ####  3   DAYS_BIRTH  TARGET     DAYS_BIRTH  \xa0 ', '# In[28]:', '# In[29]:', '# \xa0   \xa0  ', '# In[30]:', '# \xa0   \xa0  plot (TARGET=0)', '# \xa0   \xa0  plot (TARGET=1)', '# #### Feature Engineering', '# ##### apps \xa0  ', '# ##### EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 ', '# ##### AMT_CREDIT  ', '# ##### AMT_INCOME_TOTAL  ', '# ##### DAYS_BIRTH, DAYS_EMPLOYED  ', '# In[31]:', '# In[32]:', '    # EXT_SOURCE_X FEATURE ', '    # AMT_CREDIT  Feature ', '    # AMT_INCOME_TOTAL    \xa0 Feature ', '    # DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature ', '# #### previous_application data JOIN', '# ##### SK_ID_CURR MERGE', '# In[33]:', '# #### TARGET\xa0    histogram ', '# In[34]:', '# prev_app app_train inner  ==> TARGET  application_train(app_train)  ', '# ==> prev_app TARGET \xa0  FEATURE  \xa0 \xa0  ', ""# app_train[['SK_ID_CURR', 'TARGET']]   \xa0 "", ""# on='SK_ID_CURR'   "", '# In[35]:', '# In[36]:', '#  dtype \xa0   ', '# In[37]:', '# In[38]:', '# In[39]:', '# AMT_ANNUITY, AMT_CREDIT, AMT_APPLICATION, AMT_CREDIT TARGET=1     (  )', '# #### TARGET \xa0  Category  Histogram ', '# In[40]:', '# In[41]:', '# ## prev_app data  Feature Engineering', '# In[42]:', '# In[43]:', '# prev   /  \xa0\xad       ', '# In[44]:', '# DAYS_XXX  365243  NULL \xa0,      ', '#     ', '# In[45]:', '# In[46]:', '# In[47]:', '     #  . ', '    #  ', '# In[48]:', '# In[49]:', '# #### NAME_CONTRACT_STATUS = Refused  ', '# In[50]:', '# In[51]:', '# In[52]:', '# Null 0 ', '# SK_ID_CURR    PREV_REFUSED_COUNT  ', '# In[53]:', '#  \xa0 groupby  unstack()  Group by Case  >>   2 ', '# In[54]:', '#  , Null , \xa0  prev_amt_agg    ', '# In[55]:', '# In[56]:', '# prev_amt_agg . prev_amt_agg prev_refused_appr_agg  SK_ID_CURR INDEX \xa0  ', '# SK_ID_CURR    APPROVED_COUNT  REFUSED_COUNT  ', ""# 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'  drop "", '# prev_amt_agg prev_refused_appr_agg INDEX SK_ID_CURR   \xa0   ', '# In[ ]:', '# ####   ', '# In[57]:', '#    numeric    ', '# In[58]:', '# In[59]:', '# In[60]:', '# In[61]:', '# In[62]:', '# In[63]:', '# In[64]:', '# In[65]:', '# In[66]:', '# #### bureau EDA', '# In[67]:', '# TARGET  \xa0  bureau apps ', '# In[68]:', '# In[69]:', '# Null      ( )', '# In[70]:', '# Numerical Features( )  Categorical Features(/object )  ', '#    , \xa0  ', '# In[71]:', '#  ', '    # Rotate x-labels', '# #### Numerical Feature ', '# In[72]:', '# DAYS_CREDIT:   \xa0\xad     \xa0\xad    ', '# CREDIT_DAY_OVERDUE:  \xa0\xad  CB \xa0  ', '# DAYS_CREDIT_UPDATE:  \xa0\xad\xa0  \xa0  ', '# AMT_CREDIT_SUM_LIMIT: \xa0   \xa0', '# AMT_CREDIT_SUM_DEBT:    ', '# AMT_CREDIT_SUM_OVERDUE:  ', '# In[73]:', '# \xa0(\xa0)    ,    type   ', '# CNT_CREDIT_PROLONG \xa0  ', '# CREDIT_TYPE  \xa0', '# In[74]:', '# correlated features', '# #### bureau_balance  EDA', '# In[75]:', '# In[76]:', '# In[77]:', '# Types of colors', '# Count Plot ', '# #### Bureau Feature Engineering', '# In[78]:', '# bureau_bal  bureau  join', '# In[79]:', '# \xa0    ', '# In[80]:', '# \xa0    \xa0 ', '# In[81]:', '# \xa0  \xa0 \xa0    = \xa0     / \xa0    \xa0 ', '# --> \xa0  \xa0    \xa0  \xa0  ', '# In[82]:', '#   \xad\xa0', '# In[83]:', '# BUREAU  active   (CREDIT ACTIVE  CLOSED)', '# Closed   y 0, ACTIVE  Y=1 ', '# CUSTOMER    \xa0   ', '#   \xad\xa0', '# In[84]:', '# bureau   \xa0       \xa0  ', '# \xa0     \xa0     \xa0  .  ', '#   /     ', '# In[85]:', '# \xa0  \xa0  ', '# ####  Classifier    \xa0  Kaggle Submit ', '# In[86]:']",198
bigdata-project-eda-fe-seoyeong.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# #    9 plot ', '# In[24]:', '# In[25]:', '# ', '# #   plot (by type of the loan --)', '# In[26]:', '    # Calculate the percentage of target=1 per category value', '# In[27]:', '# In[28]:', '# In[29]:', '# previous_application ', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# In[34]:', ""# 'AMT_CREDIT'  groupby"", '# In[35]:', '# In[36]:', ""# 'SK_ID_CURR'  groupby"", '# In[37]:', '# In[38]:', '    # EXT_SOURCE_X FEATURE ', '    # AMT_CREDIT  Feature ', '    # AMT_INCOME_TOTAL    \xa0 Feature ', '    # DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature ', '# In[39]:', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# In[44]:', '# In[45]:', '# In[46]:', '# In[47]:']",70
bigdata-project-eda-fe-shyeon43.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# **  import app   **', '# In[2]:', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# traing ', '# In[4]:', '# testing ', '# # **Application_train   EDA**', '# ** **', '# In[5]:', '    mis_val=df.isnull().sum() # \xa0   ', '    mis_val_percent=100*df.isnull().sum()/len(df) # \xa0', '    mis_val_table=pd.concat([mis_val, mis_val_percent],axis=1) #   & \xa0  ', ""    mis_val_table_ren_columns=mis_val_table.rename(columns={0:'Total',1:'Percent'}) #  "", '    #  0  \xa0\xa0 \xa0\xa0', '         ""      ""+str(mis_val_table_ren_columns.shape[0])+\' .\') #   print', '# In[6]:', 'mis_values = missing_values(app_train) #    ', '# In[7]:', 'app_train.isnull().sum() # \xa0   122,     67', '# In[8]:', 'mis_values = missing_values(app_test) #    ', '# In[9]:', 'app_test.isnull().sum() # \xa0   121,     64', '# **Target  **', '# In[10]:', ""app_train['TARGET'].value_counts() # value  count"", '# In[11]:', ""app_train['TARGET'].astype(int).plot.hist(); # \xa0     "", '# ** Types**', '# ', '# int64, float64:   & object :  ', '# ', '# In[12]:', '# In[13]:', 'app_train.dtypes.value_counts() # value  count', '# In[14]:', ""app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0) # object  unique  "", '# > name_contract_type 2  unique  2', '# **  feature  TARGET  0 1 Histogram **', '# In[15]:', ""    cond_1 = (df['TARGET'] == 1) # target 1 "", ""    cond_0 = (df['TARGET'] == 0) # target 0 "", '        fig, ax = plt.subplots(figsize=(12, 4), nrows=1, ncols=2, squeeze=False) # plot ', ""        sns.distplot(df[cond_1][column], label='1', color='red', ax=ax[0][1]) # target 1 "", ""        sns.distplot(df[cond_0][column], label='0', color='blue', ax=ax[0][1]) # target 2 "", '# In[16]:', '# Feature Importances     \xa0', '# > * EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3    ', '# > *   3 \xad ', '# # \xa0  TARGET \xa0  ', '#  histplot \xa0 \xa0 countplot ', '# In[17]:', ""cate_col = app_train.dtypes[app_train.dtypes == 'object'].index.tolist() # object   "", '# In[18]:', 'def show_category_by_target(df, columns): #    ', ""        chart = sns.catplot(x=column, col='TARGET', data=df, kind='count') #  count"", 'show_category_by_target(app_train, cate_col) # app_train  object   ', '# **     **', '# In[19]:', '# DAYS_BIRTH \xa0 TARGET ', '# \xa0    \xa0  ', '# In[20]:', '# \xa0   \xa0  ', '# In[21]:', '# \xa0   \xa0  plot (TARGET=0)', '# \xa0   \xa0  plot (TARGET=1)', '# *   ', '# #   TARGET  ', '# In[22]:', ""               'DAYS_EMPLOYED','DAYS_ID_PUBLISH', 'DAYS_REGISTRATION', 'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'TARGET'] #   "", 'Im_col = app_train[T_columns].corr() #   \xa0    ', '# In[23]:', '# ', '# #  ', '# In[24]:', '# TARGET train     ', '# train  test      \xa0', '# TARGET  ', '# #     ', '# In[25]:', ""(app_train['DAYS_BIRTH'] / -365).describe() #  "", '# In[26]:', ""app_train['DAYS_EMPLOYED'].describe() #  "", '# In[27]:', ""plt.xlabel('Days Employment'); #  \xa0      "", '# In[28]:', ""anom=app_train[app_train['DAYS_EMPLOYED']==365243] #  "", ""non_anom=app_train[app_train['DAYS_EMPLOYED']!=365243] #    "", '#   \xa0   \xa0 \xa0 5.4% ', '# In[29]:', '# (365243)   True , False ', '#  nan ', '# In[30]:', '# test  train   ', '# # EXT_SOURCE ', '# In[31]:', '#    ', '# In[32]:', '# ', '# > *  EXT_SOURCE TARGET   : EXT_SOURCE   \xa0 ', '# > * source_1 DAYS_BIRTH  ', '# In[33]:', '# ', ""sns.kdeplot(app_train.loc[app_train['TARGET']==0, 'EXT_SOURCE_1'],label='target==0') # target=0"", ""sns.kdeplot(app_train.loc[app_train['TARGET']==1, 'EXT_SOURCE_1'],label='target==1'); # target=1"", '# In[34]:', '# source_1, 2, 3 ', '# * source_3 TARGET     ', '# In[ ]:', '# #  Feature  FE1', '# In[35]:', '# null  ', '# In[36]:', ""app_train['EXT_SOURCE_1'].value_counts(dropna=False) # value  count & \xa0 NaN "", '# In[37]:', ""app_train['EXT_SOURCE_2'].value_counts(dropna=False) # value  count & \xa0 NaN "", '# In[38]:', ""app_train['EXT_SOURCE_3'].value_counts(dropna=False) # value  count & \xa0 NaN "", '# **  \xa0    **', '# In[39]:', 'apps = pd.concat([app_train, app_test]) # train  test  ', '# **source     **', '# In[40]:', '# EXT_SOURCE_X  \xa0///  ', '# In[41]:', ""apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].head() #   "", '# In[42]:', '#        \xa0    max, min \xa0  mean std  ', '# In[43]:', ""apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APPS_EXT_SOURCE_STD']].head(10) #   "", '# In[44]:', ""apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean()) #  "", '# **Feature **', '# In[45]:', '# AMT_CREDIT  Feature ', '# In[46]:', '# AMT_INCOME_TOTAL  Feature ', '# AMT_INCOME_TOTAL    \xa0  ', '#  \xa0\xa0    . ', '# In[47]:', '# DAYS_BIRTH, DAYS_EMPLOYED  Feature ', '# DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature . ', '# In[48]:', '#  \xa0 , NULL LightGBM     .', '# **     **', '# In[49]:', '# **LGBM **', '# In[50]:', '# In[51]:', '# In[52]:', '# ** Classifier    \xa0  Kaggle Submit **', '# In[53]:', '# In[54]:', '# In[ ]:', '# # **FE2**', '# ****', '# In[55]:', '#   import', '#  iterate', '        #  object\xa0     ( type  unique  2)', '            # train test    train fit', '            # train-set, test-set Transform', '            #     ', '# In[56]:', '# Label-encoding\xa0     One-hot encoding \xa0', '# **  **', '# In[57]:', '# polynomial features    ', '#    import', ""imputer = SimpleImputer(strategy='median') #  "", '# target \xa0   null  imuter', '#   null ', '# In[58]:', 'poly_ft_train # imputer \xa0  ', '# In[59]:', 'poly_ft_test # imputer \xa0  ', '# In[60]:', '# 3 \xad ', '# In[61]:', '# train  fit', '# features transform', '# In[62]:', 'poly_ft_train # transform \xa0  ', '# In[63]:', '#    ', '# > * degree 3 \xa0 1, 2, 3 \xad   ', '# ** feature target  **', '# In[64]:', '# features   \xa0 ', '# TARGET  ', '# TARGET   (corr )', '#      ', '# In[65]:', '# Put test features into dataframe', '#  train   \xa0 \xad merge   ', '#  test   \xa0 \xad merge   ', '# Align the dataframes => train  align ', '# Print out the new shapes', '# In[66]:', '#   test  merge', '#  train  SK_ID_CURR \xa0\xa0  \xa0 \xad join', '#  test  SK_ID_CURR \xa0\xa0  \xa0 \xad join', '# train  align ', '# shape ', '# In[67]:', '#     ', '# train   ', '# In[68]:', '# test   ', '# In[69]:', '# domain   ', ""    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==0,feature],label='target==0') # target=0"", ""    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET']==1,feature],label='target==1') # target=1"", '# > * target      ', '# In[ ]:', '# In[ ]:']",229
bigdata-project-eda-jisun.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# In[3]:', '# In[4]:', '# Application_traion  feature', '# In[5]:', '# Testing data features ', '# # EDA', '# ### Examine the distribution of the target cloumns', '# In[6]:', '# In[7]:', '# ### Examing Missing Values', '# In[8]:', '# In[9]:', '# ### column type', '# In[10]:', '# In[11]:', '# object type  unique  ', '# ### encoding categorical variables', '# In[12]:', '# \xa0  2 , labelencoder .', '# 2 , pd.get_dummies one-hot-encoding', '# In[13]:', '# one-hot-encoding', '# In[14]:', '# train test  . get_dummies\xa0  .', '# In[15]:', '#   ', '# In[16]:', '# ### Aligning Training and Test data', '# In[17]:', '#   train \xa0\xa0', ""train_labels = app_train['TARGET'] #    "", '# In[18]:', '# ###  anomalies', '# In[19]:', '# In[20]:', '# In[21]:', ""#   \xa0\xa0 \xa0    'DAYS_EMPLOYED'"", ""app_train['DAYS_EMPLOYED'].describe() # \xad ."", '# In[22]:', '# In[23]:', '# 365243  \xad', '# In[24]:', '# 365243   target  .', '#      loan\xa0  . ,   . ', '# In[25]:', '# ###   DAYS_EMPLOYED_ANOM  ', '#        \xa0\xa0 .', '# In[26]:', '# In[27]:', '# 365243 np.nan(NaN) \xa0 hist  \xa0\xa0 \xa0 \xa0', '# In[28]:', '# app_test ', '# ###  correlation', '# In[29]:', '# TARGET  ', '# \xa0  DAYS_BIRTH , \xa0  EXT_SOURCE_1,2,3 ', '# In[30]:', '# DAYS_BIRTH', ""app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH']) # \xa0 "", ""app_train['DAYS_BIRTH'].corr(app_train['TARGET']) # \xa0 \xa0  negative"", '# In[31]:', '# In[32]:', '# \xa0  \xa0 ', '# In[33]:', '# In[34]:', '# \xa0   10', '# In[35]:', '# \xa0 TARGET', '# In[36]:', '# \xa0     !', '# In[37]:', '# EXT_SOURCE_1,2,3', '# In[38]:', '# EXT_SOURCE_1 DAYS_BIRTH  .', '# In[39]:', '# TARGET  EXT_SOURCE_1,2,3 ', '# EXT_SOURCE_1,3 TARGET   . , TARGET    .', '# # bureau \xa0  ', '# bureau.csv: \xa0    \xa0     \xad  ', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# SK_ID_CURR  SK_ID_BUREAU    ', '# In[44]:', '#    \xa0.. SK_ID_BUREAU -> previous_loan_counts', '# In[45]:', '# In[46]:', '# ### seaborn kdeplot    ', '# ##### .iloc', '# integer position     . X', '# ##### .loc', '#  , integer positionX', '# In[47]:', '# In[48]:', '# In[49]:', '#    ..', '# ### Numeric  : Aggregation Numeric Columns', '# In[50]:', ""# bureau 'SK_ID_CURR' \xa0  . \xa0 'SK_ID_BUREAU' ."", '# In[51]:', '#   . -> levels[0]/ levels[1]', '# max, min    numeric type columns ', '# In[52]:', ""#    ' +level1'   "", '# In[53]:', '# In[54]:', '# In[55]:', 'bureau_agg.head() #    ', '# In[56]:', '# train ', '# In[57]:', '# In[58]:', '# \xad   corr ', '# In[59]:', '# In[60]:', '#    \xa0\xa0', '# In[61]:', ""#    'bureau_DAYS_CREDIT_mean'"", '# ### Function for Numeric Aggregations', '#   \xa0  :   ', '# In[62]:', '        # col group_var , SK_ID   drop', ""        numeric_df = df.select_dtypes('number') # dtype number  \xa0"", '# In[63]:', '# In[64]:', '#      \xa0  \xa0  \xa0  . ', '# ### Correlation Function', '#  \xa0   : corr', '# In[65]:', '# ### categorical  ', '# In[66]:', '#  1 SK_ID_CURR\xa0\xa0 loan_type 3 home 1 credit ...   ', '# In[67]:', '# In[68]:', '#   levels[0], levels[1] ', '# In[69]:', '# In[70]:', '#  \xa0   ', '# In[71]:', '# numerical  index SK_ID_CURR  right_index left_on .', '# In[72]:', '#  ', '# In[73]:', '# ### bureau_balance.csv ', '# \xa0 ', '# In[74]:', '# In[75]:', '# In[76]:', '#  \xa0  bureau_balance categorical     ', '# In[77]:', '# In[78]:', '# numeric  \xa0  ', '# In[79]:', '# In[80]:', '# bureau_balance_counts bureau_balance_agg', 'bureau_by_loan.head() # 16+6=22 ', '# In[81]:', '# In[82]:', '# ###  \xa0.', '# 1. bureau, bureau_balance    \xa0', '# 2.   train ', '# 3. missing value \xa0', '# 4. test data', '# 5. ', '# In[83]:', '# In[84]:', '# In[85]:', '# In[86]:', '# ## train ', '# In[87]:', '# In[88]:', '# In[89]:', '# In[90]:', '# In[91]:', '# ### Missing Values ', '# In[92]:', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[93]:', '# In[94]:', '# ### Test data ', '# In[95]:', '# Read in the test dataframe', '# Merge with the value counts of bureau', '# Merge with the stats of bureau', '# In[96]:', '# In[97]:', ""# Align the dataframes, this will remove the 'TARGET' column"", '# In[98]:', '# In[99]:', '# In[100]:', '# In[101]:', '# In[102]:', '# Drop the missing columns', '# In[103]:', '# In[104]:', '#  \xa0 ', '# In[105]:', '# In[106]:', '# In[107]:', '# In[108]:', '# In[109]:', '# In[110]:', '# In[111]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",226
blend-boosting-for-home-credit-default-risk (1).py,"['# coding: utf-8', '# ## an Empirical Blend Boosting Study for Home Credit Default Risk ', '# ### **(as my first notebook share on Kaggle)', '# ', '# During Jan-Apr of 2021 I have attended an intense bootcamp program focusing mainly on Data Science and Machine Learning. The bootcamp is organized by VBO (https://www.veribilimiokulu.com/), and I would like to thanks to the VBO Team (especially Mustafa Vahit Keskin, Attilla Yardimci and my group members).', '# ', '# The Home Credit Default Risk dataset on the Kaggle is subjected as final project of the bootcamp, and I have spent a period of three weeks on this project personally and also with my sub-team members. I developed various models and quite a large number of them having AUC scores better than 0.8 ( highest one 0.8034). Unfortunately, I could not run any full version of my models on Kaggle because of insufficient RAM issue even though datasets are zipped to almost 4 times by integer/float dtype conversion on my datasets.  ', '# ', '# After a while I recognize that some Kagglers had already shared their distinctly combined blending performances on a bunch of submitted results from their different type of models. Thanks to their inspiration about this approach, which is completely associate to boosting strategy: ""weaker predictors turns to be much stronger one"". Therefore, I titled this study as ""an Empirical Blend Boosting"" **(a quite unique naming :)**. Here I would like to share you my boosted blending performance based on my +20 results having better than 0.802 AUC score and other +30 results having AUC score between 0.802 and 0.800. However, I do not used yet the second set of my submission results because just ~25 of them is enough to achieve the best result on the Kaggle :), and also it is a quite boring stuff to try possible linear combinations. On the other hand, I can aggressively enhance my AUC score, but I could not convince myself yet that blending is a meaningful thing for data science philosophy.   ', '# ', '# Mostly I use Colab Pro to compute LigthGBM calculations with 5-fold CV on GPUs. My models have 900-1800 features. I am also developing a micro model having less than 200 features with a 0.800 AUC score (as a future mission for me). Soon I will share my micro model on the Kaggle, there should be no problem about RAM usage :-).  ', '# ', '# I have a limited knowledge about the credit finance, therefore, I combined many Kaggle notebooks for expending number of features as much as I desire and/or acceptance of my LigthGBM models harvesting further enhance scores. I would like to thank these contributors. ', '# ', '# Some of them are listed here (sorry for missings): ', '# * https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features <=-- my models are based on this study', '# * https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution', '# * https://www.kaggle.com/sangseoseo/oof-all-home-credit-default-risk <=-- in most cases these hyperparameters are used', '# * https://www.kaggle.com/ashishpatel26/different-basic-blends-possible <=-- thank for blending idea', '# * https://www.kaggle.com/mathchi/home-credit-risk-with-detailed-feature-engineering', '# * https://www.kaggle.com/windofdl/kernelf68f763785', '# * https://www.kaggle.com/meraxes10/lgbm-credit-default-prediction', '# * https://www.kaggle.com/luudactam/hc-v500', '# * https://www.kaggle.com/aantonova/aggregating-all-tables-in-one-dataset', '# * https://www.kaggle.com/wanakon/kernel24647bb75c', '# ', '# In[1]:', '# import libraries', '# load submitted 22 results having my best AUC scores better than 0.802 ', '# In[2]:', '# check correlation map between these 22 best AUC scores', '# #### From correlation map there it is clear that there are 3 subgroups. We should firstly scale these groups their inner side, and then among of themselves.    ', '# In[3]:', '# create a submission dataset', '# create a linear combination between 22 individual results ', '# coefficients determine empirically and based on correlational interactions ', '# In[4]:', '# create the submission file with my first blend result: 0.80602 AUC score', '# In[5]:', '# In[6]:', '# create the submission file with my first blend result: AUC score is better than 0.811']",41
bnu-dataming.py,"['# coding: utf-8', '# ### Home Credit Default Risk', '# ### BNU_Datamining', '# In[1]:', '# numpy and pandas for data manipulation', '# sklearn preprocessing for dealing with categorical variables', '# File system manangement', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[2]:', '# data', '# In[3]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# In[4]:', '# one-hot encoding of categorical variables', '# In[5]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# In[6]:', '# In[7]:', '# In[8]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[9]:', '# Make the model with the specified regularization parameter', '# Train on the training data', '# In[10]:', '# Make predictions', '# Make sure to select the second column only', '# In[11]:', '# Submission dataframe', '# In[12]:', '# Make the random forest classifier', '# In[13]:', '# In[14]:', '# Make a submission dataframe', '# Save the submission dataframe', '# In[15]:', '    # Extract the ids', '    # Extract the labels for training', '    # Remove the ids and target', '    # One Hot Encoding', '        # Align the dataframes by the columns', '        # No categorical indices to record', '    # Integer label encoding', '        # Create a label encoder', '        # List for storing categorical indices', '        # Iterate through each column', '                # Map the categorical features to integers', '                # Record the categorical indices', '    # Catch error if label encoding scheme is not valid', '    # Extract feature names', '    # Convert to np arrays', '    # Create the kfold object', '    # Empty array for feature importances', '    # Empty array for test predictions', '    # Empty array for out of fold validation predictions', '    # Lists for recording validation and training scores', '    # Iterate through each fold', '        # Training data for the fold', '        # Validation data for the fold', '        # Create the model', '        # Train the model', '        # Record the best iteration', '        # Record the feature importances', '        # Make predictions', '        # Record the out of fold predictions', '        # Record the best score', '        # Clean up memory', '    # Make the submission dataframe', '    # Make the feature importance dataframe', '    # Overall validation score', '    # Add the overall scores to the metrics', '    # Needed for creating dataframe of validation scores', '    # Dataframe of validation scores', '# In[16]:', '# In[17]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    # Set the yticks and labels', '    # Plot labeling', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', 'data_train = xgb.DMatrix(X_train, y_train)  # XGBoost', '# ', '# In[22]:', '# ', '# In[23]:', '# Make a submission dataframe', '# Save the submission dataframe', '# In[24]:']",106
catboostarter.py,[],0
catboost_comparison.py,"['    # # Add new features', '    # Amount loaned relative to salary', '    # Number of overall payments (I think!)', '    # Social features', '    # A lot of the continuous days variables have integers as missing value indicators.', '    # # Aggregate and merge supplementary datasets', '    # Previous applications', '    # Average the rest of the previous app data', '    # Max and min the previous app data', '    # Previous app categorical features', '    # Credit card data - numerical features', '    # Credit card data - categorical features', '    # Credit bureau data - numerical features', '    # Bureau balance data', '    # Pos cash data - weight values by recency when averaging', '    # Pos cash data data - categorical features', '    # Installments data', '    # Add more value counts', '    # Label encode categoricals', '# Merge the datasets into a single one for training', '# Separate metadata', '# Process the data set.', '# Capture other categorical features not as object data types:', '# Re-separate into train and test', '# Extract target', '# Define the categorical feats and CatBoost params', ""    # 'learning_rate': 0.1,"", '# Estimate LB score through CV', '# Fit final model']",29
cat_and_lgb_ensemble.py,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# print(os.listdir(""../input""))', '# print(catboost_result)', '# print(lgb_result)', '# Any results you write to the current directory are saved as output.']",11
classification-using-random-search-xgboost.py,"['# coding: utf-8', '# In[ ]:', '# Data manipulation', '# Modeling', '# Splitting data', '# In[ ]:', '# Sample 17000 rows (10000 for training, 7000 for testing)', '# Only numeric features', '# Extract the labels', '# Split into training and testing data', '# In[ ]:', '# In[ ]:', '# Create a training and testing dataset', '# In[ ]:', '# Get default hyperparameters', '# Remove the number of estimators because we set this to 10000 in the cv call', '# Cross validation with early stopping', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Optimal number of esimators found in cv', '# Train and make predicions with model', '# In[ ]:', '    # Number of estimators will be found using early stopping', '     # Perform n_folds cross validation', '    # results to retun', '# In[ ]:', '# In[ ]:', '# Create a default model', '# In[ ]:', '# Hyperparameter grid', '# In[ ]:', '# Randomly sample a boosting type', '# Set subsample depending on boosting type', '# In[ ]:', '# Learning rate histogram', '# In[ ]:', '# Check number of values in each category', '    # Check values', '# As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale.', '# In[ ]:', '# number of leaves domain', '# In[ ]:', '# Dataframes for random and grid search', '# In[ ]:', '# In[ ]:', '# In[ ]:', '    # Dataframe to store results', '    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists', '    # Iterate through every possible combination of hyperparameters', '        # Create a hyperparameter dictionary', '        # Set the subsample ratio accounting for boosting type', '        # Evalute the hyperparameters', '        # Normally would not limit iterations', '    # Sort with best score on top', '# In[ ]:', '# In[ ]:', '# Get the best parameters', '# Create, train, test model', '# In[ ]:', '# In[ ]:', '# Randomly sample from dictionary', '# Deal with subsample ratio', '# In[ ]:', '    # Dataframe for results', '    # Keep searching until reach max evaluations', '        # Choose random hyperparameters', '        # Evaluate randomly selected hyperparameters', '    # Sort with best score on top', '# In[ ]:', '# In[ ]:', '# Get the best parameters', '# Create, train, test model', '# In[ ]:', '# In[ ]:', '# Create file and open connection', '# Write column names', '# In[ ]:', '    # Dataframe for results', '        # Choose random hyperparameters', '        # Evaluate randomly selected hyperparameters', '        # open connection (append option) and write results', '        # make sure to close connection', '    # Sort with best score on top', '# In[ ]:', '    # Dataframe to store results', '    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists', '    # Iterate through every possible combination of hyperparameters', '        # Select the hyperparameters', '        # Set the subsample ratio accounting for boosting type', '        # Evalute the hyperparameters', '        # open connection (append option) and write results', '        # make sure to close connection', '        # Normally would not limit iterations', '    # Sort with best score on top', '# In[ ]:', '# MAX_EVALS = 1000', '# # Create file and open connection', ""# out_file = 'grid_search_trials_1000.csv'"", ""# of_connection = open(out_file, 'w')"", '# writer = csv.writer(of_connection)', '# # Write column names', ""# headers = ['score', 'hyperparameters', 'iteration']"", '# writer.writerow(headers)', '# of_connection.close()', '# grid_results = grid_search(param_grid, out_file)', '# # Create file and open connection', ""# out_file = 'random_search_trials_1000.csv'"", ""# of_connection = open(out_file, 'w')"", '# writer = csv.writer(of_connection)', '# # Write column names', ""# headers = ['score', 'hyperparameters', 'iteration']"", '# writer.writerow(headers)', '# of_connection.close()', '# random_results = random_search(param_grid, out_file)', '# In[ ]:', '# In[ ]:', '# Convert strings to dictionaries', '# In[ ]:', '    # Sort with best values on top', '    # Print out cross validation high score', '    # Use best hyperparameters to create a model', '    # Train and make predictions', '    # Create dataframe of hyperparameters', '    # Iterate through each set of hyperparameters that were evaluated', '    # Put the iteration and score in the hyperparameter dataframe', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Combine results into one dataframe', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Plot of scores over the course of searching', '# In[ ]:', '# In[ ]:', '# Create bar chart', '# Add text for labels', '# Display', '# In[ ]:', '# Bar plots of boosting type', '# In[ ]:', '# In[ ]:', '# Density plots of the learning rate distributions ', '# In[ ]:', '# Iterate through each hyperparameter', '        # Plot the random search distribution and the sampling distribution', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '# In[ ]:', '# Scatterplot of next four hyperparameters', '# ## Score versus Hyperparameters', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '# Scatterplot of next four hyperparameters', '# In[ ]:', '# Read in full dataset', '# Extract the test ids and train labels', '# In[ ]:', '# Cross validation with n_folds and early stopping', '# In[ ]:', '# In[ ]:', '# Train the model with the optimal number of estimators from early stopping', '# Predictions on the test data', '# In[ ]:', '# In[ ]:', '# Fit the models', '# In[ ]:', '# In[ ]:']",172
clean-manual-feature-engineering.py,"['# coding: utf-8', '# # Clean Manual Feature Engineering', '# ', '# The purpose of this notebook is to clean up the manual feature engineering I had scattered over several other kernels. We will implement the complete manual feature engineering and then test the results.', '# ', '# Update August 7: __After some modifications, this can now run in a kernel!__ The features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. ', '# ', '# ### Roadmap', '# ', '# Our plan of action is as follows.We have to be very careful about memory usage in the kernels, which affects the order of operations:', '# ', '# 1. Define functions:', '#     * `agg_numeric`', '#     * `agg_categorical`', '#     * `agg_child` ', '#     * `agg_grandchild`', '#  2. Add in domain knowledge features to `app`', '#  3. Work through the `bureau` and `bureau_balance` data', '#      * Add in hand built features', '#      * Aggregate both using the appropriate functions', '#      * Merge with `app` and delete the dataframes', '# 4. Work through `previous`, `installments`, `cash`, and `credit`', '#     * Add in hand built features', '#     * Aggregate using the appropriate functions', '#     * Merge with `app` and delete the dataframes', '# 5. Modeling using a Gradient Boosting Machine', '#     * Train model on training data using best hyperparameters from random search notebook', '#     * Make predictions and submit', '# ', '# ', '# ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Read in the datasets and replace the anomalous values', '# # Numeric Aggregation Function', '# ', '# The following function aggregates all the numeric variables in a child dataframe at the parent level. That is, for each parent, gather together (group) all of their children, and calculate the aggregations statistics across the children. The function also removes any columns that share the exact same values (which might happen using `count`). ', '# In[ ]:', '    # Remove id variables other than grouping variable', '    # Only want the numeric variables', '    # Group by the specified variable and calculate the statistics', '    # Need to create new column names', '    # Iterate through the variables names', '            # Iterate through the stat names', '                # Make a new column name for the variable and stat', '    # Remove the columns with all redundant values', '# # Categorical Aggregation Function', '# ', '# Much like the numerical aggregation function, the `agg_categorical` function works on a child dataframe to aggregate statistics at the parent level. This can work with any child of `app` and might even be extensible to other problems with only minor changes in syntax.', '# In[ ]:', '    # Select the categorical columns', '    # Make sure to put the identifying id on the column', '    # Groupby the group var and calculate the sum and mean', '    # Iterate through the columns in level 0', '        # Iterate through the stats in level 1', '            # Make a new column name', '    # Remove duplicate columns by values', '# # Combined Aggregation Function', '# ', '# We can put these steps together into a function that will handle a child dataframe. The function will take care of both the numeric and categorical variables and will return the result of merging the two dataframes. ', '# In[ ]:', '    # Numeric and then categorical', '    # Merge on the parent variable', '    # Remove any columns with duplicate values', '    # memory management', '# This function can be applied to both `bureau` and `previous` because these are direct children of `app`. For the children of the children, we will need to take an additional aggregation step. ', '# # Aggregate Grandchild Data Tables', '# ', '# Several of the tables (`bureau_balance, cash, credit_card`, and `installments`) are children of the child dataframes. In other words, these are grandchildren of the main `app` data table. To aggregate these tables, they must first be aggregated at the parent level (which is on a per loan basis) and then at the grandparent level (which is on the client basis). For example, in the `bureau_balance` dataframe, there is monthly information on the loans in `bureau`. To get this data into the `app` dataframe will first require grouping the monthly information for each loan and then grouping the loans for each client. ', '# ', ""# Hopefully, the nomenclature does not get too confusing, but here's a rounddown:"", '# ', '# * __grandchild__: the child of a child data table, for instance, `bureau_balance`. For every row in the child table, there can be multiple rows in the grandchild. ', '# * __parent__: the parent table of the grandchild that links the grandchild to the grandparent. For example, the `bureau` dataframe is the parent of the `bureau_balance` dataframe in this situation. `bureau` is in turn the child of the `app` dataframe. `bureau_balance` can be connected to `app` through `bureau`.', '# * __grandparent__: the parent of the parent of the grandchild, in this problem the `app` dataframe. The end goal is to aggregate the information in the grandchild into the grandparent. This will be done in two stages: first at the parent (loan) level and then at the grandparent (client) level', '# * __parent variable__: the variable linking the grandchild to the parent. For the `bureau` and `bureau_balance` data this is `SK_ID_BUREAU` which uniquely identifies each previous loan', '# * __grandparent variable__: the variable linking the parent to the grandparent. This is `SK_ID_CURR` which uniquely identifies each client in `app`.', '# ', '# ### Aggregating Grandchildren Function', '# ', '# We can take the individual steps required for aggregating a grandchild dataframe at the grandparent level in a function. These are:', '# ', '# 1. Aggregate the numeric variables at the parent (the loan, `SK_ID_BUREAU` or `SK_ID_PREV`) level.', '# 2. Merge with the parent of the grandchild to get the grandparent variable in the data (for example `SK_ID_CURR`)', '# 3. Aggregate the numeric variables at the grandparent (the client, `SK_ID_CURR`) level. ', '# 4. Aggregate the categorical variables at the parent level.', '# 5. Merge the aggregated data with the parent to get the grandparent variable', '# 6. Aggregate the categorical variables at the grandparent level', '# 7. Merge the numeric and categorical dataframes on the grandparent varible', '# 8. Remove the columns with all duplicated values.', '# 9. The resulting dataframe should now have one row for every grandparent (client) observation', '# 10. Merge with the main dataframe (`app`) on the grandparent variable (`SK_ID_CURR`). ', '# ', '# This function can be applied to __all 4 grandchildren__ without the need for hard-coding in specific variables. ', '# In[ ]:', '    # set the parent_var as the index of the parent_df for faster merges', '    # Aggregate the numeric variables at the parent level', '    # Merge to get the grandparent variable in the data', '    # Aggregate the numeric variables at the grandparent level', '    # Can only apply one-hot encoding to categorical variables', '        # Aggregate the categorical variables at the parent level', '        # Aggregate the categorical variables at the grandparent level', '    # If there are no categorical variables, then we only need the numeric aggregations', '    # Drop the columns with all duplicated values', '# # Putting it Together', '# ', '# Now that we have the individual pieces of semi-automated feature engineering, we need to put them together. There are two functions that can handle the children and the grandchildren data tables:', '# ', '# 1. `agg_child(df, parent_var, df_name)`: aggregate the numeric and categorical variables of a child dataframe at the parent level. For example, the `previous` dataframe is a child of the `app` dataframe that must be aggregated for each client. ', '# 2. `agg_grandchild(df, parent_df, parent_var, grandparent_var, df_name)`: aggregate the numeric and categorical variables of a grandchild dataframe at the grandparent level. For example, the `bureau_balance` dataframe is the grandchild of the `app` dataframe with `bureau` as the parent. ', '# ', '# For each of the children dataframes of `app`, (`previous` and `bureau`), we will use the first function and merge the result into the `app` on the parent variable, `SK_ID_CURR`. For the four grandchild dataframes, we will use the second function, which returns a single dataframe that can then be merged into app on `SK_ID_CURR`. ', '# ## Hand-Built Features', '# ', '# Along the way, we will add in hand-built features to the datasets. These have come from my own ideas (probably not very optimal) and from the community.', '# ', '# First we will add in ""domain knowledge"" features to the `app` dataframe. These were developed based on work done in other kernels (both from the community and my own work)', '# In[ ]:', '# Add domain features to base dataframe', '# ### Hand-Built Features for other Dataframes', '# ', '# We can also add in hand built features for the other dataframes. Since these are not the main dataframe, these features will end up being aggregated in different ways. These will be added as we go through the tables.', '# #### Aggregate the bureau data', '# ', '# First add the loan rate for previous loans at other institutions.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# #### Aggregate the bureau balance', '# ', '# Now we turn to the `bureau_balance` dataframe. We will make a column indicating whether a loan was past due for the month or whether the payment was on time.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Merge with the main dataframe', '# ', ""# The individual dataframes can all be merged into the main `app` dataframe. Merging is much quicker if done on any index, so it's good practice to first set the index to the variable on which we will merge. In each case, we use a `left` join so that all the observations in `app` are kept even if they are not present in the other dataframes (which occurs because not every client has previous records at Home Bureau or other credit institutions). After each step of mergning, we remove the dataframe from memory in order to hopefully let the kernel continue to run."", '# ', '# The final result is one dataframe with a single row for each client that can be used for training a machine learning model. ', '# In[ ]:', '# In[ ]:', '# #### Aggregate previous loans at Home Credit', '# ', '# We will add in two domain features, first the loan rate and then the difference between the amount applied for and the amount awarded.', '# In[ ]:', '# `AMT_DIFFERENCE` is the difference between what was given to the client and what the client requested on previous loans at Home Credit.', '# In[ ]:', '# In[ ]:', '# #### Aggregate Installments Data', '# ', '# The installments table has each installment (payment) for previous loans at Home Credit. We can create a column indicating whether or not a loan was late.', '# In[ ]:', '# `LOW_PAYMENT` represents a payment that was less than the prescribed amount. ', '# In[ ]:', '# In[ ]:', '# #### Aggregate Cash previous loans', '# ', '# The next dataframe is the `cash` which has monthly information on previous cash loans at Home Credit. We can create a column indicating if the loan was overdue for the month. ', '# In[ ]:', '# `INSTALLMENTS_PAID` is meant to represent the number of already paid (or I guess missed) installments by subtracting the future installments from the total installments.', '# In[ ]:', '# In[ ]:', '# #### Aggregate Credit previous loans', '# ', '# The last dataframe is `credit` which has previous credit card loans at Home Credit. We can make a column indicating whether the balance is greater than the credit limit, a column showing whether or not the balance was cleared (equal to 0), whether or not the payment was below the prescribed amount, and whether or not the payment was behind. Then we aggregate as with the other grandchildren.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# __This is usually the point at which the kernel fails.__ To try and alleviate the problem, I have added a pause of 10 minutes.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# __Update August 7__: The kernel can now run!', '# In[ ]:', '# Check for columns with duplicated values', '# _, idx = np.unique(app, axis = 1, return_index = True)', ""# print('There are {} columns with all duplicated values.'.format(app.shape[1] - len(idx)))"", '# In[ ]:', '# # Modeling', '# ', '# After all the hard work, now we get to test our features! We will use a model with the hyperparameters from random search that are documented in another notebook. ', '# ', '# The final model scores __0.792__ when uploaded to the competition.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Feature Importances', '# ', '# Now we can see if all that time was worth it! In the code below, we find the most important features and show them in a plot and dataframe.', '# In[ ]:', '# In[ ]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Bar plot of n most important features', '        # Cumulative importance plot', '        # Number of features needed for threshold cumulative importance', '        # Add vertical line to plot', '# In[ ]:', '# # Conclusions', '# ', '# This code is a little too much to run in the Kaggle kernels. However, the features themselves are available at https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features under `clean_manual.csv`. The feature importances for these features in a gradient boosting model are also available at the same link with the name `fi_clean_manual.csv`. ', '# ', ""# This notebook is meant to serve as a clean version of the manual feature engineering I had scattered across several other notebooks. We were able to build a complete set of __ features that scored 0.792 on the public leaderboard__. Further hyperparameter tuning might improve the performance. For additional feature engineering, we will probably want to turn to more technical operations such as treating this as a time-series problem. Since we have relative time information (relative to the current loan at Home Credit), it's possible to find the most recent information and also trends over time. These can be useful because changes in behavior might inform us as to whether or not a client will be able to repay a loan! "", '# ', ""# Thanks for reading and as always, I welcome feedback and constructive criticism. I'll see you in the next notebook."", '# ', '# Best,', '# ', '# Will', '# In[ ]:']",214
credit-skeleton.py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[ ]:', '# In[ ]:', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '        # For best performance 10000 estimators, use fewer to speed up prediction time', '    # Write submission file and plot feature importance', '# In[ ]:', '# In[ ]:', '# In[ ]:']",21
creditriskpipeline.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# **BUREAU**', '# In[8]:', '# Read in bureau', '# **Bureau Balance**', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# **MERGE WITH BUREAU**', '# In[13]:', '# **MERGE WITH app_train**', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# **Pipelines**', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# dummies', '# ordinal encoder', '# standard scaler', '# Polynomial Features', '# In[23]:', '# column transformer', '# In[24]:', '# In[25]:', '# In[26]:', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# pipeline_modelo = Pipeline(steps=[', '#     (""preprocessamento"", classe_pipeline.preprocessamento),', '#     (""modelo"", classe_pipeline.modelo)', '# ])', '# pipeline_modelo.predict(X_teste)', '# In[32]:', '# In[33]:', '# In[34]:']",51
cse445.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '    # Calculate rate for each category with decay', '    # Min, Max, Count and mean duration of payments (months)', '# In[24]:', '# In[25]:', '# In[26]:', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '    # Engineered features', '    # Engineered features', '    # Engineered features', '    # Engineered features', '    # Engineered features', '    # The following features are only for approved applications', '    # Engineered features', '# In[32]:', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# In[44]:', '# In[45]:', '# In[46]:', '# In[47]:', '# In[48]:', '# In[49]:', '# In[50]:', '# In[51]:', '# In[52]:', '# In[53]:', '# In[54]:', '# In[55]:', '# In[56]:', '# In[57]:', '# Ref: https://pranaysite.netlify.app/lightgbm/', '    # Extract the ids', '    # Extract the labels for training', '    # Remove the ids and target', '    # One Hot Encoding', '        # Align the dataframes by the columns', '        # No categorical indices to record', '    # Integer label encoding', '        # Create a label encoder', '        # List for storing categorical indices', '        # Iterate through each column', '                # Map the categorical features to integers', '                # Record the categorical indices', '    # Catch error if label encoding scheme is not valid', '    # Extract feature names', '    # Convert to np arrays', '    # Create the kfold object', '    # Empty array for feature importances', '    # Empty array for test predictions', '    # Empty array for out of fold validation predictions', '    # Lists for recording validation and training scores', '    # Iterate through each fold', '        # Training data for the fold', '        # Validation data for the fold', '        # Create the model', '        # Train the model', '        # Record the best iteration', '        # Record the feature importances', '        # Make predictions', '        # Record the out of fold predictions', '        # Record the best score', '        # Clean up memory', '    # Make the submission dataframe', '    # Make the feature importance dataframe', '    # Overall validation score', '    # Add the overall scores to the metrics', '    # Needed for creating dataframe of validation scores', '    # Dataframe of validation scores', '# In[58]:', '# In[59]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    # Set the yticks and labels', '    # Plot labeling', '# In[60]:', '# In[61]:', '# In[62]:', '# In[63]:', '# In[64]:', '# In[65]:']",119
dic-as-13.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# In[3]:', '# In[4]:', '# Clear value with missing value > 0', '# Remove the FLAG_DOCUMENT_* columns and SK_ID_CURR (since its represent the ID No. only)', '# In[5]:', '# Plot the target proportion', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# Get the correlation', '# In[11]:', '# In[12]:', '# Extract the data', '# In[13]:', '# In[14]:', '# Get the name of selected features', '# In[15]:', '# In[16]:', '# Normalized the data', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[24]:', '# Predict with application_train.csv', '# In[25]:', '# In[26]:', '# Normalized the data', '# In[27]:', '# In[28]:', '# In[29]:', '# In[ ]:']",49
dic-as-14.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# In[3]:', '# In[4]:', '# Select 2 variables as in the pre-assignment', '# In[5]:', '# In[6]:', '# Normalized the data', '# In[7]:', '# Choose LogisticRegression with C = 0.0001 for estimating results.', '# In[8]:', '# Predict with application_train.csv', '# In[9]:', '# In[10]:', '# Normalized the data', '# In[11]:', '# In[12]:', '# In[13]:', '# In[ ]:']",29
different-basic-blends-possible (1).py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# ## Blend with one rank weighted submission [0.8 LB]', '# In[ ]:', '# In[ ]:', '# Function for merging dataframes efficiently ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Diversified blend [0.803 LB]', '# ', '# ', '# **The blending ingredients are taken from three different type of models.**', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Blending lowest correlated models', '# In[ ]:', 'print(Corr_Mat) # Correlation matrix of five submission files', '# In[ ]:', '# In[ ]:', '# **This is my first try towards blending. Do upvote if you like it and also if you have any ideas do share in the comment section below.**', '# ', '# **To be Continued**', '# In[ ]:']",35
different-basic-blends-possible (2).py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# ## Blend with one rank weighted submission [0.8 LB]', '# In[ ]:', '# In[ ]:', '# Function for merging dataframes efficiently ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Diversified blend [0.799 LB]', '# ', '# ', '# **The blending ingredients are taken from three different type of models.**', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Blending lowest correlated models', '# In[ ]:', 'print(Corr_Mat) # Correlation matrix of five submission files', '# In[ ]:', '# In[ ]:', '# **This is my first try towards blending. Do upvote if you like it and also if you have any ideas do share in the comment section below.**', '# ', '# **To be Continued**', '# In[ ]:']",35
eda-and-feature-engineering-for-beginner.py,"['# coding: utf-8', '# ## EDA Feature-engineering    ', '# ', '# #### reference', '# * Home Credit Default Risk - A Gentle Introduction ', '# ', '# https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction ', '# https://bkshin.tistory.com/entry/-5-Home-Credit-Default-Risk', '# ', '# ###  ', '#  \xa0 \xa0   \xa0        \xa0   ', '# (supervised classification task)', '# > * 0   , 1  \xa0', '# *     ,     application_train, application_test \xa0 \xa0', '# ### Metric : ROC AUC', '# AUC-ROC \xa0       \xa0 ', '# * ROC = \xa0      ', '# * AUC(Area Under the Curve) = ROC\xa0\xad', '#     *   AUC 1 \xa0,     ', '#     * AUC  0.5,      \xa0 \xa0  ', '#     ', '#     > AUC', '#       * AUC=0.7,        \xa0   \xa0 70%', '#     ', '# #### ROC-curve ', '# * True positive rates(=recall, sensitivity)', '#     * TPR = R = TP / (TP+FN)', '#         * \xa0    \xa0  \xa0', '# * True negative rates(=specificity)', '#     * TNR = TN / (TN+FP)', '#         * \xa0 \xa0   \xa0\xa0  \xa0', '# * ROC-curve x, y', '#     * y : TPR(=Recall)', '#     * x : 1-(TNR)', '# ', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# numpy and pandas for data manipulation', '# sklearn preprocessing for dealing with categorical variables', '# File system manangement', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# List files available', '# In[4]:', '# Training data', '# In[5]:', '# Testing data features', '# --------------------------------------------------------', '# ## Exploratory Data Analysis', '# ## 1) Target   ', '# Target   . 0 \xa0   , 1  \xa0 . ', '# In[6]:', '# In[7]:', '# *  \xa0   0 1   imbalanced data.', '# ## 2)  ', '#       ', '# In[8]:', '    # \xa0   ', '    #   ', '    #   ,    ', '    #  ', '    #  0  \xa0\xa0 \xa0\xa0', '    #   print', '# In[9]:', '# * \xa0  \xa0,   .(imputation)', '# *  imputation     XGBoost  \xa0.', '# *        \xad\xa0\xa0,      \xa0   \xa0 \xa0.', '# ## 3) Column Types', '# int64, float64  \xa0 object   ', '# In[10]:', '# In[11]:', '#   \xa0   ', ""# app_train.select_dtypes('object').apply(pd.Series.nunique)"", ""app_train.select_dtypes('object').nunique()   # apply  "", '# *    \xa0  \xa0 .', '#     * ORGANIZATION_TYPE  OCCUPATION_TYPE  ', '# ## 4)   Encoding', '# LightGBM  \xa0\xa0  \xa0       ,    encode .     ', '# * 1) Label encoding :', '#     *      .    ', '#     * /      Label encoding  ,   One-hot encoding  ', '# * 2) One-hot encoding :', '#     *   \xa0     \xa0\xa0    1 \xa0   0  ', '#     * One-hot encoding    \xa0   ', '#         *    PCA   \xa0', '#         ', '# >    \xa0  2 Label encoding \xa0    One-hot encoding \xa0.', '# ### Label Encoding and One-Hot Encoding', '# * LabelEncoder(), get_dummies() ', '# In[12]:', '#  iterate ', '        #  object\xa0    ,', '            # train test    train fit train,test  transform', '            # train-set, test-set  Transform', '            #     ', '# In[13]:', '#  Label-encoding\xa0     One-hot encoding \xa0', '# ## 5) Train Test  ', '# train  test   feature  . ', '# train   \xa0 \xa0   test   \xa0  \xa0      one-hot-encoding , train  test   .', '# ', '#  test  \xa0 train   \xad\xa0. ', '# > \xa0, train  TARGET  . ', '#     * TARGET  test  train    ', '# ', '# > align()  join inner \xa0    .', '# #### Python align() ', '# *   \xa0     \xa0   \xa0   /      \xa0 ', '# In[14]:', '# ', '# In[15]:', '# In[16]:', '# In[17]:', '#  \xa0   D,B,A  ', '# In[18]:', '# TARGET train     \xa0 \xa0', '# TARGET  ', '# ## 6) Back to Exploratory Data Analysis', '# ', '# ### 6-1)  (Anomalies)', '# *  \xa0     describe()   .', '# In[19]:', '# DAYS_BIRTH   ', '# In[20]:', '# DAYS_EMPLOYED ..', '# In[21]:', '#   \xa0       \xa0     ', '# ', '# In[22]:', '# ', '#  ', '# *   \xa0  \xa0 \xa0 5.4%  .', '# *    \xa0     ', '# *   \xa0   \xa0 ,    .', '# *  , \xa0      \xa0.', '# ', '# > \xa0\xa0', '#     *    \xa0,  boolean     \xa0.', '# In[23]:', '# Create an anomalous flag column', '#  nan ', '# In[24]:', '# test  train   ', '# True, False   sum True  .', '# ### 6-2) Correlations', '# \xa0 \xa0  outlier . ', '#     , \xa0 target  . ', '# .corr()  , \xa0 target Pearson  .', '# In[25]:', '# TARGET  ', '# * DAYS_BIRTH     .   ,    \xa0 . ', '#     *   \xa0    \xa0  \xa0?    DAYS_BIRTH  \xa0  . , DAYS_BIRTH \xa0    \xa0\xa0 .', '# ### 6-3) Effect of Age on Repayment', '# In[26]:', '# DAYS_BIRTH \xa0 TARGET ', '# * \xa0   TARGET   \xa0  ,  \xa0 \xa0  \xa0 .', '# In[27]:', '# \xa0   \xa0  ', '# *    outlier  \xa0  . ', '# * \xa0  TARGET     KDE plot \xa0.', '# ', '# > KDE plot  \xa0', '# *   \xa0  \xa0  .  \xa0   \xa0    \xa0  \xa0    .', '# *      \xa0(KDE)   .', '#     * \xa0   \xa0  ', '# In[28]:', '# \xa0   \xa0  plot (TARGET=0)', '# \xa0   \xa0  plot (TARGET=1)', '# * target==1()    20-30     .  \xa0    \xa0 \xa0 \xa0 \xa0\xa0  .', '# * target==0 1 TARGET      \xa0  \xa0 \xa0  .', '# * \xa0     \xa0 target=1(  \xa0)  \xa0 .', '# In[29]:', '#  20  70  10 ', '# In[30]:', '# Bin the age data', '# In[31]:', '# Group by the bin and calculate averages', '# In[32]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# * \xa0     ', '# * 20-25, 25-30 30-35  10%    \xa0, 55-60, 60-65, 65-70 5%   .', '# ### 6-4) Exterior Sources', '# *     3  EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 .', '#     *    \xa0 \xa0 score .', '# * , \xa0 TARGET  EXT_SOURCE  EXT_SOURCE   ', '# In[33]:', '# In[34]:', '# * EXT_SOURCE TARGET    , EXT_SOURCE \xa0     .', '# * , DAYS_BIRTH  EXT_SOURCE_1       score  \xa0  \xa0.', '# *    EXT_SOURCE  TARGET  \xa0  .', '# In[35]:', '# In[36]:', '# iterate through the sources', '# * EXT_SOURCE_3  target     .', '# * target     , target 0 1         \xa0 \xa0  .', '# ### 6-5) Pairs Plot', '# * EXT_SOURCE  DAYS_BIRTH  pair plot \xa0. ', '# * pair plot     ,      .', '# In[37]:', '# Copy the data for plotting', '# \xa0   ', '#  drop', '#       ', '# Create the pairgrid object', '#   \xad \xa0', '# \xa0 \xa0', '#   density plot', '# *      ,     .', '# * EXT_SOURCE_1 YEARS_BIRTH   \xa0 .', '# ', '# ## 7) Feature Engineering', '# *     feature ,   \xa0,      feature engineering  .', '#      feature engineering .', '# ', '# ### 7-1) Polynomial Features', '# > \xa0    \xa0, \xa0   \xa0  \xa0  . \xa0 dataset feature \xa0 \xad \xa0  Gradient Descent  \xa0   .', '# ', '# *  EXT_SOURCE_1 \xa0 EXT_SOURCE_2 \xa0 , \xa0 EXT_SOURCE_1 x EXT_SOURCE_2  EXT_SOURCE_1 x EXT_SOURCE_2^2         .  \xad \xa0 .', '# ', '# *    target     ,     target  \xa0  .', '# ', '# * \xad      .  \xa0    .    \xad \xa0   .', '# ', '# *   EXT_SOURCE, DAYS_BIRTH   polynomial feature .', '# ', '# In[38]:', '# Make a new dataframe for polynomial features', '#    imputer ', '# target  \xa0', '# target \xa0   \xa0 ', '#  impute  (train   fit\xa0, train test  transform \xa0)', '# In[39]:', '# imputer \xa0 ', '# In[40]:', '# In[41]:', '# Create the polynomial object with specified degree', '# In[42]:', '# Train the polynomial features (train  fit)', '# Transform the features', '# * get_feature_names   \xad \xa0  ', '# In[43]:', '# * 35 feature  .\xa0   Feature target   ', '# In[44]:', '# Create a dataframe of the features ', '# drop TARGET  ', '# TARGET  ', '# Display most negative and most positive', '# * \xad         .', '# (   TARGET     EXT_SOURCE_3  -0.18,   EXT_SOURCE_2 EXT_SOURCE_3    -0.19 )', '# ', '# * \xa0           .', '# ', '# In[45]:', '# Put test features into dataframe', '#  train   \xa0 \xad merge   ', '#  test   \xa0 \xad merge   ', '# Align the dataframes => train  align ', '# Print out the new shapes', '# ### 7-2) Domain Knowledge Features', ""# * CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income"", ""# * ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income"", '# * CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due', ""# * DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age"", '# In[46]:', '# train   ', '# In[47]:', '# test   ', '# * domain  \xa0  TARGET   KDE plot \xa0', '# In[48]:', '# * Target=0 1       feature \xa0\xa0 ', '# ### Baseline', '# -    \xa0 \xa0 \xa0 .    \xa0\xa0 test set \xa0  0.5\xa0 \xa0. \xa0 AUC ROC 0.5 .', '# ## 8) Logistic Regression Implementation', '# - \xa0 categorical  encoding  \xa0. \xa0  imputation \xa0,  normalizing\xa0. ', '# In[49]:', '# training  TARGET  drop -> TARGET   Scaling', '# ', '# testing  ', '#  median ', '#   0~1    MinMaxScaler ', '# training  fit', '# training testing  transform', '# Scaling', '# - \xa0 LogisticRegression ,  \xa0 regularization  C    . ', '# - \xa0 log_reg  model \xa0, .fit()   \xa0\xa0, .predict_proba()  testing data   \xa0.', '# In[50]:', '# Make the model with the specified regularization parameter', '# Train on the training data', '# -   mx2  (m  ),  target 0 \xa0\xa0   target 1 \xa0.(,    1)', '# -      \xa0 \xa0, target=1 \xa0   \xa0.', '# In[51]:', '#     ', '# In[52]:', '# Make predictions', '#   \xa0', '# In[53]:', '# submission   SK_ID_CURR  TARGET  ', '# In[54]:', '# submission  csv file \xa0', '# * LogisticRegression Score : 0.67887', '# ## 9) Improved Model: Random Forest', ""# * \xa0 \xa0 \xa0 \xa0  \xa0  . \xa0 '' .((Bagging) bootstrap aggregating )"", '# * \xa0 \xa0   \xa0        .', '# *  \xa0         \xa0    \xa0 , \xa0   \xa0   \xa0 \xa0.', '# ', '# ', '# ###  ', '# > **n_estimators**', '#     * \xa0  \xa0 (Default=10)', '#     * \xa0   \xa0    .   ', '# > **random_state**', '#     *    random_state \xa0\xa0     ', '# > **verbose**', '#     *  \xa0 \xa0 ', '# > **n_jobs**', '#     * \xa0   \xa0 \xa0  ', '#     * n_jobs=-1 \xa0  \xa0  ', '# In[55]:', '# In[56]:', '# training data \xa0', '# feature importances ', '# test   ', '# In[57]:', '# \xa0 dataframe', '# csv  \xa0', '# * RandomForest Score :', '# ### 9-1) Feature engineering  ', '# * (\xa0)  train  \xad  app_train_poly , \xad  poly_features ', '# In[58]:', '# Impute the polynomial features', '# poly_features \xad  ', '# app_train_poly  train  \xad  ', '# Scale the polynomial features', '# In[59]:', '# training data \xa0', '# test ', '# In[60]:', '# * Random Forest engineered Score :0.60467', '# ### 9-2) Domain \xa0 feature ', '# In[61]:', '# In[62]:', '# TARGET\xa0', '#  \xa0   ', '#  ', '# imputer \xa0 DataFrame  array ', '# \xa0  ', '# \xa0', '#   ', '# test   TARGET=0 \xa0    TARGET=1 \xa0    ', '# In[63]:', '# * Random Forest domain features :0.68354', '# ## 10) Model Interpretation: Feature Importances', '# ', '# *    \xa0      \xa0 feature importances  . EDA\xa0    EXT_SOURCE  DAYS_BIRTH   \xa0 .', '# *   feature importances   .', '# In[64]:', '    #    ', '    #  \xa0      ', '    # ', '    # Set the yticks and labels', '    # Plot labeling', '# In[65]:', '# In[66]:', '# feature engineering     ', '# *   EXT_SOURCE  DAYS_BIRTH    .', '# * feature importances  \xa0     \xa0 \xa0  ,  \xa0    \xa0\xa0   .', '# In[67]:', '# In[68]:', '# domain \xa0     ', '# *   \xa0 4    top15     . ', '# > \xa0\xa0,  \xa0    score  .', '# In[ ]:', '# In[ ]:']",384
fan-hcd-submission.py,"['# coding: utf-8', '# ## Imports', '# In[1]:', '# numpy and pandas for data manipulation', '# sklearn preprocessing for dealing with categorical variables', '# File system manangement', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# ## Read in Data ', '# ', '# In[2]:', '# List files available', '# In[3]:', '# Training data', '# The training data has 307511 observations (each one a separate loan) and 122 features (variables) including the `TARGET` (the label we want to predict).', '# In[4]:', '# Testing data features', '# The test set is considerably smaller and lacks a `TARGET` column. ', '# ## Examine the Distribution of the Target Column', '# ', '# The target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.', '# In[5]:', '# In[6]:', '# From this plot, we can see it is an imbalanced class problem.', '# ## Examine Missing Values', '# ', '# Next we can look at the number and percentage of missing values in each column. ', '# In[7]:', ""# Let's now look at the number of unique entries in each of the `object` (categorical) columns."", '# In[8]:', '# Number of unique classes in each object column', '# ### Label Encoding and One-Hot Encoding', '# ', '# For any categorical variable (`dtype == object`) with 2 unique categories, use label encoding, and for any categorical variable with more than 2 unique categories, use one-hot encoding. ', '# ', '# For label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function.', '# In[9]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# In[10]:', '# one-hot encoding of categorical variables', '# ### Aligning Training and Testing Data', '# ', '# In[11]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# ### Find outlier for column""DAYS_BIRTH"" and ""DAYS_EMPLOYED""', '# ', '# ', '# ', '# In[12]:', '# In[13]:', '# In[14]:', '# Check the anomalous clients to see they have higher or low rates of default than the rest of the clients.', '# In[15]:', '# Fill in the anomalous values with not a number (`np.nan`) in training and test dataset, then create a new boolean column indicating whether or not the value was anomalous.', '# ', '# ', '# In[16]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# Check values distribution after repalcementin in train dataset', '# In[17]:', '# ### Find correlations', '# ', '# ', '# ', '# In[18]:', '# Find correlations with the target and sort', '# Display correlations', '# `DAYS_BIRTH` (the age in days of the client at the time of the loan in negative days) is the most positive correlation, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). ', '# ### Effect of Age on Repayment', '# In[19]:', '# Find the correlation of the positive days since birth and target', '# In[20]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[21]:', '# Group by the bin and calculate averages', '# In[22]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. ', '# ## Domain Knowledge Features', '# ', '# ', '# ', '# In[23]:', '# In[24]:', '# In[25]:', '# Print out the new shapes', '# ### Read in bureau and Bureau Balance', '# In[26]:', '# In[27]:', '# ### Function for Numeric Aggregations', '# In[28]:', '    # Remove id variables other than grouping variable', '    # Group by the specified variable and calculate the statistics', '    # Need to create new column names', '    # Iterate through the variables names', '        # Skip the grouping variable', '            # Iterate through the stat names', '                # Make a new column name for the variable and stat', '# ### Function to Handle Categorical Variables', '# In[29]:', '    # Select the categorical columns', '    # Make sure to put the identifying id on the column', '    # Groupby the group var and calculate the sum and mean', '    # Iterate through the columns in level 0', '        # Iterate through the stats in level 1', '            # Make a new column name', '# In[30]:', '# Calculate value count statistics for each `SK_ID_CURR` in bureau', '# In[31]:', '# Counts of bureau', '# In[32]:', '# Calculate value count statistics for each `SK_ID_CURR` in butrau_balance', '# In[33]:', '# Counts of each type of status for each previous loan', '# ### Aggregated Stats of Bureau Balance by loan', '# In[34]:', '# Dataframe grouped by the loan', '# Merge to include the SK_ID_CURR', '# ### Aggregated Stats of Bureau Balance by Client', '# In[35]:', '# Merge to include the SK_ID_CURR', '# Aggregate the stats for each client', '# ### Insert Computed Features into app_train_domain Data', '# In[36]:', '# Merge with the value counts of bureau', '# Merge with the stats of bureau', '# Merge with the monthly information grouped by client', '# In[37]:', '# ### Calculate Information for test_domain Data', '# In[38]:', '# Merge with the value counts of bureau', '# Merge with the stats of bureau', '# Merge with the value counts of bureau balance', '# In[39]:', '# ### Align the testing and training dataframes', '# In[40]:', ""# Align the dataframes, this will remove the 'TARGET' column"", '# In[41]:', '# In[42]:', '# Free up memory by deleting old objects', '# ### Logistic Regression Implementation', '# In[43]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[44]:', '# Make the model with the specified regularization parameter', '# Train on the training data', '# In[45]:', '# Make predictions', '# In[46]:', '# Submission dataframe', '# In[47]:', '# Save the submission to a csv file', '# ##### This model scores 0.6837.', '# ##  Random Forest with Domain Features', '# ', '# In[48]:', '# Impute the domainnomial features', '# Scale the domainnomial features', '# Train on the training data', '# Extract feature importances', '# Make predictions on the test data', '# In[49]:', '# Make a submission dataframe', '# Save the submission dataframe', '# ##### This model scores 0.6781.', '# ### Light Gradient Boosting Machine', '# In[50]:', '    # Extract the ids', '    # Extract the labels for training', '    # Remove the ids and target', '    # One Hot Encoding', '        # Align the dataframes by the columns', '        # No categorical indices to record', '    # Integer label encoding', '        # Create a label encoder', '        # List for storing categorical indices', '        # Iterate through each column', '                # Map the categorical features to integers', '                # Record the categorical indices', '    # Catch error if label encoding scheme is not valid', '    # Extract feature names', '    # Convert to np arrays', '    # Create the kfold object', '    # Empty array for feature importances', '    # Empty array for test predictions', '    # Empty array for out of fold validation predictions', '    # Lists for recording validation and training scores', '    # Iterate through each fold', '        # Training data for the fold', '        # Validation data for the fold', '        # Create the model', '        # Train the model', '        # Record the best iteration', '        # Record the feature importances', '        # Make predictions', '        # Record the out of fold predictions', '        # Record the best score', '        # Clean up memory', '    # Make the submission dataframe', '    # Make the feature importance dataframe', '    # Overall validation score', '    # Add the overall scores to the metrics', '    # Needed for creating dataframe of validation scores', '    # Dataframe of validation scores', '# In[51]:', '# In[52]:', '# This submission scores 0.7628. ']",223
features.py,"['# coding: utf-8', '# # Features', '# In[ ]:', '# bb feature', '# bureau', ""bureau['ADJ_DAYS'] = (bureau.DAYS_CREDIT - bureau.DAYS_CREDIT.min()) / (bureau.DAYS_CREDIT.max() - bureau.DAYS_CREDIT.min()) + 0.5 # more recent, more effecitve"", '# application count', ""bur_act_count = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['SK_ID_BUREAU'].count() # fillna: 0"", ""bur_bad_count = bureau.loc[bureau.CREDIT_ACTIVE=='Bad debt'].groupby('SK_ID_CURR')['SK_ID_BUREAU'].count() # fillna: 0"", ""bur_sold_count = bureau.loc[bureau.CREDIT_ACTIVE=='Sold out'].groupby('SK_ID_CURR')['SK_ID_BUREAU'].count() # fillna: 0"", '# application date', '# application itervel', '# overdue days', '# overdue amount', ""bur_total_max_overdue_adj = bureau.groupby('SK_ID_CURR')['ADJ_AMT_CREDIT_MAX_OVERDUE'].sum() # use adj days"", '# adj prelong', ""bur_avg_prelonged = bureau.groupby('SK_ID_CURR')['ADJ_CNT_CREDIT_PROLONG'].mean().fillna(0) # use adj days"", '# historical amount', '# current amount', ""bur_active_total_amount = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum() # fillna 0"", ""bur_active_avg_amount = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].mean() # fillna 0"", ""bur_active_total_debt = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].sum() # fillna 0"", ""bur_active_avg_debt = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].mean() # fillna 0"", ""bur_active_total_limit  = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].sum() # fillna 0"", ""bur_active_avg_limit  = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_LIMIT'].mean() # fillna 0"", ""bur_active_total_overdue = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].sum() # fillna 0"", ""bur_active_avg_overdue = bureau.loc[bureau.CREDIT_ACTIVE=='Active'].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].mean() # fillna 0"", 'bur_active_ratio_debt_credit = (bur_active_total_debt / bur_active_total_amount.map(lambda x: x+0.1)) # fillna 0', 'bur_active_ratio_overdue_debt = (bur_active_total_overdue / bur_active_total_debt.map(lambda x: x+0.1)) # fillna 0', '# credit update', '# annuity', ""bur_avg_annuity = bureau.groupby('SK_ID_CURR')['AMT_ANNUITY'].mean() # can't fillna 0"", ""bur_total_annuity = bureau.groupby('SK_ID_CURR')['AMT_ANNUITY'].sum() # can't fillna 0"", ""bur_avg_term = bureau.loc[bureau.term < float('inf')].groupby('SK_ID_CURR')['term'].mean() # can't fillna 0"", '# In[ ]:', '# count cards', '# INSTALMENTS', '# limit', '# avg drawing amount', '# count Refused', '# current credit card situation', ""ccb_cur_total_limit = last_month_credit.loc[last_month_credit.NAME_CONTRACT_STATUS == 'Active'].groupby('SK_ID_CURR')['AMT_CREDIT_LIMIT_ACTUAL'].sum() # fillna: 0"", '# drawing in 1y', '# drawing in 6m', '# DPD', '# In[ ]:', '# recent application', '# recent late & less time ', '# recent late & less amount', '# previous application times', '# credit card', '# change times', '# avg instl', '# total late & less time ', '# total late & less amount', '# total payment', '# total late & less time in recent 1 year', '# payment 1 year', '# avg instl', '# total late & less amount', '# total payment', '# total late & less time in recent 6 months', '# avg instl', '# total late & less amount', '# total payment', '# active account', '# In[ ]:', '# times of INSTALMENT change', '# avg INSTALMENT', '# active INSTALMENT', '# DPD', '# INSTALMENT: ratio of end status of each type', '# In[ ]:', '# cat', '# selected version', '# num', '# create new variables', '# compare application and actual credit', '# adjusted: more recent, more important', '# down payment', '# goods price', '# down payment rate', '# selling area', '# term', '# recent decision', '# application interval', '# days', '# In[ ]:', '# # Derivatives', '# In[ ]:', '# data manipulation', '# time', '# Replace all the day outliers', '# bureau', '# Create the date columns', '# balance', '# Make a date column', '# Convert to timedeltas in days', '# Make date columns', '# Drop the time offset columns', '# # cash', '# # credit', '# installment', '# Make an entityset', '# part 1', '# part 2', '# Relationship between app and bureau', '# Test Relationship between app and bureau', '# Relationship between bureau and bureau balance', '# Relationship between current app and previous apps', '# Test Relationship between current app and previous apps', '# Relationships between previous apps and cash, installments, and credit', '# Add in the defined relationships', '# train features', '# test features', '# # Merge', '# In[4]:', '# bureau_feature_df = pd.read_csv(""../input/features/bureau_feature.csv"")', '# ccb_feature_df = pd.read_csv(""../input/features/ccb_feature.csv"")', '# ip_feature_df = pd.read_csv(""../input/features/ip_feature.csv"")', '# pcb_feature_df = pd.read_csv(""../input/features/pcb_feature.csv"")', '# pa_feature_df = pd.read_csv(""../input/features/pa_feature.csv"")', '# bur_cluster_df = pd.read_csv(""../input/features/bur_cluster.csv"")', ""# full_df = full_df.merge(bureau_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# full_df = full_df.merge(ccb_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# full_df = full_df.merge(ip_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# full_df = full_df.merge(pcb_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# full_df = full_df.merge(pa_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# full_df = full_df.merge(bur_cluster_df, on = 'SK_ID_CURR', how = 'left')"", ""# test_df = test_df.merge(bureau_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# test_df = test_df.merge(ccb_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# test_df = test_df.merge(ip_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# test_df = test_df.merge(pcb_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# test_df = test_df.merge(pa_feature_df, on = 'SK_ID_CURR', how = 'left')"", ""# test_df = test_df.merge(bur_cluster_df, on = 'SK_ID_CURR', how = 'left')"", '# del bureau_feature_df', '# del ccb_feature_df', '# del ip_feature_df', '# del pcb_feature_df ', '# del pa_feature_df ', '# del bur_cluster_df ', '# # One-Hot', '# ', '# In[ ]:', '# one-hot encoding of categorical variables', '# # Impute', '# ', '# In[ ]:', ""# list(full_df.drop(columns=['TARGET']).columns)"", '# In[ ]:', '# import numpy as np', '# from sklearn.preprocessing import StandardScaler', '# from sklearn.impute import SimpleImputer', '# full_df = full_df.replace([np.inf, -np.inf], np.nan)', '# test_df = test_df.replace([np.inf, -np.inf], np.nan)', '# test_df.head(10)', '# # Feature names', '# features = list(full_df.columns)', '# # Median imputation of missing values', ""# imputer = SimpleImputer(strategy = 'median')"", '# # Scale each feature to 0-1', '# scaler = StandardScaler()', '# # Fit on the training data', '# imputer.fit(full_df)', '# # Transform both training and testing data', '# train = imputer.transform(full_df)', '# test = imputer.transform(test_df)', '# # Repeat with the scaler', '# scaler.fit(train)', '# train = scaler.transform(train)', '# test = scaler.transform(test)', ""# print('Training data shape: ', train.shape)"", ""# print('Testing data shape: ', test.shape)"", '# full_df = pd.DataFrame(data=train, columns=full_df.columns)', '# test_df = pd.DataFrame(data=test, columns=test_df.columns)', '# In[ ]:', '# In[ ]:', '# # Train', '# In[5]:', '    # Extract the ids', '    # Extract the labels for training', '    # Remove the ids and target', '    # One Hot Encoding', '        # Align the dataframes by the columns', '        # No categorical indices to record', '    # Integer label encoding', '        # Create a label encoder', '        # List for storing categorical indices', '        # Iterate through each column', '                # Map the categorical features to integers', '                # Record the categorical indices', '    # Catch error if label encoding scheme is not valid', '    # Extract feature names', '    # Convert to np arrays', '    # Create the kfold object', '    # Empty array for feature importances', '    # Empty array for test predictions', '    # Empty array for out of fold validation predictions', '    # Lists for recording validation and training scores', '    # Iterate through each fold', '        # Training data for the fold', '        # Validation data for the fold', '        # Create the model', '        # Train the model', '        # Record the best iteration', '        # Record the feature importances', '        # Make predictions', '        # Record the out of fold predictions', '        # Record the best score', '        # Clean up memory', '    # Make the submission dataframe', '    # Make the feature importance dataframe', '    # Overall validation score', '    # Add the overall scores to the metrics', '    # Needed for creating dataframe of validation scores', '    # Dataframe of validation scores', '# # Predict', '# In[7]:']",218
first_model_simple_logistic_regression.R,[],0
fork-of-kernel-20200311.py,"['# coding: utf-8', '# # Home Credit Default Risk', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# \xad', '# display(objct_cols)', '# print(objct_cols_list)', '# \xad\xa0', '# print(df[objct_cols_list].isnull().sum())', '# ', ""    label_mean = df.groupby(col).TARGET.mean() # groupby()'label'"", '    df[col] = df[col].map(label_mean).copy() # df[C]', '    test[col] = test[col].map(label_mean).copy() # test ', '# df[objct_cols_list]', '# \xa0', 'df = df.dropna() # \xa0', '# df = df.fillna(df.mean()) # \xa0', '# df = df.fillna(df.median()) # \xa0\xad', '# print(df.isnull().sum())', '# print(df.shape)', '# ', '# \xad', 'select = SelectPercentile(percentile=4) # select 1% features', 'df_selected = select.transform(df) # type:dataFrame->ndarray', '# ndarray', '# ', '# X_train.shape', '# ', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:']",36
fork-of-simple-features-0064d3.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', ' #  n_estimators=1327, n_jobs=-1, num_leaves=106, objective=None,random_state=None, reg_alpha=0.5129992714397862, reg_lambda=0.38268769901820565, silent=True, subsample=0.7177561548329953, subsample_for_bin=80000,', '  #     subsample_freq=0, verbose=1)', '# In[7]:', '# In[8]:', '# Save the submission dataframe']",12
fork_lightgbm_with_simple_features.py,"['# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features', '# From Kaggler : https://www.kaggle.com/jsaguiar', '# Just added a few features so I thought I had to make release it as well...', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '    # Some simple new features (percentages)', ""    # df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']"", ""    # df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']"", ""    # df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']"", ""    # df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']"", ""    # df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']"", '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '            # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance']",47
fork_of_fork_lightgbm_with_simple_features (1).py,"['# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features', '# From Kaggler : https://www.kaggle.com/jsaguiar', '# Just removed a few min, max features. U can see the CV is not good. Dont believe in LB.', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",42
fork_of_fork_lightgbm_with_simple_features.py,"['# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",39
fork_of_fork_lightgbm_with_simple_features_cee847.py,"['# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",39
fork_of_magic_of_weighted_average_rank_0_80 (1).py,"['# USING WEIGHTED AVERAGE RANK METHOD', '# Plese refer this discussion for more detials - https://www.kaggle.com/c/home-credit-default-risk/discussion/60934']",2
fork_tidy_xgb_all_tables_lightgbm_0_792.R,[],0
getting_rid_of_ext_source_variables.R,[],0
giba_post_processing_user_id_boost (1).R,[],0
good-transformations-to-continuous-variables.r,[],0
good_fun.py,"[""# buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '        # n_estimators=1000,', '        # num_leaves=20,', '        # colsample_bytree=.8,', '        # subsample=.8,', '        # max_depth=7,', '        # reg_alpha=.1,', '        # reg_lambda=.1,', '        # min_split_gain=.01', '    #    pos = pd.Series(trn_y == 1)', '    #    # Add positive examples', '    #    trn_x = pd.concat([trn_x, trn_x.loc[pos]], axis=0)', '    #    trn_y = pd.concat([trn_y, trn_y.loc[pos]], axis=0)', '    #    # Shuffle data', '    #    idx = np.arange(len(trn_x))', '    #    np.random.shuffle(idx)', '    #    trn_x = trn_x.iloc[idx]', '    #    trn_y = trn_y.iloc[idx]', '# Plot feature importances', '# Plot ROC curves', '    # Plot the roc curve', '# Plot ROC curves']",22
good_fun_lightgbm_in_depth.py,"[""# buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '        # n_estimators=1000,', '        # num_leaves=20,', '        # colsample_bytree=.8,', '        # subsample=.8,', '        # max_depth=7,', '        # reg_alpha=.1,', '        # reg_lambda=.1,', '        # min_split_gain=.01', '# Plot feature importances', '# Plot ROC curves', '    # Plot the roc curve', '# Plot ROC curves']",13
good_fun_opti_params_updatedjun4_prev_ratios.py,"[""# buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '        # n_estimators=1000,', '        # num_leaves=20,', '        # colsample_bytree=.8,', '        # subsample=.8,', '        # max_depth=7,', '        # reg_alpha=.1,', '        # reg_lambda=.1,', '        # min_split_gain=.01', '    #    pos = pd.Series(trn_y == 1)', '    #    # Add positive examples', '    #    trn_x = pd.concat([trn_x, trn_x.loc[pos]], axis=0)', '    #    trn_y = pd.concat([trn_y, trn_y.loc[pos]], axis=0)', '    #    # Shuffle data', '    #    idx = np.arange(len(trn_x))', '    #    np.random.shuffle(idx)', '    #    trn_x = trn_x.iloc[idx]', '    #    trn_y = trn_y.iloc[idx]', '# Plot feature importances', '# Plot ROC curves', '    # Plot the roc curve', '# Plot ROC curves']",22
good_fun_with_ligthgbm (1).py,"['        # n_estimators=10000,', '        # num_leaves=30,', '        # colsample_bytree=.8,', '        # subsample=.9,', '        # max_depth=7,', '        # reg_alpha=.1,', '        # reg_lambda=.1,', '        # min_split_gain=.01', '# Plot feature importances']",9
good_fun_with_ligthgbm (2).py,"[""# buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '        # n_estimators=1000,', '        # num_leaves=20,', '        # colsample_bytree=.8,', '        # subsample=.8,', '        # max_depth=7,', '        # reg_alpha=.1,', '        # reg_lambda=.1,', '        # min_split_gain=.01']",9
good_fun_with_ligthgbm (3).py,"[""    # buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '    # Plot feature importances', '    # Plot ROC curves', '        # Plot the roc curve', '    # Plot ROC curves', '        # Plot the roc curve', '    # Build model inputs', '    # Create Folds', '    # Train model and get oof and test predictions', '    # Save test predictions', '    # Display a few graphs']",11
good_fun_with_ligthgbm.py,['# Plot feature importances'],1
hc-default-risk-application-data-0-70.r,"['# One - Hot encoding', '# One - Hot encoding']",2
hc-v500.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '#                                                                .agg({""returns"": [np.min, np.max,np.mean]})\\', '#                                                                .reset_index()', ""#    tmp_merge = df[['SK_ID_CURR']]"", ""#    tmp_merge = tmp_merge.merge(tmp, on=['SK_ID_CURR'], how='left')"", ""#    df['min_amt_annuity_v1'] = tmp_merge['des1']"", ""#    df['max_amt_annuity_v1'] = tmp_merge['des2']"", ""#    df['mean_amt_annuity_v1'] = tmp_merge['des3']"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Bureau', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Installments', '# ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""             # 'min_amt_app_fail_v1','max_amt_app_fail_v1','mean_amt_app_fail_v1',"", ""             # 'min_amt_card_fail_v1','max_amt_card_fail_v1','mean_amt_card_fail_v1',"", ""             # 'min_limit_bureau_v1','max_limit_bureau_v1','mean_limit_bureau_v1',"", ""              # 'min_later_install','max_later_install','mean_later_install'"", ""             # 'min_first2_install','max_first2_install','mean_first2_install'"", ""             # 'min_num_version_install','max_num_version_install','mean_num_version_install'"", '# In[ ]:', '    #    continue', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",113
hc-v600.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '#                                                                .agg({""returns"": [np.min, np.max,np.mean]})\\', '#                                                                .reset_index()', ""#    tmp_merge = df[['SK_ID_CURR']]"", ""#    tmp_merge = tmp_merge.merge(tmp, on=['SK_ID_CURR'], how='left')"", ""#    df['min_amt_annuity_v1'] = tmp_merge['des1']"", ""#    df['max_amt_annuity_v1'] = tmp_merge['des2']"", ""#    df['mean_amt_annuity_v1'] = tmp_merge['des3']"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Bureau', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Installments', '# ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""             # 'min_amt_app_fail_v1','max_amt_app_fail_v1','mean_amt_app_fail_v1',"", ""             # 'min_amt_card_fail_v1','max_amt_card_fail_v1','mean_amt_card_fail_v1',"", ""             # 'min_limit_bureau_v1','max_limit_bureau_v1','mean_limit_bureau_v1',"", ""              # 'min_later_install','max_later_install','mean_later_install'"", ""             # 'min_first2_install','max_first2_install','mean_first2_install'"", ""             # 'min_num_version_install','max_num_version_install','mean_num_version_install'"", '# In[ ]:', '    #    continue', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",113
hcdr-single-model-private-score-0-79167-catboost.py,"['# coding: utf-8', '# Please upvote if you find this Notebook useful', '# ', '# My Learnings and mistakes in this competition', '# Learnings', '# All Learning points are covered in this notebook', '# ', '#     Handling large number of features', '#         ', '#             Had around 1000 features', '#             Identification of most valuable features', '#             Feature Engineering and Feature selection are most important aspects of Machine Learning', '#             Good Feature Engineering and Feature selection super seeds superior hardware', '#             Different set of features perform better with different algorithms. ', '#         ', '#     Combining different tables together and preparing them for model fitting', '#         ', '#             Tables have well defined relationships for creating joins', '#             Data from supporting tables is grouped before joining it with final tables', '#             Grouping process is a very good application of groupby aggregation functionality ', '#         ', '#     Extensive use of Groupby and aggregation on supporting table', '#     Manual feature engineering', '#     Dropping highly correlated features.', '#    Categorization of observations can help improving score. ', '#     Feature selection and feature exclusion. Check following Notebooks for Feature selection and exclusion:', '#         ', '#        https://www.kaggle.com/rahullalu/hcdr-installments-table-feature-selection', '#         https://www.kaggle.com/rahullalu/hcdr-feature-selection-for-pos-table', '#        https://www.kaggle.com/rahullalu/home-credit-default-risk-preparing-bureau-data', '#        https://www.kaggle.com/rahullalu/hcdr-feature-selection-for-creditcard-table', '#         ', '#      Ensembling: With Private score of 0.79134', '#     ', '#  ', '# Mistakes', '# Mistake of not exploring all available Boosting models. Biggest mistake :(', '# ', '#     Extensively worked with XGBoost and LGBM. But not worked extensively with CatBoost.', '#     Got best result with CatBoost. Realized pretty late.', '#     Thought LGBM is the best model. Got CV score of 0.7869.', '#     Realized this mistake on last day. So was not able to submit best score with CatBoost.', ""#     Didn't worked extensively on ensemble. Ensemble techinques helped a lot in improving scores."", '#     ', '# ', '# ![](http://)Problem Statement', '# The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task:', '# ', '# **Supervised:** The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features', '# **Classification:** The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)', '# ', '# ', '# ', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Dataset with file size', '# In[ ]:', '# In[ ]:', '    # Find index of feature columns with correlation greater than 0.97', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# with pd.option_context('display.max_columns',prev_appl_f.shape[1]):"", '#     display(prev_appl_f.head(10))', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",167
hcdrparseforlibraries.py,"['# imports', '    # create a new data frame', '    # used for keping track of dataframe index', '    # I use this line instead of the one after to only look at 10 files at a time when just testing code', ""    # for filename in os.listdir('/Users/nadiadomnina/Desktop/REU_data/titanic/html_titanic/sub-group'):"", '    # /Users/nadiadomnina/Desktop/REU_data/titanic/titanic_html', '        # finds the file names and libraries', '        # finds all function calls for each file', '       # find_functions(df, filename, i)', '   # df.to_csv(""libaries_functions.csv"", index=False)', '    # open the file', '        # create an array for the libraries in this document', '        # line scanner', '            # split the line into an array by spaces', '            # if the needed word  ""import"" is in the line', '                # case 2,3: ""from x import y""', '                    # get the library', '                # case 1: ""import x""', '                    # get the library name (next word)', '                    # add this word to the libraries array', '        # get only the main library name (only save the x in a x.1 type of import)', '        # count the number of libraries', '        # create a row with the gathered data', '        # add the row to our data frame']",24
home-cred-default-risk.py,"['# coding: utf-8', '# **Introduction**', '# This notebook is divided into 3 parts:', '# ', '# first part ', '# ', '# second part', '# ', '# third part', '# ', '# ', '# **first part**', '# ', '# In this section, predictions based on the target for the application_test file will be made, based on predictors from the application_test file. Descriptions of each of the 2 files will also be executes. Likewise, the correlation between target and days of registration will be shown along with relationship between education type and days of registration. ', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[ ]:', ""main_file_path1 = '../input/application_train.csv' # this is the path to the training data that you will use"", '# In[ ]:', '# In[ ]:', ""main_file_path2 = '../input/application_test.csv' # this is the path to the test data that you will use"", '# In[ ]:', '# seaborn plot shows that for a target, days of DAYS_REGISTRATION must be greater that- 20000', '# In[ ]:', '# box plot shows that on average, higher DAYS_REGISTRATION is due to the the fact that the customer could possibly have a higher education', '# In[ ]:', '# In[ ]:', '# In[ ]:', 'from sklearn.tree import DecisionTreeRegressor # used to make predictions from certain data', '# In[ ]:', '# **second part**', '# ', '# In this section, the categorical variable NAME_EDUCATION_TYPE is used to predict the TARGET using encoding. Likewise, the 3D correlation between DAYS_CREDIT_UPDATE and DAYS_CREDIT is revealed through the plotly surface.', '# ', '# Similarly, this section also reveals the common credit types according to the days of credit using plotnine.', '# In[ ]:', ""main_file_path3 = '../input/bureau.csv' # this is the path to the test data that you will use"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# **third part**', '# ', '# Contains even more data visualisations and predictions', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# KDE plot (what plotly refers to as a Histogram2dContour) and scatter plot of the same data.', '# In[ ]:', '# In[ ]:', '# In[ ]:']",59
home-credit (1).py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# ### Creating combined dataframe from train and test file', '# - Purpuse of combining train and test file is to handle data modification at same time on both file', '# - Once data pre-processing is done we can easily split it again with below logic', '# - if TARGET=NaN meaning its test file else its train file', '# In[6]:', '# In[7]:', '# In[8]:', '# ### Considering basic numeric features', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# - Creating dataframe with required columns only', '# In[13]:', '# In[14]:', '# ## EDA And Pre-Processing ', '# In[15]:', '# In[16]:', '# ### Handling missing values', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[ ]:', '# In[24]:', '# In[25]:', '# In[26]:', '# Heatmap', '# - Name Type Suite and Occupation type has missing values', '# - Occupation type has lots of missing value so for now droping this column', '# - Name Type suite will create some dummy NTS_XNA category for now', '# In[ ]:', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# - Draw distribution of numeric features', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[44]:', '# In[45]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# - Handling Outlier', '# In[46]:', '# In[47]:', '# In[48]:', '# In[49]:', '# - found that DAYS_EMPLOYED has some anomalies', ""# - Around 18% of data amongs all data has some '365243' value in this fields"", '# - as its not make sence to current data so we need to handle it somehow', '# - so i am replacing this value with np.nan', '# - creating new column called DAYS_EMPLOYED_ANOM Anomalous flag which will have True or False value based on this field', '# In[50]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# After removing anomalies we can see above histogram that DAYS_EMPLOYED has maximum as 49 years and minimum is 0 year as discribe below', '# ### creating combined basic features from numerical and categorical', '# In[51]:', '# In[52]:', '# In[53]:', '# In[ ]:', '# In[54]:', '# In[55]:', '# In[56]:', '# In[57]:', '# In[58]:', '# In[59]:', '# In[60]:', '# In[61]:', '# ### Lable encoding for categorical features whose values are binary like Y/N, Yes/No, True/False, M/F etc.', '# In[62]:', '# In[63]:', '# Categorical features with Binary encode (0 or 1; two categories)', '# In[64]:', '# out of above basic categorical features we already encoded binary ', '# - FLAG_OWN_CAR', '# - FLAG_OWN_REALITY', '# - CODE_GENDER', '# - DAYS_EMPLYED_ANOM', '# ', '# Now doing one hot encoding for remaining features', '# - NAME_CONTRACT_TYPE', '# - NAME_TYPE_SUITE', '# - NAME_INCOME_TYPE', '# - NAME_EDUCATION_TYPE', '# - NAME_FAMILY_STATUS', '# - NAME_HOUSING_TYPE', '# - ORGANIZATION_TYPE', '# In[65]:', '# In[66]:', '# In[67]:', '# In[68]:', '# In[69]:', '# In[70]:', '# In[71]:', '# In[72]:', '# In[73]:', '# ### creating final dataframe with required features', '# In[74]:', '# In[75]:', '# In[76]:', '# In[77]:', '# In[78]:', '# ## Model 1 : Logistic Regression', '# In[79]:', '# Make the model with the specified regularization parameter', '# In[80]:', '# In[81]:', '# In[82]:', '# In[83]:', '# In[84]:', '# In[85]:', '# In[86]:', '# In[87]:', '# In[88]:', '# Train on the training data', '# In[89]:', '# Make predictions', '# Make sure to select the second column only', '# In[90]:', '# In[91]:', '# In[92]:', '# ## Dealing with Imbalance Data using SMOTE', '# In[93]:', '# In[94]:', '# In[95]:', '# In[96]:', '# In[97]:', '# In[ ]:', '# In[ ]:']",153
home-credit-default-analysis.py,"['# coding: utf-8', '# ## Home Credit Default Risk', '# ##### Can you predict how capable each applicant is of repaying a loan?', '# #### Overview ', '# This project was inspired by that fact that many people who deserves loan do not get it and ends up in the hands of untrustworthy lenders.', '# This project is a competition from Kaggle. Below is the link: [Kaggle | Home Credit Default Risk Competition](https://www.kaggle.com/c/home-credit-default-risk)', '# ', '# ', '# Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.', '# ', '# ![about homecredit](https://storage.googleapis.com/kaggle-media/competitions/home-credit/about-us-home-credit.jpg)       [Source : Kaggle](https://storage.googleapis.com/kaggle-media/competitions/home-credit/about-us-home-credit.jpg)', '# ', '# Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a', '#  ', ""# variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities."", ""# While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful."", '# ', '# ## Problem Statement.', '# ### Can you predict how capable each applicant is of repaying a loan ?', '# - My analysis will be predicting how capable each applicant is at repaying a loan.', '# ### Datasets and Inputs.', '# The dataset for this project has been provided by Kaggle. ', '# Data description is below :', '# There are 7 different sources of data:', '# + **application_train/application_test**: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating **0** if the loan was repaid **`Repayers`** and **`1`** for default **`Defaulters`**', ""# + **bureau**: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits."", '# + **bureau_balance**: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.', '# + **previous_application**: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.', '# + **POS_CASH_BALANCE:** monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.', '# + **credit_card_balance**: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.', '# + **installments_payment**: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.', ""# For more information on what each data represents, please read the [PROPOSAL]('/Users/bhetey/version_control/machine-learning/projects/capstone/proposal.pdf'), or [Kaggle](https://www.kaggle.com/c/home-credit-default-risk) "", '# - Below is a diagram of how the data are connected. ', '# ![Data structure](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)', '# ### Loading Data ', '# In[ ]:', 'import pandas as pd # this is to import the pandas module', 'import numpy as np # importing the numpy module ', 'import os # file system management ', 'import zipfile # module to read ZIP archive files.', '# Figures inline and set visualization style', '# In[ ]:', '# reading the data with pandas ', '# In[ ]:', '# Training data has 307511 rows (*each one represents separate loan*) and 112 featurees (columns) including the TARGET(What is to be predicted)', '# In[ ]:', 'y = app_train.TARGET # y is going to be our target variable', '# In[ ]:', '# The Testing set does not have target variable ', '# In[ ]:', '# traing and testing set do not have the same shape. ', '# ### Data conversion in pandas DataFrame', '# In[ ]:', '# converted all csv in pandas dataframe. ', '# HomeCredit Columns Description gives us the details about each features in the dataset ', '# In[ ]:', '# this is done for indexing of the joint data later ', '# ### Visual Exploratory Data Analysis (EDA)', '# In[ ]:', '# + ### How many people repay loans : ', '# **Take away here is that**: Looking at the picture below, **`1`** for **Defaulter** and **`0`** for  **Repayers**. The image below shows that most applicant pay back the loan. This is what we called [Imbalanced Class Problem](http://www.chioka.in/class-imbalance-problem/). The differences between Repayer and Defaulter is too big ', '# In[ ]:', '# + ### What is the family status of the applicant:', '# In the image, it is shown that more married candidates pay back thier loans', '# In[ ]:', '# + ### What is the Income Class and Family type that default the most :   ', '# **The Takeaway:** Most married and working class mostly default on loan payment ', '# In[ ]:', '                       # filter the train set by using TARGET column == 1', '# Checking for missing values.', '# In[ ]:', '# below is a function to check for missing values. ', '    # checking total missing values', '    # percentage of missing values. ', ""    # table of total_miss_values and it's percentage"", '    # columns renamed', '    # descending table sort ', '    # display information ', '# In[ ]:', '# **Take away :** For some machine learning models, we have to deal with the missing values buy imputing or dropping either the roles or the columns with the highest percentage of missing values. However we might be loosing some data from them. We also do not know if the data removed will harm the analysis or help it ahead of time until we experiment on them. ', '# Algorithmns like **XGBoost** can handle missing data without imputation. [It automatically learn how to deal with missing data point.](https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/)', '# + [Additonal reading](https://arxiv.org/abs/1603.02754)', '# Dealing with features ', '# **Obviously we have 3 data types :** Numeric and Non-numeric (e.g Text ) called _object_.', '# ', '# Numeric can be of discrete time or continuous time horizon. ', '# Non_numeric are [variables containing label values rather numeric values.](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) They are sometimes called [nominal](https://en.wikipedia.org/wiki/Nominal_category)', '# In[ ]:', 'app_train.get_dtype_counts() # Shows the numbers of types of values ', '# Looking at the dataset with object type, below is the total number of object. However since we want to work with them we will need to hot encode them. ', '# ', '# However this depends on personal view. it depend on how big the categorical variables are. ', '# ', '# One of the major problems with categorical data is that only few machine learning alogorithms works with them without any special form of implementation while others needs some implementation where the data needs to be encoded into numeric variables. ', '# ', '# How to convert categorical data into numerical data: ', '# + **Integer Encoding** _where integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship._ ', '# ', '# ', '# + **One-Hot Encoding** _where the integer encoded variable is removed and a new binary variable is added for each unique integer value._', '# ', '# [Read more](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)', '# In[ ]:', '# Checking the number of unique class in each object column ', '# In[ ]:', '# **.Describe** enerates descriptive statistics that summarize the central tendency, dispersion and shape of a datasets distribution, excluding NaN values.', '# ', '# DAYS_BIRTH was originally in days and now it will be converted to years. The columns has negative as they were recorded relative to the current loan application ', '# In[ ]:', '# Looking at the result above everything seems okay. Cannot seems to find any outlier in this analysis ', ""# **DAYS_EMPOYED:** How many days before the application the person started current employment'"", '# ', '# This is also relative to the current loan application ', '# In[ ]:', '# In[ ]:', '# Looking at the image above, 1000 years does not seem right. ', '# We will use imputation to solve this. ', '# ### Checking correlation of the data.', '# It helps to show possible relationship within our data. ', '# This article helps in interpreting [correlation](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf), [How to interpret a Correlation Coefficient](https://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/)', '# In[ ]:', '# In[ ]:', '# #### ONE-HOT ENCODING ', ""# Let's **One-hot Encode** the categorical variable "", '# We need to import the module from scikit-learn library', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Looking at the analysis above is obvious that **One-Hot Encoding** has added extra features to the original ones we have hereby leaving our data unaligned. ', '# ', '# We need to have same features in both the training and testing data for our machine learning model to work if not we will get error when running the algorithm. ', '# ', '# **STEPS TAKING:** ', '# + I decided to remove any column that is present on the training set but not on our testing set. ', '# + Intuitively the **y** which is our **TARGET** is expected to  be removed as well but will add it back', '# In[ ]:', '# In[ ]:', ""one_hot_encoded_app_train['TARGET'] = y # adding it back to the data "", '# In[ ]:', '# dropping the target to get our X ', '# **Removing missing values**', '# In[ ]:', '# function is to drop the missing values ', '# In[ ]:', '# In[ ]:', '# Looking at the dataset now after removing the **NaN** in the data, we have the columns reduced to **181 columns**', '# **Imputing Nan values**', '# In[ ]:', '# making a copy for the data before imputing ', 'new_y = y.copy() # a copy of our target ', '# dropping the target columns before imputing ', '# calling imputer and transforming the data', '# ## EVALUATION METRICS ', '# There are different kind of evaluation metrics we can used since this is a **Classification problem.** ', '# Below are some of the metrics : ', '# ', '# + Classification accuracy ', '# + Logarithmic Loss ', '# + Area Under ROC Curve ', '# + Confusion Matrix ', '# + Classification report', '# ', '# [**Read more about them**](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)', '# ### CLASSIFICATION MODELS ', '# Classification depends on whether the variables we are trying to predict are **Binary or Non-Binary**.', '# ', '# **Binary variables** are those variables where the outcome we are looking are either 1 or 0, True or False. ', '# ', '# **Non-Binary variables** are those variables where the outcome we are looking are categorical. for example looking at the dataset and predicting where the color of the dress of a person will be `Yellow, Brown or Blue`', '# ', '# #### Binary Classification Model : ', '# + Logistic regression ', '# + Decision Trees ', '# + Support Vector Machine (SVM) : _good for anomaly detection especially in large feature sets_', '# ', '# #### Non- Binary Classification Model: ', '# + Adaboost ', '# + Random Forest ', '# + Decision Tree', '# + Neural Networks ', '# ', '# #### Considering choosing an algorithm, : ', '# + Take note of the accuracy ', '# + Training time ', '# + Linearity ', '# + Number of parameters ', '# + Number of features ', '# ', '# [The Machine Learning Algorithm Cheat Sheet](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)', '# ', '# However, I am going to try each of them and compare their performance.', '# ### Logistic Regression Model ', '# This is my first model. ', '# C is used to control overfitting and a small tends to reduce overfitting ', '# Logistic regression using the data set with dropped missing values. ', '# #### Model Evaluation using a validation set', '# In[ ]:', '# Import statements ', '# instantiate a logistic regression model, and fit with X and y', '# evaluate the model by splitting into train and test sets', '# In[ ]:', '# #### Model Evaluation Using Cross-Validation', ""# Now let's try 10-fold cross-validation, to see if the accuracy holds up more rigorously."", '# In[ ]:', '# evaluate the model using 10-fold cross-validation', '# In[ ]:', '# In[ ]:', '# In[ ]:']",211
home-credit-default-risk (1).py,"['# coding: utf-8', '# In[1]:', '# ># Data prepare', '# ## read original dataset', '# In[2]:', '# ## choose features (first selection, based on missing data)', '# In[3]:', '# In[4]:', '# ## check feature dtypes', '# In[5]:', '# ## encoder category feature (sub-dataset)', '# In[6]:', '# ## groupby, aggregrate and merge dataset', '# In[7]:', '# ## choose feature and objectives (based on missing data)', '# In[8]:', '# ># Catboost (for analysing feature importance)', '# ## devide features and labels', '# In[9]:', '# ## splite train and validation dataset', '# In[10]:', '# ## check label distribution', '# In[11]:', '# ## weights list and cat features index with filling missing data ', '# In[12]:', '# ## build and train a model', '# In[13]:', '# ## evaluation a model', '# In[14]:', '# ## analysis feature importance', '# In[15]:', '# In[16]:', '# ># neural network', '# ## missing data', '# In[17]:', '# ## devide features and label', '# In[18]:', '# ## encode features (main table)', '# In[19]:', '# ## standardize data', '# In[20]:', '# ## transform useless features to PCA features', '# In[21]:', '# ## combine PCA features and usefull features', '# In[22]:', '# ## split training and validation dataset', '# In[23]:', '# ## train deep learning model', '# In[24]:', '# In[25]:', '# In[26]:', '# ## evaluate model', '# In[27]:', '# In[28]:', '# In[29]:', '# ># save model', '# In[30]:', '# > ## import saved model', '# In[31]:', '# ># predict test dataset', '# ## compare test dataset with training dataset', '# In[32]:', '# ## predict values', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:']",67
home-credit-default-risk (2).py,"['# coding: utf-8', '# # Introduction', ""# This notebook is based on Will Koehrsen's kernel titled [Start Here: A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction). This kernel is for practice only, to learn about Kaggle competition first hand."", '# # Imports', '# In[1]:', '# numpy and pandas for data manipulation ', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# sklearn preprocessing for dealing with categorical variables', '# File system management', '# Suppress warnings', '# matplotlib.and seaborn for visualization', '# # Read in Data', '# In[2]:', '# List available files', '# In[3]:', '# Read training data', '# The training data has 307511 observations (each one a separate loan) and 122 features, including the TARGET label (which we want to predict).', '# In[4]:', '# Read testing data', '# # Exploratory Data Analysis', '# ', '# ## Examine the Distribution of the Target Column', '# In[5]:', '# In[6]:', '# Imbalanced class problem: there are far more repaid loan than loans that were not repaid.', '# ## Examine Missing Values', '# In[7]:', '    # Total missing values:', '    # Missing values in percent:', '    # Make a table with the results', '    # Rename the columns', '    # Sort the table by percentage of missing, descending', '    # Print some summary information', '# In[8]:', '# ## Column Types', '# In[9]:', '# Number of each type of column', '# In[10]:', '# Number of unique entries in each object column', '# Most object-type columns have small number of unique entries.', '# ## Encoding Categorical Variables', '# ### Label Encoding', '# For any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding.', '# In[11]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# ### One-Hot Encoding', '# In[12]:', '# In[13]:', '# Align training and test data', '# ## Back to EDA', '# ### Anomalies', '# Use the `describe` method to find anomalies.', '# In[14]:', '# DAYS_BIRTH column was negative because it was recorded against current loan application. Multiply by -1 and divide by 365.25 to get the number of years.', '# In[15]:', '# In[16]:', '# In[17]:', '# Minimum employed is *minus* 1000 years!', '# In[18]:', '# Anayze the default rate for anomalies:', '# In[19]:', '# The anomalies have a lower level of default. They might have some importance.', '# Solution: fill the anomalies with `np.nan` and then create a new boolean column to indicate the anomaly.', '# In[20]:', '# Create an anomalous flag olumn', '# Replace the anomalous values with NaN', '# Do the same with test data', '# In[21]:', '# ### Correlations', '# In[22]:', '# Find correlations with the target and sort', '# Display correlations', '# As the client gets older, the repayment level is better, but the correlation is kinda weak.', '# In[23]:', '# Convert DAYS_BIRTH column to absolute value', '# In[24]:', '# To visualize the effect of the age on the target, use kernel density estimation plot (KDE).', '# In[25]:', '# KDE plot on loans that were repaid on time', '# KDE plot on loans which were not repaid on time', '# Labeling of plot', '# In[26]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[27]:', '# Group by the bind and calculate averages', '# In[28]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# ### Exterior Sources', '# In[29]:', '# Extract the EXT_SOURCE variables and show correlations', '# In[30]:', '# Heatmap of correlation', '# # Feature Engineering', '# ## Polynomial Features', '# In[31]:', '# Make a new datafrane for polynomial features', '# Imputer for handling missing values', '# Create the polynomial object with specified degreee', '# In[32]:', '# Train the polynomial', '# Transform the features', '# In[33]:', '# In[34]:', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# In[35]:', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# ## Domain Knowledge Features', '# ', ""# * `CREDIT_INCOME_PERCENT`: the percentage of the credit amount relative to a client's income"", ""# * `ANNUITY_INCOME_PERCENT`: the percentage of the loan annuity relative to a client's income"", '# * `CREDIT_TERM`: the length of the payment in months (since the annuity is the monthly amount due', ""# * `DAYS_EMPLOYED_PERCENT`: the percentage of the days employed relative to the client's age"", '# In[36]:', '# In[37]:', '# ## Baseline', '# ### Logistic Regression Implementation', '# In[38]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[39]:', '# In[40]:', '# In[41]:', '# Submission dataframe', '# In[42]:', '# ', '# In[ ]:']",147
home-credit-default-risk-001.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# One-hot encoding for categorical columns with get_dummies', '# In[4]:', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Categorical features: Binary features and One-Hot encoding', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Some simple new features (percentages)', '# In[5]:', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[6]:', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# In[7]:', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# In[8]:', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[9]:', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# In[10]:', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[11]:', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# In[12]:', '# Display/plot feature importance', '# In[14]:', '# In[ ]:', '# In[ ]:']",60
home-credit-default-risk-lgbm-w-domain-fts (1).py,"['# coding: utf-8', '# # Introduction', '# The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task.', '# ', '# * **Supervised**: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features', '# * **Classification**: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)', '# ', '# Some practices to do for you to get familier with the **standard supervised classification task**:', '# - [Titanic Top 11%| Starter I: Models Comparing](https://www.kaggle.com/chienhsianghung/titanic-top-11-starter-i-models-comparing)', '# - [Titanic Top 11%| Starter II: Hyperparameter Tuning](https://www.kaggle.com/chienhsianghung/titanic-top-11-starter-ii-hyperparameter-tuning)', '# - [TPS Apr.| Starter Pack: All Models](https://www.kaggle.com/chienhsianghung/tps-apr-starter-pack-all-models)', '# ', '# # Manual Setting', '# In[1]:', '# # Load In Data', '# ## Training and Testing', '# In[2]:', '# ## Reduce Memory Allotted Size', '# ', '# POS_CASH_balance, installments_payments, credit_card_balance', '# In[3]:', '# # EDA', '# ## Data Imbalanced', '# In[4]:', ""# app_train['TARGET'].value_counts()"", '# ## Missing Values', '# In[5]:', '# Function to calculate missing values by column# Funct ', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[6]:', '# When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can [handle missing values with no need for imputation.](https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase) Another option would be to drop columns with a high percentage of missing values.', '# ## Column Types', '# In[7]:', '# ## Unique Entries', '# of the `object`(categorical) columns.', '# In[8]:', '# Most of the categorical variables have a relatively small number of unique entries. We will need to find a way to deal with these categorical variables!', '# ### Encoding Categorical Variables', '# ', '# For categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. ', '# ', ""# Let's implement the policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding."", '# #### Label Encoding', '# In[9]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# #### One-Hot Encoding', '# In[10]:', '# one-hot encoding of categorical variables', '# ### Aligning Training and Testing Data', '# ', '# There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data.', '# ', ""# * Only column labels that are present in both df1 and df2 are retained [(join='inner').](https://stackoverflow.com/questions/51645195/pandas-align-function-illustrative-example/51645550)"", '# In[11]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# ## Anomalies', '# ', '# Mis-typed numbers, errors in measuring equipment, or extreme measurements.', '# In[12]:', ""# (app_train['DAYS_BIRTH'] / -365).describe()"", '# In[13]:', ""# Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients."", '# In[14]:', '# Well that is extremely interesting! It turns out that the anomalies have a lower rate of default. We will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous.', '# In[15]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# ## Correlations', '# In[16]:', '# Find correlations with the target and sort', '# Display correlations', '# ### Effect of Age on Repayment', '# In[17]:', '# Find the correlation of the positive days since birth and target', '# As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.', '# #### Distribution of Age', '# In[18]:', '# Set the style of plots', '# Plot the distribution of ages in years', '# #### Visualize The Effect of The Age on The Target', '# In[19]:', '# KDE plot of loans that were repaid on time', '# KDE plot of loans which were not repaid on time', '# Labeling of plot', ""# The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket."", '# ', '# To make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category.', '# In[20]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[21]:', '# Group by the bin and calculate averages', '# In[22]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.', '# ### Exterior Sources', '# ', '# The 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`.', '# In[23]:', '# In[24]:', '# Heatmap of correlations', '# All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.', '# In[25]:', '# iterate through the sources', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# # Feature Engineering (Example)', '# ', '# Kaggle competitions are won by feature engineering: those win are those who can create the most useful features out of the data. This represents one of the patterns in machine learning: **Feature engineering has a greater return on investment than model building and hyperparameter tuning.** As Andrew Ng is fond of saying: **Applied machine learning is basically feature engineering.**', '# ## Polynomial Features', '# ', ""# Dealing with [ImportError](https://stackoverflow.com/questions/59439096/importerror-cannnot-import-name-imputer-from-sklearn-preprocessing): `cannot import name 'Imputer' from 'sklearn.preprocessing'`: "", '# It has been deprecated with **scikit-learn v0.20.4** and removed as of **v0.22.2**.', '# In[26]:', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# from sklearn.preprocessing import Imputer', ""# imputer = Imputer(strategy = 'median')"", ""# cannot import name 'Imputer' from 'sklearn.preprocessing'"", '# Need to impute missing values', '# Create the polynomial object with specified degree', '# In[27]:', '# Train the polynomial features', '# Transform the features', '# This creates a considerable number of new features. To get the names we have to use the polynomial features `get_feature_names` method.', '# In[28]:', '# There are 35 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correlated with the target.', '# In[29]:', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# We will add these features to a copy of the training and testing data and then evaluate models with and without the features. The only way to know if an approach will work is to try it out!', '# In[30]:', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# ## Domain Knowledge Features', '# ', ""# * `CREDIT_INCOME_PERCENT`: the percentage of the credit amount relative to a client's income"", ""# * `ANNUITY_INCOME_PERCENT`: the percentage of the loan annuity relative to a client's income"", '# * `CREDIT_TERM`: the length of the payment in months (since the annuity is the monthly amount due', ""# * `DAYS_EMPLOYED_PERCENT`: the percentage of the days employed relative to the client's age"", '# In[31]:', '# iterate through the new features', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # matplotlib.pyplot.legend', '    # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html', '    # Label the plots', '# # Feature Engineering (Final)', '# In[32]:', '    # 1.EXT_SOURCE_X FEATURE ', '    # AMT_CREDIT ', '    # AMT_INCOME_TOTAL ', '    # DAYS_BIRTH, DAYS_EMPLOYED ', '# In[33]:', ""    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']"", '    # Data Cleansing', '    # substraction between DAYS_LAST_DUE_1ST_VERSION and DAYS_LAST_DUE', '    # 1.Calculate the interest rate', '    # multi index ', '    # unstack() ', '    # rename column ', '    # NaN', '# DAYS_DECISION', '    # multi index ', '    # prev_amt_agg', '    # SK_ID_CURR APPROVED_COUNT REFUSED_COUNT', ""    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT' drop "", '# In[34]:', '    # SK_ID_CURR reset_index()', ""    # CREDIT_ACTIVE='Active' filtering"", '    # SK_ID_CURR reset_index() ', '# BUREAU DAYS_CREDIT -750', '# bureau_bal SK_ID_CURR MONTHS_BALANCE aggregation', '    # SK_ID_CURR Group by bureau SK_ID_CURR ', '    # STATUS 120', '    # SK_ID_CURR MONTHS_BALANCE aggregation', '    # SK_ID_CURR reset_index()', '# bureau aggregation', '    # bureau_day_amt_agg bureau_active_agg', '    # STATUS ACTIVE IS_DPD RATIO', '    # bureau_agg bureau_bal_agg', '# In[35]:', '    # (SK_DPD) 0 , 0~ 100 , 100', '    # 0~ 120 120', '    # SK_ID_CURR aggregation', '    # MONTHS_BALANCE (20)', '    # SK_ID_CURR reset_index()', '# In[36]:', '    # DPD  ', '    # 30~ 120 100', '    # SK_ID_CURR aggregation', '    # (DAYS_ENTRY_PAYMENT) (1)', '# In[37]:', '    # DPD', '    # SK_ID_CURR aggregation', '    # MONTHS_BALANCE (3)', '# # Training (Baseline)', '# In[38]:', '# Scale each feature to 0-1', '    # Median imputation of missing values', '        # Make the model with the specified regularization parameter', '        # Train on the training data', '        # Make predictions', '        # Make sure to select the second column only', '        # Submission dataframe', '        # submit.head()', '        # Save the submission to a csv file', '        # Make the random forest classifier', '        # Train on the training data', '        # Extract feature importances', '        # Make predictions on the test data', '        # Make a submission dataframe', '        # submit.head()', '        # Save the submission dataframe', '# # Training (Engineered Features Implemented)', '# In[39]:', '    # Train on the training data', '    # Make predictions on the test data', '    # Make a submission dataframe', '    # Save the submission dataframe', '    # Train on the training data', '    # Extract feature importances', '    # Make predictions on the test data', '    # Make a submission dataframe', '    # Save the submission dataframe', '# # Feature Importances (Example)', '# In[40]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    # Set the yticks and labels', '    # Plot labeling', '# In[41]:', '# In[42]:', '# # Reload The Original Datasets For Final Training', '# In[43]:', '# ## Datasets Concatenation and Join', '# In[44]:', '# apps prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg', '    # Join with apps_all', '# In[45]:', '# # Final Training (LGBM)', '# In[46]:', '# In[47]:', '# application, previous, bureau, bureau_bal', '# Category Label', '# # Prediction', '# In[48]:', '# In[49]:', '# # Feature Importances (Final)', '# In[50]:', '# [How to save a matplotlib figure and fix text cutting off || Matplotlib Tips](https://www.youtube.com/watch?v=C8MT-A7Mvk4)', '# In[51]:', '# https://stackoverflow.com/questions/42579908/use-corr-to-get-the-correlation-between-two-columns', '# Heatmap of correlations', ""# plt.savefig('./Domain Knowledge Features Correlation Heatmap.png', bbox_inches = 'tight')"", ""# plt.imshow(plt.imread('./Domain Knowledge Features Correlation Heatmap.png'))"", '# # References', '# ', '# * [Start Here: A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction#Conclusions)', '# * [Home Credit Default Risk Prediction](https://www.kaggle.com/sangseoseo/home-credit-default-risk-prediction#data-cleansing,-EDA,-model-creation)', '# * [More domain knowledge from former Home Credit analyst](https://www.kaggle.com/c/home-credit-default-risk/discussion/63032)']",284
home-credit-default-risk-prediction.py,"['# coding: utf-8', '# ## data cleansing, EDA, model creation', '#  - POS_CASH_balance, installments_payments, credit_card_balance ', '# In[1]:', '# ### load dataset under colab', '# In[2]:', '# In[3]:', '# ### column data type convert to reduce memory allocated size', '# In[4]:', '# In[5]:', '# ### load package', '# In[6]:', '# ### EDA , Feature Engineering', '# In[7]:', '    # 1.EXT_SOURCE_X FEATURE ', '    # AMT_CREDIT  Feature ', '    # AMT_INCOME_TOTAL  Feature ', '    # DAYS_BIRTH, DAYS_EMPLOYED  Feature ', ""  # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']"", '  # Data Cleansing', '  # substraction between DAYS_LAST_DUE_1ST_VERSION and DAYS_LAST_DUE', '  # 1.Calculate the interest rate', '  # \xad   \xa0\xad       aggregation . ', '        #   aggregation. ', '      #   aggregation', ""  # multi index  '_'   "", '  # unstack() ', '  # rename column ', '  # NaN  0 . ', '        #   aggregation. ', '      #   aggregation', ""  # multi index  '_'   "", '    # prev_amt_agg . ', '    # SK_ID_CURR    APPROVED_COUNT  REFUSED_COUNT  . ', ""    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'  drop "", '# bureau   \xa0       \xa0  .', '    # \xa0     \xa0     \xa0  .  ', '    #   /     ', '    #    120    ', '  #   ', '  #   SK_ID_CURR reset_index()  ', ""  # CREDIT_ACTIVE='Active'   filtering"", '      #   ', '  #   SK_ID_CURR reset_index()  ', '# BUREAU DAYS_CREDIT  -750    . ', '        #   ', '# bureau_bal SK_ID_CURR \xa0  MONTHS_BALANCE aggregation  ', '    # SK_ID_CURR\xa0 Group by  bureau SK_ID_CURR  \xa0  . ', '    # STATUS      120     . ', '    # SK_ID_CURR \xa0  MONTHS_BALANCE aggregation  ', '    #   SK_ID_CURR reset_index()  ', '#  bureau\xa0 aggregation      ', '    # bureau_day_amt_agg bureau_active_agg .  ', '    # STATUS ACTIVE IS_DPD RATIO\xa0  . ', '    # bureau_agg bureau_bal_agg . ', '# #### aggregation for pos, install, credit card', '# In[8]:', '    #  (SK_DPD) 0   , 0~ 100 , 100    ', '    # ,   0~ 120  ,   120   ', '    #   \xa0  SK_ID_CURR \xa0 \xa0 aggregation  ', '        #  . ', '    #   ', '    # MONTHS_BALANCE (20 )    . ', '        #  . ', '    #   ', '    # SK_ID_CURR reset_index()   ', '    # \xa0    \xa0   \xa0  . \xa0    \xa0    DPD    ', '    # ,   30~ 120  ,   100    . ', '    #   \xa0  SK_ID_CURR \xa0 \xa0 aggregation  . ', '        #   ', '    # \xa0  (DAYS_ENTRY_PAYMENT) \xa0 (1 )   ', '        #   ', '    #     \xa0    ', '    # DPD    .', '    #     SK_ID_CURR \xa0 aggregation \xa0  . ', '        #   ', '    # MONTHS_BALANCE \xa0  ( 3 )  .  ', '# ### datasets concatenation and Join', '# In[9]:', '# apps prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg      ', '  # Join with apps_all', '# ### reload the original *datasets*', '# In[10]:', '# In[11]:', '# ### Data preparation, encodeing, datasets split, fit', '# In[12]:', '# application, previous, bureau, bureau_bal \xa0    . ', '# Category   Label  . ', '#    . ', '# ### CSV for Predicted result', '# In[13]:', '# In[14]:', '# ### Plot importance of features', '# In[15]:', '# In[16]:']",95
home-credit-default-risk-production-level.py,"['# coding: utf-8', '# ', '# ## This notebook is the slightly revised version of our project notebook created in collobaration with all Machine Learning bootcamp students of Mustafa Vahit Keskin (https://github.com/mvahit) in April 2020. Thanks to all my colleagues and my teacher.', '# ## We evaluated the notebook altogether, but I have worked specifically on the previous application part.', '# ', '# # Libraries', '# In[1]:', '# # Helper Functions', '# In[2]:', '# In[3]:', '# Display/plot feature importance', '# # application_train', '# In[4]:', '# # bureau & bureau_balance', '# In[5]:', '    # Degisken isimlerinin yeniden adlandirilmasi ', '    # Status_sum ile ilgili yeni bir degisken olusturma', '    # bureau_bb tablosundaki kategorik degiskenlere One Hot Encoding uygulanmasi', '    # CREDIT_CURRENCY degiskeninin %99u currency1, bu sebeple ayirt ediciligi olmayacagini dusundugumuz icin sildik  ', '    # bureau_bb_agg tablosuna aggreagation islemlerinin uygulanamasi  ', '    # Degisken isimlerinin yeniden adlandirilmasi ', '    # kisinin aldg en yuksek ve en dusuk kredinin farkn gsteren yeni degisken', '    # ortalama kac ayda bir kredi cektigini ifade eden  yeni degisken', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# # installments_payments', '# In[6]:', '    # Her bir taksit demesinin gec olup olmama durumu 1: gec dedi 0: erken demeyi temsil eder', '    # Agrregation ve degisken tekillestirme', '    # Multi index problemi czm', '    # drop variables ', '    # Kredi deme yzdesi ve toplam kalan borc', '# # pos_cash_balance', '# In[7]:', '    # Kategorik Degiskenimizi Dummy Degiskenine Dnstrme', '    # Aggregation Islemi - Tekillestirme', ""    # Multilayer index'i tek boyutlu index'e dnstrme"", '    # SK_DPD kac kredide 0 olma durumu (SK_DPD MAX alacagiz 0 durumunu veriyor) ', '    # SK_DPD_DEF (SK_DPD_DEF_MAX sifir olma durumunu veriyor)', '    # CNT_INSTALMENT_FUTURE_MIN==0 oldugunda NAME_CONTRACT_STATUS_Completed_SUM==0 olma durumu ', '    # 1:kredi zamaninda kapanmamis 0:kredi zamaninda kapanmis', '# # credit_card_balance', '# In[8]:', ""    CCB = pd.get_dummies(CCB, columns= ['NAME_CONTRACT_STATUS'] )  # artik tumu sayisal "", '    # Bu fonksiyon, kac defa odemelerin geciktigini hesaplar   # Function to calculate number of times Days Past Due occurred ', '        # DPD ile beklenen bir seri: SK_DPD degiskeninin her bir prev_app daki gecmis kredi icin olan degerleri  # DPD is a series of values of SK_DPD for each of the groupby combination ', '        # We convert it to a list to get the number of SK_DPD values NOT EQUALS ZERO', '        P = len(M)        # P: taksit sayisi', '        # Find the count of transactions when Payment made is less than Minimum Payment ', ""    CCB['CASH_CARD_RATIO1'] = (CCB['DRAWINGS_ATM']/CCB['DRAWINGS_TOTAL'])*100  # ATM den cektigi nakit / toplam cektigi"", ""    CCB['DRAWINGS_RATIO1'] = (CCB['TOTAL_DRAWINGS']/CCB['NUMBER_OF_DRAWINGS'])*100     # yuzdelik degil, genisleme yapmis"", '# In[ ]:', '# # previous_application', '# In[9]:', '    # ""WEEKDAY_APPR_PROCESS_START""  deikeninin  WEEK_DAY ve WEEKEND olarak iki kategoriye ayrlmas', '    # ""HOUR_APPR_PROCESS_START""  deikeninin working_hours ve off_hours olarak iki kategoriye ayrlmas', '    # DAYS_DECISION deeri 1 yldan kk olanlara 1, byk olanlara 0 deeri verildi.', '    # ""NAME_TYPE_SUITE""  deikeninin alone ve not_alone olarak iki kategoriye ayrlmas', '    # ""NAME_GOODS_CATEGORY""  deikenindeki bu deerler others olarak kategorize edilecek', '    # ""NAME_SELLER_INDUSTRY""  deikenindeki bu deerler others olarak kategorize edilecek', '    # stenilen krecinin verilen krediye oran ieren deikeni tretir', '    # stenilen krecinin verilen krediye oran ieren deikeni tretir', '    # deme gnn geciktirmi mi bunu gsteren churn_prev deikeni tretilir.', '    # 1= geciktirmi, 0 = geciktirmemi, NaN = bo deer', '    # NFLAG_INSURED_ON_APPROVAL deikeni yerine kullanlmak izere NEW_INSURANCE deikeni tanmland.', '    # INTEREST_RATE deikenini oluturur.', '    # Previous tablosundaki kategorik deikenlerin isimlerini tutar.', '# # Combine', '# In[10]:', '# In[ ]:', '# # Model Tuning (Will be done in the future)', '# In[11]:', '#              ""n_estimators"": [200, 500, 100],', '#              ""max_depth"":[1,2,35,8]}', '# In[12]:', '# # Machine Learning', '# In[13]:', '# # main', '# In[14]:', '# In[15]:']",80
home-credit-default-risk-project-with-lightgbm.py,"['# coding: utf-8', '# Based on kernels and discussions:', '# ', '#   - https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features', '#     ', '#   - https://github.com/mvahit/home_credit (collaboration with the community of Veri Bilimi Okulu that I also joined)', '#     ', '#   - https://www.kaggle.com/c/home-credit-default-risk/discussion/64821', '# In[1]:', '# In[2]:', '# Time function for tracking run times of functions', '# In[3]:', '# One-hot encoding function for categorical variables with get_dummies', '# In[4]:', '    original_columns = list(df.columns) # col names as string in a list ', '    # Create a label encoder object', '    # Iterate through the columns', '            # If 2 or fewer unique categories', '                # Train on the training data', '                # Transform both training and testing data', '                # Keep track of how many columns were label encoded', '# In[5]:', '# Preprocess application_train.csv and application_test.csv', '    # Some simple new features (percentages)', '    # Categorical features with One-Hot encode', '# In[6]:', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '     # Bureau new features', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[7]:', '# Preprocess previous_applications.csv', '# In[8]:', '# Preprocess POS_CASH_balance.csv', '# In[9]:', '# Preprocess installments_payments.csv', '# In[10]:', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[11]:', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# In[12]:', '# Display/plot feature importance', '# In[13]:']",54
home-credit-default-risk-with-xgboost-and-lightgbm.r,[],0
home-credit-default-risk.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# sklearn preprocessing for dealing with categorical variables', '# File system manangement', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# In[4]:', '# In[5]:', '# Testing data features', '# In[6]:', '# In[7]:', '# In[8]:', '# Function to calculate missing values by column# Funct ', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[9]:', '# Missing values statistics', '# In[10]:', '# In[11]:', '# Number of unique classes in each object column', '# In[12]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# In[13]:', '# one-hot encoding of categorical variables', '# In[14]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# In[20]:', '# In[21]:', '# Find correlations with the target and sort', '# Display correlations', '# In[22]:', '# Find the correlation of the positive days since birth and target', '# In[23]:', '# Set the style of plots', '# Plot the distribution of ages in years', '# In[24]:', '# KDE plot of loans that were repaid on time', '# KDE plot of loans which were not repaid on time', '# Labeling of plot', '# In[25]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[26]:', '# Group by the bin and calculate averages', '# In[27]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# In[28]:', '# Extract the EXT_SOURCE variables and show correlations', '# In[29]:', '# Heatmap of correlations', '# In[30]:', '# iterate through the sources', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# In[31]:', '# Copy the data for plotting', '# Add in the age of the client in years', '# Drop na values and limit to first 100000 rows', '# Function to calculate correlation coefficient between two columns', '# Create the pairgrid object', '# Upper is a scatter plot', '# Diagonal is a histogram', '# Bottom is density plot', '# In[32]:', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# Need to impute missing values', '# Create the polynomial object with specified degree', '# In[33]:', '# Train the polynomial features', '# Transform the features', '# In[34]:', '# In[35]:', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# In[36]:', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# In[37]:', '# In[38]:', '# In[39]:', '# iterate through the new features', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# In[40]:', '# from sklearn.preprocessing import MinMaxScaler, Imputer', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# Median imputation of missing values', ""# imputer = Imputer(strategy = 'median')"", '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[41]:', '# Make the model with the specified regularization parameter', '# Train on the training data', '# In[42]:', '# Make predictions', '# Make sure to select the second column only', '# In[43]:', '# Submission dataframe', '# In[44]:', '# Save the submission to a csv file', '# In[45]:', '# Make the random forest classifier', '# In[46]:', '# Train on the training data', '# Extract feature importances', '# Make predictions on the test data', '# In[47]:', '# Make a submission dataframe', '# Save the submission dataframe', '# In[48]:', '# Impute the polynomial features', ""# imputer = Imputer(strategy = 'median')"", '# Scale the polynomial features', '# In[49]:', '# Train on the training data', '# Make predictions on the test data', '# In[50]:', '# Make a submission dataframe', '# Save the submission dataframe', '# In[51]:', '# Impute the domainnomial features', '# Scale the domainnomial features', '# Train on the training data', '# Extract feature importances', '# Make predictions on the test data', '# In[52]:', '# Make a submission dataframe', '# Save the submission dataframe', '# In[53]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    # Set the yticks and labels', '    # Plot labeling', '# In[54]:', '# Show the feature importances for the default features', '# In[55]:', '# In[ ]:']",183
home-credit-difficult.py,"['# coding: utf-8', '# ', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# In[3]:', '# DAYS_EMPLOYED()', '# In[4]:', '# testtrain', '# In[5]:', '# \xa0', '# In[6]:', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[7]:', '# \xa060%', '# ', '# \xa0', '# ', '# ', '# In[8]:', '# ', '# ', '# ', '# In[9]:', '# Find correlations with the target and sort', '# Display correlations', '# ', '# ', '# ', '# ', '# AMT_INCOME_TOTAL()NAME_HOUSING_TYPE(\xa0)', '# In[10]:', '# traintest', '# In[11]:', '# In[12]:', '# In[13]:', '# \xad', '# ', '# \xa02(LightGBM)', '# In[14]:', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# pandasget_dummies', '# In[15]:', '# \xa0', '# ', '# \xa0', '# In[16]:', '# ', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '#     print(train_index, valid_index)', '    # : train_index', '# In[21]:', '# In[22]:']",73
home-credit-fastai-trial.py,"['# coding: utf-8', '# In[27]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# In[44]:', '# In[45]:', '# In[46]:', '# In[47]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",36
home-credit-feature-engineering-and-prediction.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# For model estimation', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[2]:', '# In[25]:', '# # Feature Engineering', '# ', '# ### Application Train', '# - income to credit', '# - income per person', '# - annuity to income', '# - days employed relative to age', '# ', '# ', '# ### Bureau', '# - groupby for counts, means, min, max', '# In[1]:', '# In[ ]:', '# In[26]:', ""# data['SOURCE_1_PERCENT'] = data['EXT_SOURCE_1']/(data['EXT_SOURCE_1']+data['EXT_SOURCE_2']+data['EXT_SOURCE_3'])"", ""# data['SOURCE_2_PERCENT'] = data['EXT_SOURCE_2']/(data['EXT_SOURCE_1']+data['EXT_SOURCE_2']+data['EXT_SOURCE_3'])"", ""# data['SOURCE_3_PERCENT'] = data['EXT_SOURCE_3']/(data['EXT_SOURCE_1']+data['EXT_SOURCE_2']+data['EXT_SOURCE_3'])"", ""# test['SOURCE_1_PERCENT'] = test['EXT_SOURCE_1']/(test['EXT_SOURCE_1']+test['EXT_SOURCE_2']+test['EXT_SOURCE_3'])"", ""# test['SOURCE_2_PERCENT'] = test['EXT_SOURCE_2']/(test['EXT_SOURCE_1']+test['EXT_SOURCE_2']+test['EXT_SOURCE_3'])"", ""# test['SOURCE_3_PERCENT'] = test['EXT_SOURCE_3']/(test['EXT_SOURCE_1']+test['EXT_SOURCE_2']+test['EXT_SOURCE_3'])"", '# In[ ]:', '# In[4]:', '# def plot_target(data, feature, xlab= \'\', ylab= \'\', title= """"):', '#     plt.figure(figsize=(12,9))', ""#     sns.kdeplot(data.loc[data['TARGET'] == 0, feature], label = 'target == 0')"", '#     # KDE plot of loans which were not repaid on time', ""#     sns.kdeplot(data.loc[data['TARGET'] == 1, feature], label = 'target == 1')"", '#     # Labeling of plot', '#     plt.xlabel(feature); plt.ylabel(\'Density\'); plt.title(""Distribution of %s""%(feature));', '# In[6]:', ""# plot_target(data, 'Annuity_Income')"", '# In[6]:', ""# plot_target(data,'Income_Cred')"", '# In[7]:', ""# plot_target(data, 'EMP_AGE')"", '# In[8]:', ""# plot_target(data,'Income_PP')"", '# ## Bureau summary statistics', '# In[7]:', '# df1 = bureau', '# def new_features(df1, group_by ,stats, data_name):', '#     columns = [group_by]', '#     data_features = df1.groupby(group_by).agg(stats).reset_index()', '#     for var in data_features.columns.levels[0]:', '#         # ignore grouping variable', '#         if var != group_by:', '#             # get rid of original variable', '#             for stat in data_features.columns.levels[1][:-1]:', ""#                 columns.append('%s_%s_%s' %(data_name,var,stat))"", '#     data_features.columns = columns            ', ""#     data = bureau.merge(data_features , on='SK_ID_CURR', how = 'left')"", '#     return data', '# In[11]:', ""# bureau_new = new_features(bureau.drop(columns = ['SK_ID_BUREAU']),'SK_ID_CURR', stats ,data_name = 'bureau')"", '# bureau_new.head()', '# ## Add number of loans as a variable: count', '# In[27]:', '# COUNT', '# ## Unique loan types per customer', '# In[28]:', '# ## Average loan type ', '# ### are customer taking out the same type of loans or different types', '# In[29]:', '# ## Percentage of active loans', '# In[30]:', '# ## Number of days between loans', '# In[13]:', ""# gp = bureau_new[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT']].groupby('SK_ID_CURR')"", ""# gp1 = gp.apply(lambda x: x.sort_values(['DAYS_CREDIT'], ascending = False)).reset_index(drop=True)"", '# # Difference between the days', '# gp1[""DAYS_CREDIT1""] = gp1[""DAYS_CREDIT""]*-1', ""# gp1['DAYS_DIFF']  = gp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT1'].diff()"", ""# gp1['DAYS_DIFF'] = gp1['DAYS_DIFF'].fillna(0).astype('uint32')"", ""# del gp1['DAYS_CREDIT'], gp1['DAYS_CREDIT1'], gp1['SK_ID_CURR']"", ""# bureau_new = bureau_new.merge(gp1, on = 'SK_ID_BUREAU', how = 'left')"", '# ## % of loans where end date of credit is past', '# ## value < 0 means the end date has past', '# In[31]:', '# ## Further cleaning and modelling', '# In[32]:', '# get some summary stats of numeric variables', '# ## Bureau and Bureau Balance Merging', '# - merges bureau and bureau balance and aggregates them to the  SK_ID_CURR level the same as data', '# In[34]:', '# Bureau Balance ', '# # Number of loans per person', '# ## Installment Applications', '# In[35]:', '# Was it paid on early or late?', ""# installments_payments['DAYS_P'] = installments_payments['DAYS_P'].apply(lambda x: x if x > 0 else 0)"", ""# installments_payments['DAYS_I'] = installments_payments['DAYS_I'].apply(lambda x: x if x > 0 else 0)"", '# ## Previous applications', '# In[37]:', '# 365243 is NAN', '# ## Group the approved and non approved previous loan data', '# In[38]:', '# Previous Applications: Approved Applications - only numerical features', '# Previous Applications: Refused Applications - only numerical features', ""# del previous_application['SK_ID_PREV']"", '# ## POS_CASH balance', '# In[39]:', '# ## Credit Card Balance', '# In[ ]:', '# In[44]:', '# ## Merging Datasets', '# In[45]:', '# Previous Application', '# POS_CASH_BALANCE', '# Installments_payments', '# In[23]:', '# In[ ]:', '# Plot feature importances', '# # Try blending some models', '# In[1]:', '# Plot importances', '# In[ ]:']",129
home-credit-kor-ver.py,"['# coding: utf-8', '# *     ,   \xa0   .', '# * It is referenced by the kernel down below for study.', '# ', '# https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction', '# ## \xa0  .', '# In[1]:', '# numpy and pandas for data manipulation', '# sklearn preprocessing for dealing with categorical variables', '# File system management', '# Suppress warnings', '# matplotlib and seaborn for plotting', '# ##  ', '# ', '# \xa0,  \xa0    . 9  , 1 \xa0,   . , \xa0   6     .', '# In[2]:', '# List of files available.', '# In[3]:', '# Training data', '# \xa0 307511  \xa0    \xa0 , 122   .  \xa0 .', '# In[4]:', '# Testing data features', '#   \xa0      . ', '# `Target` \xa0\xa0  .  121  !', '# ## EDA(Exploratory Data Analysis)', '# EDA   \xa0 ,       \xa0  .', '# ,    \xa0   , \xa0\xa0  \xa0 \xa0, \xa0\xa0     .', '#    ,  \xa0    \xa0,   \xa0  \xa0\xa0   .', '# ### Examine the Distribution of the Target Column', '#  \xa0   \xa0   \xa0     0  1 \xa0 .', '# ', '# 0  \xa0  \xa0  ', '# ', '# 1     !', '# ', '# ', '# In[5]:', '#  \xa0   (0  )  .', '# \xa0    !', '# In[6]:', '# > From this information, we see this is an imbalanced class problem. There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance.', '# ### Examine Missing value', '# ', '# \xa0   missing value    .', '# In[7]:', '# Total missing values', '# Percentage of missing values', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# Rename the columns', '# In[12]:', '# In[13]:', '# Sort the table by percentage of missing descending', '# Print some summary information', '# In[14]:', '#  \xa0    !', '# In[15]:', '    # Total missing values', '    # Percentage of missing values', '    # Make a table with the results / concat \xa0  .', '    # Rename the columns', '    # Sort the table by percentage of missing descending', '    # Print some summary information', '    # Return the dataframe with missing information', '# In[16]:', '# Missing values statistics', '# > When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can handle missing values with no need for imputation. Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.', '# \xa0   ,     .(imputation?)', '#   XGBoost       imputation \xa0  ?', '#          \xad\xa0()  ,       \xa0 ,     \xa0 .', '# ### Column Types', '# ', '#       . `int64`, `float64` \xa0  (\xa0  \xa0  ). (object)   \xa0 \xa0   \xa0 .', '# In[17]:', '# Number of each type of column', '# \xa0,       .', '# In[18]:', '# Number of unique classes in each object column', '# pd.Series.nunique   \xa0       //  ', '# In[19]:', '# \xa0 \xa0   \xa0 \xa0,          \xa0, ', '# nunique \xa0 null \xa0\xa0   .', '#   `ORGANIZATION_TYPE`     \xa0 .', '# ### Encoding Categorical Variables', '# \xa0     \xa0  ,   \xa0 \xa0  .', '# ', '# ,   \xa0 encoding       \xa0  .', '# ', '# 1. Label encoding :', '# ', '#    \xa0 \xa0 . ,        \xa0 \xa0  ', '# ![image.png](attachment:image.png)', '# 2. One-hot encoding : ', '# ', '#  id     ,   1 \xa0\xa0,', '#   0 \xa0 , \xa0      \xad    .', '#    ', '# ![image.png](attachment:image.png)', '# * \xa0\xad :   vs. -    ', '# ', '# ', '# >The problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 and data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.', '# ', '# >There is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical variables with no issues. Here is a good Stack Overflow discussion. I think (and this is just a personal opinion) for categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions (while still trying to preserve information).', '# ', '# >In this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iterations).', '# ', '# ', '# , \xa0   2 , -  \xa0, 2  .', '# ', '# 2   \xa0, 2      \xa0        \xa0   ?    , 2     -   .', '# ', '#   \xa0      \xad    \xa0  -   \xa0 .', '# ### Label Encoding and One-Hot Encoding', '# , \xa0  dtype==object   2 label encoding !', '# ', '#  2  one-hot encoding  .', '# ', '#   Scikit-Learn `LabelEncoder` \xa0,', '# one-hot encoding  pandas   `get_dummies(df)`  .', '# In[20]:', '# Create a label encoder object.', '# In[21]:', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both Training and Testing data', '            # Keep track of how many columns label encoded', '# In[22]:', '#    \xa0  ', '# In[23]:', '# one-hot encoding of categorical variables', '# ### Aligning Training and Testing Data', '# ', '# , \xa0/        \xa0,     . ', '# > There need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to align the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set axis = 1 to align the dataframes based on the columns and not on the rows!', '# ', '#    (\xa0 ) \xa0     ,   \xa0   .', '# ', '# , \xa0   \xad\xa0,   align   !', '# ', '# \xa0, \xa0  ,  \xa0   \xa0!', '# ', '# \xa0 \xa0 axis=1   \xa0!, axis=0  row  !', '# In[24]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# In[25]:', '# app_test   app_train   !', '# > The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try dimensionality reduction (removing features that are not relevant) to reduce the size of the datasets.', '# ', '#  app_train   122 , \xad . one-hot encoding 240  .', '# ', '#  \xa0        .', '# ### Back to Exploratory Data Analysis', '# ', '# Anomalies(\xa0, \xa0 )', '# > One problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the describe method. The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:', '# ', '# ', '#    \xa0, \xa0    .  , \xa0 \xa0 \xa0      .', '# ', '# ,    \xa0\xa0     , `describe`   \xa0\xa0    \xa0  .', '# ', '# `DAYS_BIRTH`   \xa0   \xa0     .  -1   \xa0   .', '# ', '# ', '# In[26]:', '# original DAYS_BIRTH  ', '# In[27]:', '# \xa0 (50%)    , / \xa0    \xa0   !', '# ', '# , `DAYS_EMPLOYED`  !', '# In[28]:', '#  , \xa0,  1000, 75%  \xa0 .   . ', '# ', '# \xa0, (year)  , 1000 ,   .', '# In[29]:', '# \xa0   \xa0 ? 1000  \xa0  ?         .', '# , \xa0 , \xa0  !', '# In[30]:', '# DAYS_EMPLOYED    \xa0   \xa0 anom \xa0', '# DAYS_EMPLOYED    \xa0   \xa0!', '# In[31]:', '# > Well that is extremely interesting! It turns out that the anomalies have a lower rate of default.', '# Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous.', '# ', '# ', '#  ,   \xa0 \xa0    .  \xa0     \xa0   ...  \xa0   ,  \xa0   ', '# ', '# missing value  \xa0 .  `np.nan`  \xa0 . \xa0, boolean    \xa0 \xa0 \xa0   \xa0  .', '# In[32]:', '# Create anomalous flag column', '#     True ', '# Replace the anomalous values with nan', '# numpy nan , \xa0   365243  np.nan !', '#  , \xa0 \xa0,    ', '# In[33]:', '#      .', '#     . ,  \xa0  \xa0, \xa0   nan .', '# In[34]:', '# \xad \xa0    .', '# In[35]:', '# In[36]:', '# `DAYS_EMPLOYED_ANOM` TRUE    9274  \xa0  .', '# In[37]:', '# `DAYS_EMPLOYED == 365243`        !', '# ### Correlations', ""# >Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method."", '# The correlation coefficient is not the greatest method to represent ""relevance"" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:', '# ', '# * .00-.19 very weak', '# * .20-.39 weak', '# * .40-.59 moderate', '# * .60-.79 strong', '# * .80-1.0 very strong', '#       \xa0  , `corr`  .', '# ', '# 1   Positive  / -1   Negative  .', '# ', '# 0   \xa0  ', '# In[38]:', '# \xa0 \xa0\xa0 \xa0  \xa0 \xa0\xa0  .    \xa0 ', '#   heatmap     .', '# In[39]:', '#      . !', '# In[40]:', '# Find correlations with the target and sort', '# In[41]:', '# Display correlations', ""# > Let's take a look at some of more significant correlations: the DAYS_BIRTH is the most positive correlation. (except for TARGET because the correlation of a variable with itself is always 1!) Looking at the documentation, DAYS_BIRTH is the age in days of the client at the time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative."", '# ', '#  \xa0,  . ,  DAYS_BIRTH    \xa0 \xa0 .', '# ', '# Positive    ,  feature      . , feature      \xa0   \xa0  .', '# ', ""# ,  Correlation  DAYS_BIRTH \xad Positive   ,  'DAYS_BIRTH'   \xad,     \xa0 \xa0     ."", '# ', '#  ,   \xa0\xa0    0  ,  \xa0    TARGET  1   ,   \xa0 \xa0  \xa0   \xa0   \xa0 . ', '# ', '#   \xa0?     ,      ~ ,   \xa0    \xa0      !', '# ### Effect of Age on Repayment', '# In[42]:', '# Find the correlation of the positive days since birth and target', '#   ,   .', '# ', '# , \xa0   ,  0   \xa0    . \xa0,        \xa0 \xa0 \xa0   .', '# ', '# \xa0, \xa0      \xa0 .', '# In[43]:', '# In[44]:', '# Set the style of plots', '# Plot the distribution of ages in years', ""plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins=25) # bins    \xa0 \xa0"", '# > By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. To visualize the effect of the age on the target, we will next make a kernel density estimation plot (KDE) colored by the value of the target. A kernel density estimate plot shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph.', '# ', '#  ,      ,   \xa0 .', '# ', '# \xa0 \xa0    \xa0  , KDE     .', '# ', '#  \xa0 seaborn `kdeplot`  .', '# In[45]:', '# KDE plot of loans that were repaid on time', '# KDE plot of loans which were not repaid on time', '# Labeling of plot', ""# > The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket."", '# To make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category.', '# ', '# target == 1         \xa0\xa0.', '# ', '#  \xa0  (-0.07) .  \xa0  \xa0\xa0  .', '# ', '#     \xa0\xa0 . average failure to repay loans by age bracket.      \xa0  .', '# ', '# , 5    \xa0    \xa0 . ', '# In[46]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[47]:', 'np.linspace(20,70, num=11) #  ,  ,   =11', '# In[48]:', '# Group by the bin and calculate averages', '# ,     \xa0 \xa0\xa0    \xa0  .', '# ', '# In[49]:', '# In[50]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# >There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.', '# This is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time.', '# ', '# ', '#  \xa0   \xa0 \xa0   \xa0 .', '# ', '#    \xa0 \xa0  \xa0\xad  , \xad     \xa0 \xa0 \xa0 .  .', '# ### Exterior Sources', '# ', '#     \xad     \xad  .', '# ', '# EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 \xa0 \xa0   ,', '# ', '#  \xad     \xa0 . \xad  \xa0 \xa0 \xa0\xa0? \xa0  \xa0  .', '# ', '# \xa0,  \xad   !', '# In[51]:', '# Extract the EXT_SOURCE variables and show correlations', '# In[52]:', '# Heatmap of correlations', '# EXT       \xa0  , EXT  , \xa0  \xa0 \xa0  .', '# ', '# , EXT 1  DAYS_BIRTH   \xa0   \xa0  .', '# , EXT 1        \xa0.', '# ', '# \xa0        .', '# ', '# In[53]:', '# iterate through the sources', '    # create a new subplot for each source', '    plt.subplot(3,1,i+1) # 3  1  \xa0,    ', '    # plot repaid loans', '    # plot loans that were not repaid', '# EXT_SOURCE_3    \xa0     ,  \xad    \xa0 \xa0  \xa0  .', '# ', '# ,  \xa0     \xa0 .', '#  \xa0   \xa0 \xa0  \xa0    .', '# ### Pairs Plot', '# >As a final exploratory plot, we can make a pairs plot of the EXT_SOURCE variables and the DAYS_BIRTH variable. The Pairs Plot is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.', ""# If you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!"", '#   \xa0, pairs plot   \xa0. , EXT_SOURCE - DAYS_BIRTH       \xa0 . ', '# ', '# Pairs Plot      ,   variables      .', '# ', '# seaborn   `PairGrid`     \xa0 .', '# ', '#     !', '# ', '# In[54]:', '# Copy the ext_data for plotting', '# In[55]:', '# In[56]:', '#   !', '# In[57]:', '# Add in the age of the client in years', '# Drop na values and limit to first 100000 rows', '# In[58]:', '# \xa0    3 1 .', '# In[59]:', '# Function to calculate correlation coeffiecient between two columns', '# In[60]:', '# Create the pairgrid object', '# \xa0   \xa0 \xa0\xa0,   \xa0 , \xa0 /    ', '#         !', '# Upper is a scatter plot', '# Diagonal is a histogram', '# Bottom is density plot', '# >In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH (or equivalently YEARS_BIRTH), indicating that this feature may take into account the age of the client.', '# ', '# ', '#   x,   o .', '# ', '# EXT_SOURCE_1 , YEARS_BIRTH \xa0\xa0  \xa0   , \xad     \xa0    .', '# ## Feature Engineering', '# >Kaggle competitions are won by feature engineering: those win are those who can create the most useful features out of the data. (This is true for the most part as the winning models, at least for structured data, all tend to be variants on gradient boosting). This represents one of the patterns in machine learning: feature engineering has a greater return on investment than model building and hyperparameter tuning. This is a great article on the subject). As Andrew Ng is fond of saying: ""applied machine learning is basically feature engineering.""', '# ', '#  \xa0,    \xad feature engineering    \xa0,     !', '# ', '# >While choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the job of the data scientist (and maybe some automated tools to help us out).', '# ', '#    \xa0\xa0  \xa0  .    \xa0  \xa0   .', '# ', '# >Feature engineering refers to a geneal process and can involve both feature construction: adding new features from the existing data, and feature selection: choosing only the most important features or other methods of dimensionality reduction. There are many techniques we can use to both create features and select features.', '# ', '# feature engineering \xa0 \xa0, `feature construction`, `feature selection`  \xa0  .', '# ', '# * feature construction: adding new features from the existing data', '# * feature selection: choosing only the most important features or other methods of dimensionality reduction.', '# ', '# ', '#  feature engineering       feature construction \xa0 .', '# * Polynomial features', '# * Domain knowledge features', '# ', '# ', '# ', '# ', '# ', '# ', '# ', '# ', '# ### Polynomial Features', '# > One simple feature construction method is called polynomial features. In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can create variables EXT_SOURCE_1^2 and EXT_SOURCE_2^2 and also variables such as EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2^2, EXT_SOURCE_1^2 x EXT_SOURCE_2^2, and so on. These features that are a combination of multiple individual variables are called interaction terms because they capture the interactions between variables. In other words, while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. Interaction terms are commonly used in statistical models to capture the effects of multiple variables, but I do not see them used as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict whether or not a client will repay a loan.', '# Jake VanderPlas writes about polynomial features in his excellent book Python for Data Science for those who want more information.', '# In the following code, we create polynomial features using the EXT_SOURCE variables and the DAYS_BIRTH variable. Scikit-Learn has a useful class called PolynomialFeatures that creates the polynomials and the interaction terms up to a specified degree. We can use a degree of 3 to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into problems with overfitting).', '# ', '#  ,     ,   \xa0 , \xa0        .', '# ', '#    \xa0  \xa0 \xa0         ,  \xa0\xa0 . Scikit-Learn  PolynomialFeatures !', '# ', '#   3  degree \xa0?   .      \xa0, [](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)  \xa0 \xa0  .', '# ', '# \xa0 , EXT_SOURCE  3 DAYS_BIRTH  \xa0 4 \xa0.', '# In[61]:', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# In[62]:', '# Need to impute missing values', '# In[63]:', '#  NaN            \xa0  .', '# ', '# \xa0     \xa0, ndarray    .', '# In[64]:', '# Create the polynomial object with specified degree', '# In[65]:', '# Train the polynomial features', '# Transform the features', '# poly_features    EXT  3 DAYS_BIRTH \xa0 4  35  .', '# ', '#   polynomial   \xa0  `get_feature_names` .', '# In[66]:', '# 35   \xa0', '# \xa0 3 \xa0  \xa0,   \xad   .', '# ', '# \xa0,        .', '# In[67]:', '# Create a dataframe of the features', '# Add in the target', '#    \xa0 , polynomial feature \xa0, \xa0,    \xa0.', '#     \xa0.', '# In[68]:', '# In[69]:', '# Find the correlations with the target', '# Display most negative and most positive', '#           \xa0 .', '# ', '# , \xa0  \xa0 \xa0          .', '# ', '# \xa0,  \xa0  feature    \xa0       \xa0 .', '# ', '# \xa0   . Try!', '# In[70]:', '# Put test features into dataframe', '# In[71]:', '# In[72]:', '# In[73]:', '# Merge polynomial featueres into training dataframe', '# Merge polynomial features into testing dataframe', '# In[74]:', '# Align the dataframes', '# Print out the new shapes', '# align \xa0 join `inner` ,      \xa0,  app_train_poly TARGET       ! ,     . polynomial  1    ', '# In[75]:', '# DAYS_BIRTH_x,DAYS_BIRTH_y  .', '# ', '# EXT_SOURCE_1_x', '# ', '# EXT_SOURCE_2_x', '# ', '# EXT_SOURCE_3_x', '# ', '#   y\xad ', '# ', '# ', '# ### Domain Knowledge Features', '# ', '# >Maybe it\'s not entirely correct to call this ""domain knowledge"" because I\'m not a credit expert, but perhaps we could call this ""attempts at applying limited financial knowledge"". In this frame of mind, we can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan. Here I\'m going to use five features that were inspired by [this script](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by Aguiar:', ""# >* CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income"", ""# * ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income"", '# * CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due', ""# * DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age"", '# ', '# >Again, thanks to Aguiar and [his great script](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) for exploring these features.', '# ', '# ', '# In[76]:', '#    \xa0,     \xa0 \xa0   ', '# annuity : ', '# In[77]:', '# In[78]:', '# In[79]:', '# ### Visualize New Variables', '# ', '#    \xa0 domain knowledge .', '# ', '# \xa0  , TARGET [KDE(kernel density estimation plot) plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) \xa0 .', '# In[80]:', '# iterate through the new features', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '#    \xa0  \xa0\xa0 . \xa0      .', '# ## Baseline', '# >For a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition (random guessing on a classification task will score a 0.5).', ""# Since we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression."", '# ### Logistic Regression Implementation', '# >Here I will focus on implementing the model rather than explaining the details, but for those who want to learn more about the theory of machine learning algorithms, I recommend both [An Introduction to Statistical Learning and Hands-On Machine Learning with Scikit-Learn and TensorFlow. Both of these books present the theory and also the code needed to make the models (in R and Python respectively). They both teach with the mindset that the best way to learn is by doing, and they are very effective!', '# To get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). The following code performs both of these preprocessing steps.', '# ', '# \xa0    \xa0   \xa0,', '# ', '#  \xa0 \xa0  . , missing value ,  \xa0  \xa0  .', '# In[81]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# In[82]:', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# fit \xa0 Imputer strategy `median`  , ', '# \xa0   ?    \xa0   \xa0\xa0? .', '# Transform both training and testing data', '#  \xa0\xa0      ? ', '#  \xa0 \xa0 test         \xa0, \xa0   .', '# \xa0 missing value .', '# Repeat with the scaler(0~1)', '# >We will use LogisticRegressionfrom Scikit-Learn for our first model. The only change we will make from the default model settings is to lower the regularization parameter, C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models.', '# Here we use the familiar Scikit-Learn modeling syntax: we first create the model, then we train the model using .fit and then we make predictions on the testing data using .predict_proba (remember that we want probabilities and not a 0 or 1).', '#  \xa0        \xa0,      \xa0 \xa0  C  \xa0 ,  \xa0.', '# ', '#    fit\xa0,  .', '# In[83]:', '# Make the model with the specified regularization parameter', '# Train on the training data', ""# train_labels app_train['TARGET']  \xa0 "", '# (X, y)   \xa0,     \xa0 \xa0 ,      \xa0  .', '# >Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model predict.proba method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column.', '# ', '# >The following code makes the predictions and selects the correct column.', '# ', '#  \xa0 \xa0,  \xa0.', '# ', '#       \xa0. ', '# ', '#  , m x 2  \xa0  , m  ,', '# ', '#    0 \xa0 \xa0,    1  \xa0 \xa0.', '# ', '# ,  \xa0 \xa0 \xa0     \xa0.', '# In[84]:', '# Make predictions', '# Make sure to select the second column only', '# input   , \xa0   \xa0    .', '# In[85]:', '#  , \xa0   , \xa0  \xa0. , \xa0  !', '# ', '# `sample_submission.csv`\xa0', '# In[86]:', '# Submission dataframe', '# >The predictions represent a probability between 0 and 1 that the loan will not be repaid. If we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky.', '# ', '#  \xa0  \xa0 \xa0 \xa0,      \xa0 ,    \xa0 \xa0  \xa0 \xa0  .', '# In[87]:', '# Save the submission to a csv file.', '# >The submission has now been saved to the virtual environment in which our notebook is running. To access the submission, at the end of the notebook, we will hit the blue Commit & Run button at the upper right of the kernel. This runs the entire notebook and then lets us download any files that are created during the run.', '# Once we run the notebook, the files created are available in the Versions tab under the Output sub-tab. From here, the submission files can be submitted to the competition or downloaded. Since there are several models in this notebook, there will be multiple output files.', '# ', '# The logistic regression baseline should score around 0.671 when submitted.', '#  \xa0 , \xa0 .', '# ', '#   0.671 \xa0 \xa0 . \xa0 \xa0 commit  \xa0  !', '# ', '# * \xa0 public \xa0 0.67041 .', '# ', '#  \xa0.', '# ', '# ### Improved Model : Random Forest', ""# >To try and beat the poor performance of our baseline, we can update the algorithm. Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest."", '# ', '# (LogisticRegression) \xa0  \xa0, \xa0 \xa0 , Random Forest \xa0?         \xa0 .', '# ', '# Random Forest  hundreds of trees \xa0   ,   \xa0     .', '# ', '# [RandomForestClassifier ](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)', '# In[88]:', '#  /   \xa0 , classifier \xa0!', '# Make random forest classifier', '#  ? \xa0 \xa0   ', '# In[89]:', '# Train on the training data', '# In[90]:', '# Extract feature importances', 'feature_importance_value = random_forest.feature_importances_ # np.ndarray ', '# feature   \xa0 , \xa0   ', '# In[91]:', '# Make predictions on test data', '# In[92]:', '# Make a submission dataframe', '# In[93]:', '# Save the submission dataframe', '# \xa0   \xa0 0.678.', '# ', '#  \xa0 0.67877 \xa0 .  \xa0  \xa0?    \xa0     .', '# \xa0      ,   .', '# ### Make Predictions using Engineered Features', '# ', '# >The only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering.', '# ', '# Polynomial feature, Domain Knowledge feature \xa0      .', '# ', '# , submission \xa0 \xa0   feature   \xa0   .', '# In[94]:', '# Impute the polynomial features', '# Scale the polynomial features', '# In[95]:', '# Train on the training data', '# \xa0 \xa0 ,     \xa0    \xa0, \xa0 \xa0 ,', '#  train_labels .', '# Make predictions on the test data', '# In[96]:', '# In[97]:', '# Make a submission dataframe', '# Save the submission datafrmae', '#  , public \xa0 0.60   . \xa0 0.678   .', '# ', '#     ,  polynomial feature   . \xa0 \xa0 \xa0 .', '# ### Testing Domain Feature', '# ', '# \xa0      feature , domain feature   .', '# In[98]:', '# In[99]:', '# \xa0   .', '#    , ', '# Impute the domain nomial features', '# Scale the domainnomial features', '# Create Random Forest object for domain features data', '# Train on the training data', '# Extract feature importances,    \xa0 ', '# Make prediction on the test data', '# In[100]:', '# Make a submission dataframe', '# Save the submission dataframe', '# domain features  \xa0  0.679 .', '# ', '# \xa0     .(,   , the Gradient Boosting  \xa0 )', '# ', '# , feature engineering   ()   \xa0.  \xa0 \xa0   ', '# ### Model Interpretation: Feature Importances', '#  \xa0      , feature importance   .', '# ', '# EDA        , EXT_SOURCE DAYS_BIRTH.', '# ', '#     \xa0     ?  \xa0    .', '# In[101]:', '    # Sort features according to importance', '    # reset_index() , sorting     row   0  !', '    # Normalize the feature importances to add up to one', '    # \xa0 ?   1 ,      \xa0  .(0,1)  .', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    #      list   \xa0, ', '    #   (object) ? ,    .', '    # Set the yticks and labels, y   \xa0\xa0, y    \xa0', '    # Plot labeling', '# In[102]:', '# Show the feature importances for the dafault features', '#  , `EXT_SOURCE` `DAYS_BIRTH`   feature    .', '# ', '#      feature importance  \xa0  , \xa0   factor \xa0    .', '# In[103]:', '#  feature   15      \xa0  .', '# ## \xa0', '#   , \xa0     .', '# ', '# ,  ,     \xa0 .\\', '# ', '#   ,  EDA   , \xa0,  \xa0  . // \xa0,  \xa0, \xa0  \xa0,     ', '# ', '#  , baseline ,  \xa0,      \xa0, feature        \xa0 \xa0.', '# ', '# ', '# ', '# >We followed the general outline of a machine learning project:', '# 1. Understand the problem and the data', '# 2. Data cleaning and formatting (this was mostly done for us)', '# 3. Exploratory Data Analysis', '# 4. Baseline model', '# 5. Improved model', '# 6. Model interpretation (just a little)', '# ', '# ', '# ', '# ', '# ## Just for Fun : Light Gradient Boosting Machine', '# ,  \xa0 \xa0 .', '# ', '# LightGBM  the gradient boosting machine .', '# ', '# GBM     \xa0 .  .    \xa0    .', '# ', '# , \xa0\xa0     \xa0     . .', '# In[104]:', '#   ', '# In[105]:', '    # Extract the ids', '    # Extract the labels for training', '    # Remove the ids and target', '    # One Hot Encoding', '        # Align the dataframes by the columns', '        # No categorical indices to record', '    # Integer label encoding', '        # Create a label encoder', '        # List for storing categorical indices', '        # Iterate through each column', '                # Map the categorical features to integers', '                # Record the categorical indices', '    # Catch error if label encoding scheme is not valid', '    # Extract feature names', '    # Convert to np arrays', '    # Create the kfold object', '    # Empty array for feature importances', '    # Empty array for test predictions', '    # Empty array for out of fold validation predictions', '    # Lists for recording validation and training scores', '    # Iterate through each fold', '        # Training data for the fold', '        # Validation data for the fold', '        # Create the model', '        # Train the model', '        # Record the best iteration', '        # Record the feature importances', '        # Make predictions', '        # Record the out of fold predictions', '        # Record the best score', '        # Clean up memory', '    # Make the submission dataframe', '    # Make the feature importance dataframe', '    # Overall validation score', '    # Add the overall scores to the metrics', '    # Needed for creating dataframe of validation scores', '    # Dataframe of validation scores', '# In[106]:', '# In[107]:', '# In[108]:', '# 0.733  . \xad \xa0   .      ..', '# \xa0,  , feature\xad     , domain knowledge \xa0 \xa0,', '# ', '#  .', '# In[109]:', '# Test the domain knowledge features', '# In[110]:', '# ,  feature   .', '# ', '# CREDIT_TERM        . ,  \xa0      .', '# In[111]:', '#  Feature Engineering!', '# !', '# In[ ]:']",750
home-credit-lightgbm.py,"['# coding: utf-8', '# In[ ]:', '# ### Content', '# 1. **Preprocess application_train.csv and application_test.csv**', '#     * Add a New Feature (Train and Test)', '#     * Plot  of  New Feature on Train and Test', '# 2. ** Preprocess bureau.csv and bureau_balance.csv**', '#     * Graph of Catergorical Data', '#     * Add a New Feature in Bureau', '# 3. **Preprocess previous_applications.csv**', '#     * A few graph of Categorical Variable Previous_Application', '#     * Add a new Feature in Previous_Application', '# 4. **Preprocess POS_CASH_balance.csv**', '#     * Add a new Feature in POS_CASH_balance', '# 5. **Preprocess installments_payments.csv**', '#    * Add a New Feature in installments_payments', '# 6. **Preprocess credit_card_balance.csv**', '#     *  Add a New Feature for credit_card_balance', '# 7. *Combine all Feature*', '#     1. Train and Test', '#     1. Process bureau and bureau_balance', '#     1. Process previous_applications', '#     1. Process POS-CASH balance', '#     1. Process installments payments', '#     1. Process credit card balance', '# 8. **Split into Train and Test**', '# 9. *Apply Algorithm *', '#     *  LightGBM Classifier Algorithm', '#     *  XGBClassifier Algorithm', '#     *  CatBoostClassifier Algorithm', '# 10. *Top 30 Feature of All Algorithm*', '#     * Plot of LightGBM Classifier Algorithm', '#     * Plot of XGBClassifier Algorithm', '#     * Plot of CatBoostClassifier Algorithm', '#  ', '# ', '# ', '# In[ ]:', '# In[ ]:', '# One-hot encoding for categorical columns with get_dummies', '# In[ ]:', '# ', '# ### Preprocess application_train.csv and application_test.csv', '# #### Read data and merge (Train and Test)', '# ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Categorical features: Binary features and One-Hot encoding', '# NaN values for DAYS_EMPLOYED: 365.243 -> nan', '# #### Add a New Feature (Train and Test)', '# 1. DAYS_EMPLOYED_PERC', '# 1. INCOME_CREDIT_PERC', '# 1. INCOME_PER_PERSON', '# 1. ANNUITY_INCOME_PERC', '# ', '# In[ ]:', '# In[ ]:', '# #### Plot - Add a New Feature on Train and Test', '# In[ ]:', ""# data = [go.Histogram(x=a,marker=dict(color='#1F77B4'),opacity=0.75)]"", ""# layout = go.Layout(title='ANNUITY_INCOME_PERC',xaxis=dict(title='Value'),"", ""#     yaxis=dict(title='Count'))"", '# fig = go.Figure(data=data, layout=layout)', ""# py.iplot(fig, filename='ANNUITY_INCOME_PERC')"", ""# data = [go.Histogram(x=b,marker=dict(color='#FF7F0E'),opacity=0.75)]"", ""# layout = go.Layout(title='INCOME_CREDIT_PERC',xaxis=dict(title='Value'),"", ""#     yaxis=dict(title='Count'))"", '# fig = go.Figure(data=data, layout=layout)', ""# py.iplot(fig, filename='INCOME_CREDIT_PERC')"", ""# data = [go.Histogram(x=c,marker=dict(color='#2CA02C'),opacity=0.75)]"", ""# layout = go.Layout(title='INCOME_PER_PERSON',xaxis=dict(title='Value'),"", ""#     yaxis=dict(title='Count'))"", '# fig = go.Figure(data=data, layout=layout)', ""# py.iplot(fig, filename='INCOME_PER_PERSON')"", ""# data = [go.Histogram(x=d,marker=dict(color='#D62728'),opacity=0.75)]"", ""# layout = go.Layout(title='ANNUITY_INCOME_PERC',xaxis=dict(title='Value'),"", ""#     yaxis=dict(title='Count'))"", '# fig = go.Figure(data=data, layout=layout)', ""# py.iplot(fig, filename='ANNUITY_INCOME_PERC')"", '# del d,data', '# gc.collect()', '# In[ ]:', '# ## Preprocess bureau.csv and bureau_balance.csv', '# In[ ]:', '# In[ ]:', '# #### Graph of Catergorical Data ', '# In[ ]:', '# a = bureau[""CREDIT_TYPE""].value_counts()', ""# a = pd.DataFrame({'labels': a.index,"", ""#                    'values': a.values"", '#                   })', ""# a.iplot(kind='pie',labels='labels',values='values', title='CREDIT_TYPE\\'s', hole = 0.5)"", '# b = bureau[""CREDIT_ACTIVE""].value_counts()', ""# b = pd.DataFrame({'labels': b.index,"", ""#                    'values': b.values"", '#                   })', ""# b.iplot(kind='pie',labels='labels',values='values', title='CREDIT_ACTIVE\\'s', hole = 0.5)"", '# c = bureau[""CREDIT_CURRENCY""].value_counts()', ""# c = pd.DataFrame({'labels': c.index,"", ""#                    'values': c.values"", '#                   })', ""# c.iplot(kind='pie',labels='labels',values='values', title='CREDIT_CURRENCY\\'s', hole = 0.5)"", '# del a,b,c', '# gc.collect()', '# In[ ]:', '# #### Add a New Feature in Bureau', '# In[ ]:', '# Bureau balance: Perform aggregations and merge with bureau.csv', '# In[ ]:', '# Bureau and bureau_balance numeric features', '# In[ ]:', '# Bureau and bureau_balance categorical features', '# In[ ]:', '# Bureau: Active credits - using only numerical aggregations', '# Bureau: Closed credits - using only numerical aggregations', '# In[ ]:', '# ### Preprocess previous_applications.csv', '# In[ ]:', '# In[ ]:', '# #### A few graph of Categorical Variable Previous_Application', '# 1. NAME_CONTRACT_TYPE', '# 1. WEEKDAY_APPR_PROCESS_START', '# 1. NAME_SELLER_INDUSTRY', '# 1. PRODUCT_COMBINATION', '# In[ ]:', '# a = prev[""NAME_CONTRACT_TYPE""].value_counts()', ""# a = pd.DataFrame({'labels': a.index,"", ""#                    'values': a.values"", '#                   })', ""# a.iplot(kind='pie',labels='labels',values='values', title='NAME_CONTRACT_TYPE\\'s', hole = 0.5)"", '# a = prev[""WEEKDAY_APPR_PROCESS_START""].value_counts()', '# a.iplot(kind=\'bar\', xTitle = \'Week Name\', yTitle = ""Count"", title = \'WEEKDAY_APPR_PROCESS_START\', color = \'red\')', '# a = prev[""NAME_SELLER_INDUSTRY""].value_counts()', '# a.iplot(kind=\'bar\', xTitle = \'NAME_SELLER_INDUSTRY Name\', yTitle = ""Count"", title = \'NAME_SELLER_INDUSTRY\', color = \'green\')', '# a = prev[""PRODUCT_COMBINATION""].value_counts()', '# a.iplot(kind=\'bar\', xTitle = \'PRODUCT_COMBINATION Name\', yTitle = ""Count"", title = \'PRODUCT_COMBINATION\', color = \'blue\')', '# del a', '# gc.collect()', '# In[ ]:', '# Days 365.243 values -> nan', '# #### Add a new Feature in Previous_Application', '# In[ ]:', '# In[ ]:', ""# a = prev['APP_CREDIT_PERC'].tolist()"", ""# a = [x for x in a if str(x) != 'nan']"", ""# data = [go.Histogram(x=a,marker=dict(color='#1F77B4'),opacity=0.75)]"", ""# layout = go.Layout(title='APP_CREDIT_PERC',xaxis=dict(title='Value'),"", ""#     yaxis=dict(title='Count'))"", '# fig = go.Figure(data=data, layout=layout)', ""# py.iplot(fig, filename='APP_CREDIT_PERC')"", '# del a,data', '# gc.collect()', '# In[ ]:', '# Previous applications numeric features', '# In[ ]:', '# Previous applications categorical features', '# In[ ]:', '# Previous Applications: Approved Applications - only numerical features', '# Previous Applications: Refused Applications - only numerical features', '# In[ ]:', '# In[ ]:', '# ### Preprocess POS_CASH_balance.csv', '# In[ ]:', '# In[ ]:', '# #### Add a new Feature in POS_CASH_balance', '# In[ ]:', '# Features', '# In[ ]:', '# Count pos cash accounts', '# In[ ]:', '# In[ ]:', '# ### Preprocess installments_payments.csv', '# In[ ]:', '# #### Add a New Feature', '# In[ ]:', '# Percentage and difference paid in each installment (amount paid and installment value)', '# Days past due and days before due (no negative values)', '# In[ ]:', '# Features: Perform aggregations', '# Count installments accounts', '# In[ ]:', '# In[ ]:', '# ### Preprocess credit_card_balance.csv', '# In[ ]:', '# #### Read credit card balance', '# In[ ]:', '# In[ ]:', '# a = cc[""NAME_CONTRACT_STATUS""].value_counts()', ""# a = pd.DataFrame({'labels': a.index,"", ""#                    'values': a.values"", '#                   })', ""# a.iplot(kind='pie',labels='labels',values='values', title='NAME_CONTRACT_STATUS\\'s', hole = 0.5)"", '# del a', '# gc.collect()', '# #### Add a New Feature for credit_card_balance ', '# In[ ]:', '# General aggregations', '# Count credit card lines', '# In[ ]:', '# In[ ]:', '# ## Combine all Feature', '# 1. Train and Test', '# 1. Process bureau and bureau_balance', '# 1. Process previous_applications', '# 1. Process POS-CASH balance', '# 1. Process installments payments', '# 1. Process credit card balance', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ### Split into Train and Test', '# In[ ]:', '# ### *Apply Algorithm *', '#     * LightGBM Classifier Algorithm', '#     * XGBClassifier Algorithm', '#     * CatBoostClassifier Algorithm', '# ### LightGBM Classifier Algorithm', '# In[ ]:', '# In[ ]:', '# imp = clf_lgbm.feature_importances_', '# col_name = data_only.columns', ""# d = {'name': col_name,'value':imp}"", '# d = pd.DataFrame(data =d)', ""# d = d.sort_values(['value'], ascending=False)"", ""# temp = d.set_index('name')"", '# temp[:50].iplot(kind=\'bar\',title=""Feature IMportant by LGBMClassifier "")', '# del clf_lgbm, temp', '# gc.collect()', '# ### XGBClassifier Algorithm', '# In[ ]:', '# clf_xgBoost = xgb.XGBClassifier(', ""#     learning_rate =0.01, n_estimators=1000, max_depth=4, min_child_weight=4, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', "", '#             nthread=4, scale_pos_weight=2, seed=27)', '# # Fit the models', '# clf_xgBoost.fit(data_only,labels)', '# In[ ]:', '# imp = clf_xgBoost.feature_importances_', '# col_name = data_only.columns', ""# d1 = {'name': col_name,'value':imp}"", '# d1 = pd.DataFrame(data =d1)', ""# d1 = d1.sort_values(['value'], ascending=False)"", ""# temp = d1.set_index('name')"", '# temp[:50].iplot(kind=\'bar\',title=""Feature IMportant by XGBClassifier"")', '# del clf_xgBoost, temp', '# gc.collect()', '# ### CatBoostClassifier Algorithm', '# In[ ]:', '# clf_catboost = CatBoostClassifier(iterations=1200,', '#                               learning_rate=0.1,', '#                               depth=7,', '#                               l2_leaf_reg=40,', ""#                               bootstrap_type='Bernoulli',"", '#                               subsample=0.7,', '#                               scale_pos_weight=5,', ""#                               eval_metric='AUC',"", '#                               metric_period=50,', ""#                               od_type='Iter',"", '#                               od_wait=45,', '#                               random_seed=15,', '#                               allow_writing_files=False)', '# clf_catboost.fit(data_only,labels,verbose=True)', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",282
home-credit-nn-ae-lgb.py,"['# coding: utf-8', '# **Importing libraries**', '# In[ ]:', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# **Application train and test data**', '# In[ ]:', '    # New features', '# **Bureau & Bureau_Balance**', '# In[ ]:', '    # getting the furthest date attached to bureau_id', '    # Status of bureau_id as per freshest month', '# **previous_application**', '# In[ ]:', '    # filtering the invalid contracts', '    # count of loan status', '    # new features', '# In[ ]:', '    # new features', '# **POS_CASH_balance**', '# In[ ]:', '# **installment_payments**', '# In[ ]:', '    # features for last month active loans', '# **credit_card_balance**', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# **Modeling**', '# In[ ]:', '# **LGBM**', '# In[ ]:', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '           # scale_pos_weight=1,', '    # Write submission file and plot feature importance', '# **ANN**', '# In[ ]:', '# **AE**', '# In[ ]:', '# **submission**', '# In[ ]:', '# **Prediction**', '# In[ ]:', '    # train test splitting', '    # up sampling', '# In[ ]:', '# In[ ]:']",50
home-credit-risk-soln-78-3.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '    # Get all the models tested so far in DataFrame format', '    # Get current parameters and the best parameters    ', '    # Save all model results', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# # Examine missing value', '# In[ ]:', '# Function to calculate missing values by column# Funct ', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[ ]:', '# Missing values statistics', '# In[ ]:', '# In[ ]:', '# Number of unique classes in each object column', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# #Pre-processing buro_balance', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ', '# Pre-processing previous_application', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ', '# #Pre-processing buro', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ', '# #Pre-processing POS_CASH', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ', '# #Pre-processing credit_card', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ', '# #Pre-processing payments', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ', '# #Join data bases', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ', '# #Remove features with many missing values', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",114
home-credit-risk-with-detailed-feature-engineering.py,"['# coding: utf-8', '# # DESCRIPTION', '# ', '# Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.', '# ', ""# Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities."", '# ', ""# While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful."", '# # Data Description', '# ', '#     * application_{train|test}.csv', '#         This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).', '#         Static data for all applications. One row represents one loan in our data sample.', '# ', '#     * bureau.csv', ""#         All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample)."", '#         For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.', '# ', '#     * bureau_balance.csv', '#         Monthly balances of previous credits in Credit Bureau.', '#         This table has one row for each month of history of every previous credit reported to Credit Bureau  i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.', '# ', '#     * POS_CASH_balance.csv', '#         Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.', '#         This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample  i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.', '# ', '#     * credit_card_balance.csv', '#         Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.', '#         This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample  i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.', '# ', '#     * previous_application.csv', '#         All previous applications for Home Credit loans of clients who have loans in our sample.', '#         There is one row for each previous application related to loans in our data sample.', '# ', '#     * installments_payments.csv', '#         Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.', '#         There is a) one row for every payment that was made plus b) one row each for missed payment.', '#         One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.', '# ', '#     * HomeCredit_columns_description.csv', '#         This file contains descriptions for the columns in the various data files.', '# ', '# # Libraries', '# In[1]:', '# # Helper Functions', '# In[2]:', '# In[3]:', '# Display/plot feature importance', '# In[4]:', '# # application_train', '# In[5]:', '# # bureau & bureau_balance', '# In[6]:', '    # Degisken isimlerinin yeniden adlandirilmasi ', '    # Status_sum ile ilgili yeni bir degisken olusturma', '    # bureau_bb tablosundaki kategorik degiskenlere One Hot Encoding uygulanmasi', '    # CREDIT_CURRENCY degiskeninin %99u currency1, bu sebeple ayirt ediciligi olmayacagini dusundugumuz icin sildik  ', '    # bureau_bb_agg tablosuna aggreagation islemlerinin uygulanamasi  ', '    # Degisken isimlerinin yeniden adlandirilmasi ', '    # kisinin aldg en yuksek ve en dusuk kredinin farkn gsteren yeni degisken', '    # ortalama kac ayda bir kredi cektigini ifade eden  yeni degisken', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# # installments_payments', '# In[7]:', '    # Her bir taksit demesinin gec olup olmama durumu 1: gec dedi 0: erken demeyi temsil eder', '    # Agrregation ve degisken tekillestirme', '    # Multi index problemi czm', '    # drop variables ', '    # Kredi deme yzdesi ve toplam kalan borc', '# # pos_cash_balance', '# In[ ]:', '# In[8]:', '    # Kategorik Degiskenimizi Dummy Degiskenine Dnstrme', '    # Aggregation Islemi - Tekillestirme', ""    # Multilayer index'i tek boyutlu index'e dnstrme"", '    # SK_DPD kac kredide 0 olma durumu (SK_DPD MAX alacagiz 0 durumunu veriyor) ', '    # SK_DPD_DEF (SK_DPD_DEF_MAX sifir olma durumunu veriyor)', '    # CNT_INSTALMENT_FUTURE_MIN==0 oldugunda NAME_CONTRACT_STATUS_Completed_SUM==0 olma durumu ', '    # 1:kredi zamaninda kapanmamis 0:kredi zamaninda kapanmis', '# # credit_card_balance', '# In[9]:', ""    CCB = pd.get_dummies(CCB, columns= ['NAME_CONTRACT_STATUS'] )  # artik tumu sayisal "", '    # Bu fonksiyon, kac defa odemelerin geciktigini hesaplar   # Function to calculate number of times Days Past Due occurred ', '        # DPD ile beklenen bir seri: SK_DPD degiskeninin her bir prev_app daki gecmis kredi icin olan degerleri  # DPD is a series of values of SK_DPD for each of the groupby combination ', '        # We convert it to a list to get the number of SK_DPD values NOT EQUALS ZERO', '        P = len(M)        # P: taksit sayisi', '        # Find the count of transactions when Payment made is less than Minimum Payment ', ""    CCB['CASH_CARD_RATIO1'] = (CCB['DRAWINGS_ATM']/CCB['DRAWINGS_TOTAL'])*100  # ATM den cektigi nakit / toplam cektigi"", ""    CCB['DRAWINGS_RATIO1'] = (CCB['TOTAL_DRAWINGS']/CCB['NUMBER_OF_DRAWINGS'])*100     # yuzdelik degil, genisleme yapmis"", '# In[ ]:', '# # previous_application', '# In[10]:', '    # ""WEEKDAY_APPR_PROCESS_START""  deikeninin  WEEK_DAY ve WEEKEND olarak iki kategoriye ayrlmas', '    # ""HOUR_APPR_PROCESS_START""  deikeninin working_hours ve off_hours olarak iki kategoriye ayrlmas', '    # DAYS_DECISION deeri 1 yldan kk olanlara 1, byk olanlara 0 deeri verildi.', '    # ""NAME_TYPE_SUITE""  deikeninin alone ve not_alone olarak iki kategoriye ayrlmas', '    # ""NAME_GOODS_CATEGORY""  deikenindeki bu deerler others olarak kategorize edilecek', '    # ""NAME_SELLER_INDUSTRY""  deikenindeki bu deerler others olarak kategorize edilecek', '    # stenilen krecinin verilen krediye oran ieren deikeni tretir', '    # stenilen krecinin verilen krediye oran ieren deikeni tretir', '    # deme gnn geciktirmi mi bunu gsteren churn_prev deikeni tretilir.', '    # 1= geciktirmi, 0 = geciktirmemi, NaN = bo deer', '    # NFLAG_INSURED_ON_APPROVAL deikeni yerine kullanlmak izere NEW_INSURANCE deikeni tanmland.', '    # INTEREST_RATE deikenini oluturur.', '    # Previous tablosundaki kategorik deikenlerin isimlerini tutar.', '# In[ ]:', '# # Combine', '# In[11]:', '# In[ ]:', '# # Model Tuning', '# In[12]:', '#              ""n_estimators"": [200, 500, 100],', '#              ""max_depth"":[1,2,35,8]}', '# In[13]:', '# In[ ]:', '# # Machine Learning', '# In[14]:', '# # main', '# In[15]:', '# In[16]:', '# In[ ]:', '# In[ ]:']",123
home-credit-risk.py,"['# coding: utf-8', '# # Main Work Flow', '# - **Import libraries**', '# - **Cleaning of training dataset**', '# - **Choice of method**', '# - **Preprocessing of testing dataset**', '# - **Transformation and post-processing**', '# In[ ]:', '# import lightgbm as lgb', '# In[ ]:', '# ## DATA READING', '# **Input File Names**', '# In[ ]:', '# del df_application_train', '# del df_application_test', ""# df_application_train = pd.read_csv('../input/application_train.csv')"", ""# df_application_test = pd.read_csv('../input/application_test.csv')"", '# In[ ]:', '# ## Data manupulation outliers', '# In[ ]:', '# ## TRAINING DATA SET', '# **Descriptive stats about the data set**', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# df_bureau1 = pd.merge( ', ""#              pd.merge( df_application_test,df_bureau, on='SK_ID_CURR', how='left' ),"", ""#                        df_bureau_balance, on='SK_ID_BUREAU', how='left' )"", '# In[ ]:', ""# df_bureau_balance.set_index('SK_ID_BUREAU').head(2)"", '# In[ ]:', '# def join_df(a,b,key,rsuff):', ""#     temp = a.join(b.set_index(key), on=key, rsuffix=rsuff, how='left' )"", '#     return temp;', '# def join_df_serise(dfra, b1,b2, p1,p2,p3,p4):', ""#     tempb= join_df(b1,b2,'SK_ID_BUREAU','')"", '#     temp = join_df(', '#            join_df(', '#            join_df(', ""#            join_df(dfra,p1,'SK_ID_CURR','_p1'), "", ""#                         p2,'SK_ID_CURR','_p2'), "", ""#                         p3,'SK_ID_CURR','_p3') , "", ""#                         p4,'SK_ID_CURR','_p4')"", ""#     temp = join_df(temp,tempb,'SK_ID_CURR','_b')"", '#     del tempb', '#     return temp;', '# In[ ]:', '# In[ ]:', '    # Remove some empty features', '# In[ ]:', '# Remove some rows with values not present in test set', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Data Manipulation', '# In[ ]:', '# In[ ]:', '# In[ ]:', '#             print(col)', '#             print(len(df_list),df_list)', '# In[ ]:', '# ## EDA', '# In[ ]:', '# In[ ]:', ""# plt.yscale('log')"", '# plt.ylim(10**-1,10**7)', '# In[ ]:', '# In[ ]:', ""# plt.yscale('log')"", '# plt.ylim(10**3,10**7)', '# # Validation', ""# - The test data doesn't include y_train values "", '# - Test few methodologies to model the system ', '# - Test the accuracy of the methods', ""# **Let's split up the data into a training set and a test set!**"", '# ## Train', '# In[ ]:', '# In[ ]:', '# XX_train, XX_test, yy_train, yy_test = train_test_split(X_train, y_train, test_size=0.30)', '# ## Fit & Test', '# In[ ]:', '# import sklearn.pipeline', '# scaler = sklearn.preprocessing.StandardScaler()', '# import lightgbm as lgb', '# # train', ""# gbm = lgb.LGBMRegressor(objective='binary',#'regression',"", ""#                         metric = 'binary_logloss',"", ""#                         boosting_type='gbdt',"", '#                         num_leaves=1001,', '#                         learning_rate=0.0005,', '#                         n_estimators=200)', ""# steps = [('scaler', scaler),"", ""#         ('GBM', gbm)]"", '# pipeline = sklearn.pipeline.Pipeline(steps)', '# ### fit pipeline on X_train and y_train', '# pipeline.fit( XX_train, yy_train)', '# ### call pipeline.predict() on X_test data to make a set of test predictions', '# yy_gbm = pipeline.predict( XX_test )', '# In[ ]:', '# mean = 0.5', '# results = yy_gbm + (1-mean)', '# predictions  = list(map(int, results))', ""# print('MAE:', metrics.mean_absolute_error(yy_test, predictions))"", ""# print('MSE:', metrics.mean_squared_error(yy_test, predictions))"", ""# print('RMSE:', np.sqrt(metrics.mean_squared_error(yy_test, predictions)))"", '# # TEST Data', '# In[ ]:', '# In[ ]:', '#             print(col)', '#             print(len(df_list),df_list)', '# ## TEST DATA SET - Preprocessing', '# **(a) Train and Test Datasets**', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# import sklearn.pipeline', '# scaler = sklearn.preprocessing.StandardScaler()', '# import lightgbm as lgb', '# # train', ""# # gbm = lgb.LGBMRegressor(objective='binary',#'regression',"", ""# #                         metric = 'binary_logloss',"", ""# #                         boosting_type='gbdt',"", '# #                         num_leaves=1001,', '# #                         learning_rate=0.0005,', '# #                         n_estimators=200)', '# gbm = lgb.LGBMRegressor(', '#             nthread=4,', '#             n_estimators=50000,', '#             learning_rate=0.0001,', '#             num_leaves=34,', '#             colsample_bytree=0.9497036,', '#             subsample=0.8715623,', '#             max_depth=8,', '#             reg_alpha=0.041545473,', '#             reg_lambda=0.0735294,', '#             min_split_gain=0.0222415,', '#             min_child_weight=39.3259775,', '#             silent=-1,', '#             verbose=-1) ', ""# steps = [('scaler', scaler),"", ""#         ('GBM', gbm)]"", '# pipeline = sklearn.pipeline.Pipeline(steps)', '# ### fit pipeline on X_train and y_train', '# pipeline.fit( X_train, y_train)', '# ### call pipeline.predict() on X_test data to make a set of test predictions', '# y_gbm = pipeline.predict( X_test )', '# In[ ]:', '# sns.distplot(y_gbm)', '# ## Saving predicted data to a file', '# In[ ]:', '# target = []', '# for y in y_gbm:', ""#     target.append( '{:.{prec}f}'.format(y, prec=1) ) "", '# In[ ]:', ""# pd.DataFrame( { 'SK_ID_CURR':list(df_test['SK_ID_CURR']),"", ""#                 'TARGET':target } ).set_index('SK_ID_CURR').to_csv('sample_submission.csv', sep=',')"", '# In[ ]:', '# In[ ]:', '        # LightGBM parameters found by Bayesian optimization', '#         clf = LGBMClassifier(', ""#             objective='binary',"", ""#             metric = 'auc',"", ""#             boosting_type='gbdt',"", '#             nthread=4,', '#             num_leaves=100,', '#             learning_rate=0.03,', '#             n_estimators=1000 )', '#         gc.collect()', '# In[ ]:', '    # Plot feature importances', '# In[ ]:', '    # Plot ROC curves', '        # Plot the roc curve', '# In[ ]:', '    # Plot ROC curves', '        # Plot the roc curve', '# In[ ]:', '# Create Folds', '# Train model and get oof and test predictions', '# Display a few graphs', '# In[ ]:', '# In[ ]:', '# In[ ]:']",185
home-credit.ipynb,[],0
home-credit.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# ### Creating combined dataframe from train and test file', '# - Purpuse of combining train and test file is to handle data modification at same time on both file', '# - Once data pre-processing is done we can easily split it again with below logic', '# - if TARGET=NaN meaning its test file else its train file', '# In[6]:', '# In[7]:', '# In[8]:', '# ### Considering basic numeric features', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# - Creating dataframe with required columns only', '# In[13]:', '# In[14]:', '# ## EDA And Pre-Processing ', '# In[15]:', '# In[16]:', '# ### Handling missing values', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[ ]:', '# In[24]:', '# In[25]:', '# In[26]:', '# Heatmap', '# - Name Type Suite and Occupation type has missing values', '# - Occupation type has lots of missing value so for now droping this column', '# - Name Type suite will create some dummy NTS_XNA category for now', '# In[ ]:', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# - Draw distribution of numeric features', '# In[40]:', '# In[41]:', '# In[42]:', '# In[43]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[44]:', '# In[45]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# - Handling Outlier', '# In[46]:', '# In[47]:', '# In[48]:', '# In[49]:', '# - found that DAYS_EMPLOYED has some anomalies', ""# - Around 18% of data amongs all data has some '365243' value in this fields"", '# - as its not make sence to current data so we need to handle it somehow', '# - so i am replacing this value with np.nan', '# - creating new column called DAYS_EMPLOYED_ANOM Anomalous flag which will have True or False value based on this field', '# In[50]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# After removing anomalies we can see above histogram that DAYS_EMPLOYED has maximum as 49 years and minimum is 0 year as discribe below', '# ### creating combined basic features from numerical and categorical', '# In[51]:', '# In[52]:', '# In[53]:', '# In[ ]:', '# In[54]:', '# In[55]:', '# In[56]:', '# In[57]:', '# In[58]:', '# In[59]:', '# In[60]:', '# In[61]:', '# ### Lable encoding for categorical features whose values are binary like Y/N, Yes/No, True/False, M/F etc.', '# In[62]:', '# In[63]:', '# Categorical features with Binary encode (0 or 1; two categories)', '# In[64]:', '# out of above basic categorical features we already encoded binary ', '# - FLAG_OWN_CAR', '# - FLAG_OWN_REALITY', '# - CODE_GENDER', '# - DAYS_EMPLYED_ANOM', '# ', '# Now doing one hot encoding for remaining features', '# - NAME_CONTRACT_TYPE', '# - NAME_TYPE_SUITE', '# - NAME_INCOME_TYPE', '# - NAME_EDUCATION_TYPE', '# - NAME_FAMILY_STATUS', '# - NAME_HOUSING_TYPE', '# - ORGANIZATION_TYPE', '# In[65]:', '# In[66]:', '# In[67]:', '# In[68]:', '# In[69]:', '# In[70]:', '# In[71]:', '# In[72]:', '# In[73]:', '# ### creating final dataframe with required features', '# In[74]:', '# In[75]:', '# In[76]:', '# In[77]:', '# In[78]:', '# ## Model 1 : Logistic Regression', '# In[79]:', '# Make the model with the specified regularization parameter', '# In[80]:', '# In[81]:', '# In[82]:', '# In[83]:', '# In[84]:', '# In[85]:', '# In[86]:', '# In[87]:', '# In[88]:', '# Train on the training data', '# In[89]:', '# Make predictions', '# Make sure to select the second column only', '# In[90]:', '# In[91]:', '# In[92]:', '# ## Dealing with Imbalance Data using SMOTE', '# In[93]:', '# In[94]:', '# In[95]:', '# In[96]:', '# In[97]:', '# In[ ]:', '# In[ ]:']",153
home-crerit-default-risk-002.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# One-hot encoding for categorical columns with get_dummies', '# In[4]:', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Categorical features: Binary features and One-Hot encoding', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Some simple new features (percentages)', '# In[5]:', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[6]:', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# In[7]:', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# In[8]:', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[9]:', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# In[10]:', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[14]:', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# In[15]:', '# Display/plot feature importance', '# In[16]:', '# In[ ]:', '# In[ ]:']",60
home-value-prediction-light.py,"['# coding: utf-8', '# In[ ]:', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'from sklearn.preprocessing import LabelEncoder # Machine learning', 'import seaborn as sns # data visualization', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# In[ ]:', '# In[ ]:', '# Align the training and testing data, keep only columns present in both dataframes', '# In[ ]:', '# Drop the target from the training data', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[ ]:', '# Format the training and testing data ', '# 10 fold cross validation', '# Validation and test predictions', '# Iterate through each fold', '    # Training data for the fold', '    # Validation data for the fold', '    # LightGBM classifier with hyperparameters', '    # Fit on the training data, evaluate on the validation data', '    # Validation preditions', '    # Testing predictions', '    # Display the performance for the current fold', '    # Delete variables to free up memory', '# In[ ]:', '# In[ ]:', '# In[ ]:']",43
homecredit-auc190927.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", '# One-hot encoding', '# application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    #  (0 or 1; two categories)', '    #  with One-Hot encode', '#  bureau.csv and bureau_balance.csv', '    # bureau_balanceSK_ID_BUREAU', '    # bureau and bureau_balance', '    # Bureau and bureau_balance', '    # bureau\xad Closed credits,SK_ID_CURR', '# previous_applications.csv', '    # ', '    # Previous applications', '    # Previous applications', '    # Previous ApplicationNAME_CONTRACT_STATUS_Approved', '    # Previous Applications\xadSK_ID_CURR', '    # Previous Applications\xadSK_ID_CURR', '#  POS_CASH_balance.csv', '    # POS_CASH_balance', '    # pos cash', '# installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # \xa0', '    # installments account', '# credit_card_balance.csv', '    # SK_ID_PREV', '    # credit card', '# LightGBM GBDT with KFold or Stratified KFold', '    # Divide in training/validation and test data', '    # ', '    # \xad', '        # LightGBM', '    # \xad\xa0', '# ']",40
homecredit-automated-hyperparameter-tuning.py,"['# coding: utf-8', '# In[ ]:', ""# Will Koehrsen's notebook used as a reference"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'import lightgbm as lgb # modelling', '# evaluating the model', '# visualization', '# governing choices for search', '# Any results you write to the current directory are saved as output.', '# In[ ]:', '# immporting datasets and its splitting', '# sampling 16000 rows (10000 for training, 6000 for testing)', '# selecting only numeric features', '# extracting labels', '# In[ ]:', '# training set', '# In[ ]:', '# default hyperparameters', '# using early stopping to determine number of estimators and therefore deleting the default value', '# performing cross validation with early stopping', '# highest score', '# standard deviation of best score', '# In[ ]:', '# evaluating the baseline model on the testing data', '# optimal number of estimators found in cv', '# training and making predictions with model', '# In[ ]:', '# Objective Function which takes a set of Hyperparameter values and returns the cross validation score on the training data', ""# the following excerpt about objective function in Hyperopt taken from Will Koehrsen's notebook for future reference"", '# An objective function in Hyperopt must return either a single real value to minimize, or a dictionary with a key ""loss"" ', '# with the score to minimize (and a key ""status"" indicating if the run was successful or not).', '# Optimization is typically about minimizing a value, and because our metric is Receiver Operating Characteristic Area Under the Curve (ROC AUC) ', '# where higher is better, the objective function will return  1ROC AUC Cross Validation', '    # keeping track of evals', '    # using early stopping to find number of trees trained', '    # retrieving the subsample', '    # extracting the boosting types and subsample to top level keys', '    # ensuring the integer parameters remain as intergers', '    # performing n_folds cv', '    # extracting the best score', '    # minimizing loss', '    # boosting rounds that returned the highest score', '    # adding the number of estimators to the hyperparameters', ""    # writing to the csv file. 'a' mean append"", '    # dictionary with information for evaluation', '# In[ ]:', '# specifying domain (known as space in Hyperopt) is different from grid search. In Hyperopt and other Bayesian opt. frameworks', '# the domain is not a discrete grid but it consists of probability distribution of different hyperparameters', '# In[ ]:', '# again like grid search we will go through example of learning rate which is defined on a log scale', '# creating the learning rate', '# In[ ]:', '# visualization of the learning rate', '# drawing 1000 samples from the learning rate domain', '# In[ ]:', '# on the other, no. of leaves is an uniform distribution', '# sampling 10000 times from the number of leaves distribution', '# In[ ]:', '# conditional domain', '# using nested conditional statements to indicate hyperparameters that depend on other hyperparameters', '# for example, ""goss"" boosting_type cannot use subsampling, therefore we have to preset the ""subsample"" value as 1.0 in its case', '# boosting type domain', '# drawing a sample', '# In[ ]:', '# retrieve the subsample if present otherwise set it to a default of 1.0', '# extracting the boosting type', '# In[ ]:', ""# the gbm cannot use the distionary, therefore 'boosting_type' and 'subsample' have to be set as toplevel keys"", '# COMPLETE BAYESIAN DOMAIN', '# defining the search space', '# In[ ]:', '# sampling from the full space (domain)', '# conditional logic to assign top-level keys', '# will different outputs each time its run', '# In[ ]:', '# testing objective function with domain', '# creating a new file and opening a connection', '# writing column names', '# testing the objective function', '# results vary everytime', '# In[ ]:', '# OPTIMIZATION ALGORITHM', '# creating the algorithm', '# In[ ]:', '# automated hyperparameter tuning is informed unlike grid and random methods', '# hyperopt internally keeps a track of the results for the algos to use, but if we want to monitor the results and have a saved copy of the search, storage of results is required', '# Trials object stores the dictionary returned from the objective function', '# recording results', '# In[ ]:', '# Trials object will hold everything returned from objective functions in the .results attribute', '# Create a file and open a connection', '# Write column names', '# In[ ]:', '# now we have all the four parts of the hyperparameter optimization', '# In[ ]:', '# fmin takes the four parts defined above as well as the maximum number of iterations max_evals', '# running optimization', '# In[ ]:', '# the best object holds only the hyperparameter which returned the lowest loss function', '# for understanding how each search progresses, inspection of Trials object or csv file is required', '# sorting the trials with the lowest loss (highest AUC) first', '# In[ ]:', '# reading the csv file', '# In[ ]:', '# this function takes in the results, trains a model on the training data and evaluates on the testing data', '# returns a df of hyperparameters from the search. ', '# saving the results to a csv file converts the dictionary of hyperparameters to a string', '# mapping that back to a dictionary using ast.literal_eval', '    # string to dictionary', '    # sorting with the best values on top', '    # printing out cross validation high score', '    # using the best parameters to create a model', '    # training and making predictions', '    # creating dataframes of hyperparameters', '    # iterating through each set of hyperparameters that were evaluated', '    # putting the iteration and score in hyperparameter dataframe', '# In[ ]:', '# In[ ]:', '# continuing optimization', '# Hyperopt can continue searching where a previous search left off if we pass in a  Trials object that already has results', '# In[ ]:', '# saving the Trials object so it can be realater for more training', '# In[ ]:', '# to start the training from where it left off, simply load in the Trials object and pass it to an instance of fmin', '# Create a new file and open a connection', '# Write column names', '# Record results', '# Sort the trials with lowest loss (highest AUC) first', '# Save the trial results', '# In[ ]:', '# going through the results from 300 search iterations on the reduced dataset. Looking at the scores, the distribution of hyperparameter values tried, ', '# the evolution of values over time, and compare the hyperparameters values to those from random search.', '# In[ ]:', '# getting all the score in a df in order to plot them over the course of training', '# In[ ]:', '# best scores for plotting the best hyperparameter values', '# In[ ]:', '# plot of scores over the course of searching', '# In[ ]:', '# density plots of the learning rate distribution', '# In[ ]:', '# we can make the same plot now for all the hyperparameters', '# iterating through each hyperparameter', '# In[ ]:', '# evolution of hyperparameters over search', '# In[ ]:', '# Read in full dataset', '# Extract the test ids and train labels', '# In[ ]:', '# APPLYING TO FULL DATASET', '# In[ ]:', '# bayesian optimization on full dataset', '# cross validation with n_folds and early stopping', '# In[ ]:']",155
homecredit-default-risk-step-by-step-2nd-notebook.py,"['# coding: utf-8', '# ', '# ', '# ', '# ', '#  Home Credit Default Risk Step by Step: 2nd Notebook', '# ', '# ', '# ', '# ', '# ', '# [Home Credit Default Risk Step by Step: 1st Notebook](https://www.kaggle.com/ekrembayar/homecredit-default-risk-step-by-step-1st-notebook)', '# ', '# # 1. PACKAGES', '# In[1]:', '# 1. PACKAGES', '# -----------------------------------------------------------', '# Base', '# Model', '# Configuration', '# # 2. DATA', '# In[2]:', '# # 3. TRAIN-TEST SPLIT', '# In[3]:', '# Train-Test Split', '# # 4. MODEL', '# In[4]:', '# LightGBM parameters found by Bayesian optimization', '# # 5. Submission', '# In[5]:']",30
homecredit-morefiles.py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# matplotlib', '# In[ ]:', '# Training data', '# In[ ]:', '# Testing data', '# In[ ]:', '# one-hot encoding of categorical variables', '# In[ ]:', '# Align the training and testing data. Keep only columns present in both dataframes', '# Add the target back in', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Replace the anomalous values with nan', '# In[ ]:', '# Drop the target from the training data', '# Copy the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# In[ ]:', '# split the training set', '# In[ ]:', '# Make the mode with the specified regularization parameter', '# Train on the training data', '# In[ ]:', '# In[ ]:', '# Make the mode with the specified regularization parameter', '# Train on the training data', '# In[ ]:', '# Make predictions. Select 2nd column only for the probabiliy of not paying a loan', '# In[ ]:', '# Submission dataframe', '# In[ ]:', '# Save the submission to a csv file', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Make a submission dataframe', '# Save the submission dataframe', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Make a submission dataframe', '# Save the submission dataframe']",55
homecredit.py,"['# coding: utf-8', '# # Import', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Importing the dataset', '# # Preprocessing', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Destribution of target variable', '# In[ ]:', '# From the discributio it is clear that target class is imbalance', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Missing Values', '# I will create a datafram containing comulns and missing value percentage', '# In[ ]:', '# In[ ]:', '#  ## Treating Categorical Values', '#  First we will check total no of unique label in each column and for column containing <=2 label use LabelEncoding and for >2 OneHotEncoding', '# In[ ]:', '# ### Label Encoding', '# In[ ]:', '# ### One Hot Encoding', '# In[ ]:', '# ### Aligning training and test data', '# In[ ]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# ##  EDA', '# ### OUTLIERS', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ### CORRELATION', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# BINS', '# In[ ]:', '# Group by the bin and calculate averages', '# In[ ]:', '# In[ ]:', '# Extract the EXT_SOURCE variables and show correlations', '# In[ ]:', '# In[ ]:', '# # Feature Engineering', '# ## Polynomial feature', '# In[ ]:', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# Need to impute missing values', '# Create the polynomial object with specified degree', '# In[ ]:', '# Train the polynomial features', '# Transform the features', '# In[ ]:', '# In[ ]:', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# In[ ]:', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# ## Domain Knoledge Features', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[ ]:', '# Make the model with the specified regularization parameter', '# Train on the training data', '# In[ ]:', '# Make predictions', '# In[ ]:', '# # Submission dataframe', ""# submit = test[['SK_ID_CURR']]"", ""# submit['TARGET'] = y_pred"", '# submit.head(),submit.shape', '# In[ ]:', '# # Save the submission to a csv file', ""# submit.to_csv('log_reg_baseline.csv', index = False)"", '# ## Random Forest', '# In[ ]:', '# In[ ]:', '# Train on the training data', '# Extract feature importances', '# Make predictions on the test data', '# In[ ]:', '# # Make a submission dataframe', ""# submit = test[['SK_ID_CURR']]"", ""# submit['TARGET'] = predictions"", '# # Save the submission dataframe', ""# submit.to_csv('random_forest_baseline.csv', index = False)"", '# ### Adding Engineered Features', '# In[ ]:', '# Impute the polynomial features', '# Scale the polynomial features', '# In[ ]:', '# Train on the training data', '# Make predictions on the test data', '# In[ ]:', '# Make a submission dataframe', '# Save the submission dataframe']",127
homecreditdefaultrisk-mlnd-nachorovi.py,"['# coding: utf-8', '# # **Sections:**', '# [1. Import libraries & support functions](#import)  ', '# [2. Dataset preparation](#data_import)  ', '# [3. Exploratory Data Analysis (EDA)](#eda)  ', '# \xa0 [3.1 Datasets samples](#eda_ds_samples)  ', '# \xa0 [3.2 Datasets numerical statistics](#eda_ds_desc)  ', '# \xa0 [3.3 Datasets comparisons](#eda_ds_comparison)  ', '# \xa0 [3.4 Target Label](#eda_app_train_target)  ', '# \xa0 [3.5 Amounts comparison](#eda_amts)  ', '# \xa0 [3.6 Distribution of DAYS_BIRTH](#eda_days_birth)  ', '# \xa0 [3.7 Distribution of AMT_CREDIT](#eda_amt_credit)  ', '# \xa0 [3.8 Distribution of DAYS_ID_PUBLISH](#eda_days_id_publish)  ', '# \xa0 [3.9 Distribution of DAYS_REGISTRATION](#eda_days_registration)  ', '# \xa0 [3.10 Distribution of DAYS_EMPLOYED](#eda_days_employed)  ', '# [4. Data Preprocessing](#4)  ', '# [5. Split Data into Training and Validation](#5)  ', '# [6. Hyperparameter Tuning](#6)  ', '# [7. Model Fitting & Prediction](#7)  ', '# Acknowledgements:', '# - Dataset flattening, feature engineering, LGBM parameters: https://www.kaggle.com/shep312/lightgbm-harder-better-slower', '# - Dataset flattening, LGBM model starting point: https://www.kaggle.com/shivamb/homecreditrisk-extensive-eda-baseline-0-772', '# - General ideas: https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code', '# - Reducing memory footprint: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage', '# # 1 Import Libraries and create support functions', '# In[ ]:', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSVfile I/O (e.g. pd.read_csv)', '# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html', '# http://lightgbm.readthedocs.io/en/latest/Python-Intro.html', '# https://github.com/Microsoft/LightGBM', ""# Add evaluation metric to measure the model's performance"", '# Regression metrics available:', '# http://scikit-learn.org/stable/modules/classes.html#regression-metrics', '# http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics', '# http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc', '# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html', '# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html', '# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric', 'from IPython.display import display # Allows the use of display() for DataFrames', '# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html', '# In[ ]:', '# Support functions', '# In[ ]:', '# This implementation was copied from: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage', '# # 2 Dataset Import', '# In[ ]:', '# List available data files', '# Load all the datasets and reduce the memory usage', '# # 3 Exploratory Data Analysis (EDA)', '# ## 3.1 Datasets samples', '# In[ ]:', '# Show first 5 rows of each dataset', '# ## 3.2 Datasets numerical statistics', '# In[ ]:', '# Show dataset descriptive statistics', '# ## 3.3 Datasets comparisons', '# In[ ]:', ""            np.sum(posc_bal.isnull().sum() > 0), posc_bal.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR"", ""            np.sum(bureau_bal.isnull().sum() > 0), bureau_bal.isnull().sum().sum()], # Features don't include SK_ID_BUREAU"", ""            np.sum(app_train.isnull().sum() > 0), app_train.isnull().sum().sum()], # Features don't include SK_ID_CURR or TARGET"", ""            np.sum(prev_app.isnull().sum() > 0), prev_app.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR"", ""            np.sum(inst_pay.isnull().sum() > 0), inst_pay.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR"", ""            np.sum(cc_bal.isnull().sum() > 0), cc_bal.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR"", ""            np.sum(app_test.isnull().sum() > 0), app_test.isnull().sum().sum()], # Features don't include SK_ID_CURR"", ""            np.sum(bureau.isnull().sum() > 0), bureau.isnull().sum().sum()], # Features don't include SK_ID_CURR and SK_ID_BUREAU"", '# ## 3.4 Target Label', '# In[ ]:', '# ## 3.5 Amounts comparison', '# In[ ]:', '# Implementation source: https://www.kaggleusercontent.com/kf/4442153/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Df_QZcauc2BVOOmdHRjw1Q.ruvOVG8p44cAqUgN2tZHTPK-y8DwzYtkIoGA39JWR938aOHRdCqQRYjQj1U8AAiXqRfoRScRMjXH_DrMDqBWO9JIBKjTxS7yQyC3ouVc-MuExzzH0lGZdfJT2HJGkjvqSVLm4gYg7ML3r_jmJ3dP--6dmgHGsW1TQ6D04GnZzk6xwZseKGjCzeIYavlz44Qj.WDYyfq5ILj9HsKasnQ37uA/__results__.html#Comparing-summary-statistics-between-defaulters-and-non---defaulters-for-loan-amounts-.', '# ## 3.6 Distribution of DAYS_BIRTH', '# In[ ]:', '# ## 3.7 Distribution of AMT_CREDIT', '# In[ ]:', '# ## 3.8 Distribution of DAYS_ID_PUBLISH', '# In[ ]:', '# ## 3.9 Distribution of DAYS_REGISTRATION', '# In[ ]:', '# ## 3.10 Distribution of DAYS_EMPLOYED', '# In[ ]:', '# # 4 Data Preprocessing', '# In[ ]:', '# Merge training and testing datasets - This will help in two ways:', '# When handling categorical variables it will ensure both datasets end up with the same features', '# When handling missing values, if we use the mean to fill in missing values, they will be more representative', '# data = pd.concat([app_train, app_test], axis=0, sort=False)', ""# ERROR: TypeError: concat() got an unexpected keyword argument 'sort'"", ""# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' for app_train"", ""# And substract 3 for app_test, as it doesn't have a 'TARGET' column"", '# In[ ]:', '# Handle Categorical variables - Turn categorical variables into numerical features using the one-hot encoding scheme', '# Support function for one-hot encoding', '    # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html', '# Handle categorical variables', ""# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test'"", '# In[ ]:', '# Keep a copy of the application_train & application_test datasets without merging with the rest of the datasets', '# In[ ]:', '# Merge Point of Sale Cash Balance dataset', ""# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature"", ""# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value"", '# Average values for all other features in previous applications', '# In[ ]:', '# Merge Bureau Balance dataset', ""# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature"", ""# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value"", '# Average values for all other features in previous applications', '# In[ ]:', '# Merge Previous Applications dataset', ""# Count the number of previous applications for a given 'SK_ID_CURR'"", ""# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value"", '# Average values for all other features in previous applications', '# In[ ]:', '# Merge Installments Payments dataset', ""# Count the number of installments payments for a given 'SK_ID_CURR', and create a new feature"", ""# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value"", '# In[ ]:', '# Merge Credit Card Balance dataset', ""# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature"", ""# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value"", '# In[ ]:', '# Merge Bureau dataset', ""# Count the number of credits registered in the bureau for a given 'SK_ID_CURR', and create a new feature"", ""# Remove the 'SK_ID_BUREAU' column from the dataset as it doesn't add value"", '# In[ ]:', '# Transforming skewed continuous features', '# I need to handle negative numbers, if x = -1 then it will throw an error; log(0) = Inf', '# In[ ]:', '# In[ ]:', '# Normalizing numerical features', '# Full list of top ten features, discounting EXT_SOURCE_X becuase they are already normalizaed:', ""# ['DAYS_BIRTH', 'AMT_ANNUITY', 'AMT_CREDIT', 'DAYS_ID_PUBLISH', 'pcb_CNT_INSTALMENT_FUTURE', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED']"", ""# numerical = ['DAYS_BIRTH', 'AMT_ANNUITY', 'AMT_CREDIT', 'DAYS_ID_PUBLISH']"", ""# 12 entries in 'AMT_ANNUITY' are NaN - I need to fix that first before Normalizing"", ""# 'pcb_CNT_INSTALMENT_FUTURE' belongs to a different dataset"", ""# numerical = ['DAYS_BIRTH', 'AMT_CREDIT', 'DAYS_ID_PUBLISH']"", '# In[ ]:', ""data_to_use = 'ALL' # 'ALL' or 'data_train_test'"", '# In[ ]:', '# Handle missing data', '# https://pandas.pydata.org/pandas-docs/stable/missing_data.html#filling-with-a-pandasobject', '# https://www.kaggle.com/dansbecker/handling-missing-values', '# http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html', '# In[ ]:', '# Clean variables that are no longer needed', '# Not used yet: bureau_bal_count, bureau_bal_avg', '# In[ ]:', '# Separate the data into the original test and training datasets', ""# Remove columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' as they are not features"", ""# Separate the 'target label' from the training dataset"", '# To be used when preparing the submission', '# # 5 Split Data into Training and Validation', '# In[ ]:', ""# Split 'features' and 'target label' data into training and validation data using train_test_split"", '# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html', '# In[ ]:', '# Run GridSearchCV or fully train an estimator', ""# 'grid_search_RFR', 'grid_search_LGBM', 'train_estimators', 'train_estimator_LGBM', 'train_estimator_RFR', 'LGBM_KFold'"", '# # 6 Hyperparameter Tuning', '# In[ ]:', '# Run GridSearchCV on LGBM', '          num_iteration=5000, # num_boost_round=5000,', '          # early_stopping_rounds=100', '          # ValueError: For early stopping, at least one dataset and eval metric is required for evaluation', ""          'boosting_type': ['gbdt'], # 'dart'"", ""          'min_data_in_leaf': [20], # [15, 20, 25],"", ""          'max_depth': [7], # [6, 7, 8],"", '    # Create a scorer to measure hyperparameters performance', '    # Create GridSearchCV grid object', '    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters', '    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes', '    # Get the best estimator', '    # Get the best score', '    # Get the best parameters', '    # Make predictions with unoptimized estimator on the validation set', '    # Predict with the best estimator on the validation set', '# In[ ]:', '# Run GridSearchCV', '    # Initialize the Estimator (Learner or Regression Model)', '    # Determine which Parameters to tune', ""        'max_features': [0.2], # [0.18, 0.2, 0.23]"", ""        'min_samples_split': [2], # [2, 3]"", '    # Create a scorer to measure hyperparameters performance', '    # Create GridSearchCV grid object', '    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters', '    grid_fit_time = (end - start) / 60 # Ellapsed time in minutes', '    # Get the best estimator', '    # Get the best score', '    # Get the best parameters', '    # Make predictions with unoptimized estimator on the validation set', '    # Predict with the best estimator on the validation set', '    # Predict with the best estimator on the testing set', '# # 7 Model Fitting & Prediction', '# In[ ]:', '# Train estimator LGBM', ""    data_split = 'kfold' # Possible values: 'kfold' or 'train_test_split'"", '        # Using split merged datasets with train_test_split', '        # Using KFolds to split the merged dataset for cross-validation', '# In[ ]:', '# In[ ]:', '# Parameters from Aguiar https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features/code', '# Train estimator LGBM with KFold Cross-validation', '        # LightGBM parameters found by Bayesian optimization', '# In[ ]:', '# Train estimator LGBM with KFold Cross-validation', '    # Display feature importance', '# In[ ]:', '# Train estimator LGBM with KFold Cross-validation', '    # Prepare submission file', '# In[ ]:', '# Train estimator RandonForrestRegressor', '    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters', '    # Alternative: n_estimators=135, max_features=0.2, min_samples_split=2, min_samples_leaf=62', ""    # Alternative2: criterion='mae', # default='mse', VERY SLOW"", '    estimator = RandomForestRegressor(n_estimators=125, # default=10', ""                                      max_features=0.2, # default='auto'"", '                                      min_samples_split=2, # default=2', '                                      min_samples_leaf=75, # default=1', '                                      n_jobs=-1, # default=1', '                                      random_state=42, # default=None', '                                      verbose=0) # default=0', '    # Fit the estimator with the training dataset', '    # Predict with the validation dataset', '    # Determine the feature importance', '    # TODO: GRAPH THE FEATURE IMPORTANCE', '# In[ ]:', ""    # Predict using the 'test' dataset for submission"", '    # Prepare prediction for submission', '    # Replace any negative number with zero, required for https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code', '    # pred_test[pred_test < 0] = 0', '# In[ ]:']",232
homecreditdefaultrisk_simpleblend_0_798.py,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
home_credit_default_risk_python.py,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '    # Print some summary information', '    # Return the dataframe with missing information', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# one-hot encoding of categorical variables', '# Align the training and testing data, keep only columns present in both dataframes', '# Add target back in to the data', '# Find correlations with the target and sort', '# Display correlations', '# Find the correlation of the positive days since birth and target', '# matplotlib and seaborn for plotting', '# Set the style of plots', '# Plot the distribution of ages in years', '# KDE plot of loans that were repaid on time', '# KDE plot of loans which were not repaid on time', '# Labeling of plot', '# Age information into a separate dataframe', '# Bin the age data', '# Group by the bin and calculate averages', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# Extract the EXT_SOURCE variables and show correlations', '# Heatmap of correlations', '# iterate through the sources', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# Copy the data for plotting', '# Add in the age of the client in years', '# Drop na values and limit to first 100000 rows', '# Function to calculate correlation coefficient between two columns', '# Create the pairgrid object', '# Upper is a scatter plot', '# Diagonal is a histogram', '# Bottom is density plot', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# Need to impute missing values', '# Create the polynomial object with specified degree', '# Train the polynomial features', '# Transform the features', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# Drop the target from the training data', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# Make the model with the specified regularization parameter', '# Train on the training data', '# Make predictions', '# Make sure to select the second column only', '# Submission dataframe', '# Save the submission to a csv file']",74
home_credit_default_with_lgbm (1).py,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Below ohe conversion is looking for 2d array. Need to convert pandas series to 2d array', '# Seggregate the train and test data']",9
home_credit_default_with_lgbm.py,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Below ohe conversion is looking for 2d array. Need to convert pandas series to 2d array', '# Seggregate the train and test data']",9
home_credit_submission.py,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '           # mx.fit_transform(numeric),', '       # )], axis=1)', '# Any results you write to the current directory are saved as output.   ']",10
home_rf_et_xgb_cb_stack_oof1_lb_0_789.py,"['SEED = 7    # CR7 always..', '    # # Add new features', '    # Amount loaned relative to salary', '    # Number of overall payments (I think!)', '    # Social features', '    # A lot of the continuous days variables have integers as missing value indicators.', '    # # Aggregate and merge supplementary datasets', '    # Previous applications', '    # Average the rest of the previous app data', '    # Previous app categorical features', '    # Credit card data - numerical features', '    # Credit card data - categorical features', '    # Credit bureau data - numerical features', '    # Bureau balance data', '    # Pos cash data - weight values by recency when averaging', '    # Unweighted aggregations of numeric features', '    # Pos cash data data - categorical features', '    # Installments data', '    # Add more value counts', '    # Label encode categoricals', '# Merge the datasets into a single one for training', '# Separate metadata', '# Process the data set.', '# Capture other categorical features not as object data types:', '# Re-separate into train and test', '# from https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867/code', '        oof_train[test_index] = clf.predict(x_te)[:,1]  # or [:,0]', '        oof_test_skf[i, :] = clf.predict(x_test)[:,1]  # or [:,0]', '        oof_train[test_index] = clf.predict(x_te)  # or [:,0]', '        oof_test_skf[i, :] = clf.predict(x_test)  # or [:,0]']",30
hybrid-jeepy-and-lgb-ii.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '    # Compute target mean ', '    # Compute smoothing', '    # Apply average function to all target data', '    # The bigger the count the less full_avg is taken into account', '    # Apply averages to trn and tst series', '    # pd.merge does not keep the index so restore it', '    # pd.merge does not keep the index so restore it', '# In[3]:', '# In[4]:', ""# buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:']",29
hybrid-jeepy-and-lgb.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '    # Compute target mean ', '    # Compute smoothing', '    # Apply average function to all target data', '    # The bigger the count the less full_avg is taken into account', '    # Apply averages to trn and tst series', '    # pd.merge does not keep the index so restore it', '    # pd.merge does not keep the index so restore it', '# In[3]:', '# In[4]:', ""# buro_full.columns = ['buro_' + f_ for f_ in buro_full.columns]"", '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:']",29
hyperparameter_tuning_with_caret.R,"['# Load application data (train, test)']",1
inefficient-feature-engineering.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '    # Import bureau and bureau_balance', '    # Search for categorical variables', '    # Search for numerical variables', '    # One hot encoding of balance categorical variables', '    # Aggregate all variables', '    # Groupby SK_ID_BUREAU and aggregate', ""    # Rename columns into 'column_name' + aggregation"", '    # Complete balance preprocessing', '    # Search for categorical variables', '    # Search for numerical variables', '    # One hot encoding of bureau categorical variables', '    # Merge balance and bureau dataframes', '    # Drop SK_ID_BUREAU as it is redundant now', '    # Aggregate variables', '    # Groupby SK_ID_CURR and aggregate accordingly', '    # Rename column names', '    # bureau and balance preprocessing complete', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# # Remove Collinear Variables #', '# In[ ]:', '# Remove Collinear Variables', '# Threshold for removing correlated variables', '# Absolute value correlation matrix', '# In[ ]:', '# Upper triangle of correlations', '# In[ ]:', '# Select columns with correlations above threshold', '# In[ ]:', '# # Remove Missing Values #', '# In[ ]:', '# Train missing values (in percent)', '# In[ ]:', '# Test missing values (in percent)', '# In[ ]:', '# Identify missing values above threshold', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",50
intro-home-credit-default-risk.py,"['# coding: utf-8', '# This is my introduction on the Home Credit Default Risk problem.  I first spend time reviewing the variables and examining the data in the training data.  Next, I move on to incorporating data from the other data files we have been given.  Then I begin paring down the data to the data I will use for modeling.', '# # Introduction', '# In[ ]:', '# imports', '# Suppress warnings ', '# In[ ]:', '# list of data files available', '# In[ ]:', '# training data', '# In[ ]:', '# testing data', '# In[ ]:', '# review target variable', '# In[ ]:', '# summary of training data', '# In[ ]:', '# join training and testing data sets so we keep the same number of features in both', ""# shape of combined dataset should be sum of rows in training and testing (307511 + 48744 = 356255) and 122 columns (testing data doesn't have target)"", '# In[ ]:', '# missing values', '# This is a lot of missing values to deal with.  A number of variables are missing over two-thirds of the data.', '# In[ ]:', '# types of data', '# In[ ]:', '# categorical data - how many different categories for each variable', '# In[ ]:', '# In[ ]:', '# use label encoding for categorical variables with only two categories', '# Note that four columns had two categories, but only three were label encoded - EMERGENCYSTATE_MODE was not.', '# This is because the three label encoded did not have any missing values, while EMERGENCYSTATE_MODE was missing values, so it really has three categories - Yes, No, and missing.', '# In[ ]:', '# use one-hot encoding for remaining categorical variables', '# Note that using one-hot encoding, we went from 122 variables to 243 variables - a significant increase.  At a later point, we will probably want to remove those that are not relevant.', '# # Explore the Data', ""# For the non-indicator variables, let's look at individual variables for outliers or other interesting information."", '# ## CNT_CHILDREN', '# In[ ]:', ""# Most applicants have no children or only one child.  However, the maximum is 20, which seems high.  Let's look closer at this data."", '# In[ ]:', '# In[ ]:', '# plot CNT_CHILDREN against the TARGET to better understand the data', ""# It appears that if CNT_CHILDREN is greater than six, the probability of default is higher.  From above, we know that on average the default rate is about 8.07%.  Let's look further at CNT_CHILDREN."", '# In[ ]:', '# In[ ]:', '# create a flag for outliers in the CNT_CHILDREN column, and then replace these values with nan', '# In[ ]:', '# review CNT_CHILDREN after our modifications', '# In[ ]:', '# ## AMT_INCOME_TOTAL', '# In[ ]:', '# In[ ]:', '# This looks to be close to what we would expect.  There are relatively fewer high incomes, and the mean is greater than the median.', '# ## AMT_CREDIT', '# In[ ]:', '# In[ ]:', '# This distribution is definitely skewed, but looks like we would expect for a distribution of credit.', '# ## AMT_ANNUITY', '# In[ ]:', '# In[ ]:', '# There do not appear to be any outliers here, though the distribution is definitely skewed.', '# ## AMT_GOODS_PRICE', '# In[ ]:', '# In[ ]:', '# Similar to the above, there do not appear to be any outliers here, though the distribution is also definitely skewed.', '# ## REGION_POPULATION_RELATIVE', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# plot REGION_POPULATION_RELATIVE against the TARGET to better understand the data', '# The larger values do not appear to be correlated with higher-than-average or lower-than-average default rates.  For now, I will leave these as is.', ""# ### Let's review the correlation matrix for the variables examined so far:"", '# In[ ]:', '# None of the above variables are significantly correlated with TARGET.  However, AMT_CREDIT, AMT_ANNUITY, and AMT_GOODS_PRICE are all highly correlated with each other, especially AMT_CREDIT and AMT_GOODS_PRICE.  It appears that we may need to drop at least one of these two variables.', '# ## DAYS_BIRTH', '# In[ ]:', '# In[ ]:', '# this variable appears to be equal to (date of birth) minus (date of application), which is producing negative numbers', '# if we look again at the data transformed into positive numbers and into years (by dividing by -365.25) we get the following', '# This distribution of ages at application date seems reasonable - no small children and reasonable max age.', '# In[ ]:', '# ## DAYS_EMPLOYED', '# In[ ]:', '# In[ ]:', '# this variable appears to be equal to (date of employment) minus (date of application), which is producing negative numbers', '# if we look again at the data transformed into positive numbers and into years (by dividing by -365.25) we get the following', '# In[ ]:', ""# it appears that a dummy value was used, possibly for people who didn't have a date of employment to enter into the application"", '# this group had 365243 in the data, which is approximately -1000 years', '# we should also look at the other side of the distribution - 49 years of employment is a long time', '# In[ ]:', '# In[ ]:', '# create a flag for outliers in the DAYS_EMPLOYED column, and then replace these values with nan', '# In[ ]:', '# review DAYS_EMPLOYED after our modifications', '# In[ ]:', '# While the distribution is skewed, it is now what we expect - long-tenure employees are rare and short-tenure employees are much more common.', '# ## DAYS_REGISTRATION', '# In[ ]:', '# In[ ]:', '# this variable appears to be equal to (date of registration) minus (date of application), which is producing negative numbers', '# if we look again at the data transformed into positive numbers and into years (by dividing by 365.25) we get the following', '# In[ ]:', '# The above looks as expected.', '# ## DAYS_ID_PUBLISH', '# In[ ]:', '# In[ ]:', '# convert to positive years again', '# In[ ]:', ""# I don't see any outliers, although the shape of this distribution is different from the others we have seen above."", ""# ### Let's review the correlation matrix for the additional variables examined so far:"", '# In[ ]:', '# DAYS_BIRTH appears to be the most correlated with TARGET so far.', '# ## OWN_CAR_AGE', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# it seems that there are some outliers on the car age, as the max is 91', ""# let's get a better look at the values in the tail and whether these have a higher probability of default than average"", '# In[ ]:', '# In[ ]:', '# create a flag for outliers in the OWN_CAR_AGE column, and then replace these values with nan', '# In[ ]:', '# review OWN_CAR_AGE after our modifications', '# In[ ]:', '# now this data should look more like we expect', '# ## CNT_FAM_MEMBERS', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# it seems that there are some outliers on the count of family members, as the max is 21', ""# let's look at the 99th percentile"", '# In[ ]:', ""# let's get a better look at the values in the tail and whether these have a higher probability of default than average"", ""# From the above, it appears that the probability of default for our outliers is 13.23%, which far exceeds that of the entire training data of 8.07%.  Let's remove these as outliers and keep track of which records are outliers."", '# In[ ]:', '# create a flag for outliers in the CNT_FAM_MEMBERS column, and then replace these values with nan', '# In[ ]:', '# review CNT_FAM_MEMBERS after our modifications', '# In[ ]:', '# ## REGION_RATING_CLIENT', '# In[ ]:', '# In[ ]:', '# ## REGION_RATING_CLIENT_W_CITY', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# how many are equal to -1 in the dataset?', '# In[ ]:', '# this appears to be a data entry error', ""# let's set the value of -1 equal to 1 instead"", '# In[ ]:', '# ## HOUR_APPR_PROCESS_START', '# In[ ]:', '# In[ ]:', ""# ### Let's review the correlation matrix for the additional variables examined so far:"", '# In[ ]:', '# REGION_RATING_CLIENT and REGION_RATING_CLIENT_W_CITY are the most correlated with our TARGET, though these two variables are highly correlated with each other.  We will probably want to include only one of these two.', '# ## EXT_SOURCE_1', '# In[ ]:', '# In[ ]:', '# this data looks pretty good from the above, check the histogram', '# ## EXT_SOURCE_2', '# In[ ]:', '# In[ ]:', '# this data also looks pretty good from the above, check the histogram', '# ## EXT_SOURCE_3', '# In[ ]:', '# In[ ]:', ""# this doesn't appear to have issues either, check the histogram"", ""# These three external sources variables appear similar but have different distributions.  Let's look at their correlations with each other and with our TARGET."", '# In[ ]:', '# These three are all strongly correlated with our TARGET, and they are not too highly correlated with each other.  These look like important variables for later.', '# ## OBS_30_CNT_SOCIAL_CIRCLE', '# In[ ]:', '# In[ ]:', '# there appear to be some outliers here we may need to deal with (max = 354??)', '# In[ ]:', ""# let's look more at these outliers and whether these have a higher probability of default than average"", '# In[ ]:', '# create a flag for outliers in the OBS_30_CNT_SOCIAL_CIRCLE column, and then replace these values with nan', '# In[ ]:', '# review OBS_30_CNT_SOCIAL_CIRCLE after our modifications', '# In[ ]:', '# ## DEF_30_CNT_SOCIAL_CIRCLE', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## OBS_60_CNT_SOCIAL_CIRCLE', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## DEF_60_CNT_SOCIAL_CIRCLE', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# Let's look at the correlations between these social variables and our TARGET."", '# In[ ]:', '# The DEF counts are highly correlated with each other, but none of these are very correlated with our TARGET.', '# ## DAYS_LAST_PHONE_CHANGE', '# In[ ]:', '# In[ ]:', ""# let's transform this into positive years, as we did with the other DAYS_ variables above"", '# In[ ]:', '# I am going to treat all of these credit bureau variables similarly.  I am going to remove the outliers but create a flag for the outliers.  As the period of evaluation increases (from hour to day to week, etc.), the cutoff for the outliers will also increase.', '# ## AMT_REQ_CREDIT_BUREAU_HOUR', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## AMT_REQ_CREDIT_BUREAU_DAY', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## AMT_REQ_CREDIT_BUREAU_WEEK', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## AMT_REQ_CREDIT_BUREAU_MON', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## AMT_REQ_CREDIT_BUREAU_QRT', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## AMT_REQ_CREDIT_BUREAU_YEAR', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# Let's look at the correlations of this last group of variables."", '# In[ ]:', ""# DAYS_LAST_PHONE_CHANGE is more correlated with our TARGET, while the credit bureau variables don't seem to be correlated with TARGET much at all."", '# # Additional Features to Add from the Training Data', '# Now that we have reviewed the initial data provided, there are a few more variables we can create from the existing data.  We can later determine if any of these interactions can add to our model.', '# ', '# New variables to try:', '# * EMPLOY_AGE = DAYS_EMPLOYED / DAYS_BIRTH: how long was the applicant employed relative to how old the applicant was - employed for a larger portion may indicate reliability', '# * INCOME_AGE = AMT_INCOME_TOTAL / DAYS_BIRTH: how large is the income relative to how old the applicant was - may indicate potential for income to rise and may repayment easier in the future', '# * CREDIT_AGE = AMT_CREDIT / DAYS_BIRTH: how much credit relative to how old the applicant was - may indicate sources of other financial stress', '# * CREDIT_INCOME = AMT_CREDIT / AMT_INCOME_TOTAL: how much credit relative to total income - too much credit may be too risky', '# * ANNUITY_INCOME = AMT_ANNUITY / AMT_INCOME_TOTAL: how large are the loan payments relative to total income - too large of payments may not be sustainable', '# * ANNUITY_CREDIT = AMT_ANNUITY / AMT_CREDIT: how large are the loan payments relative to the credit amount (how long will it take to pay it back, without accounting for different interest rates)', '# ', '# These may or may not be good predictors, but these were the ones I thought could be useful in the model.', '# In[ ]:', '# create new variables', '# In[ ]:', ""# let's look at the correlations of the new variables we created along with TARGET"", '# In[ ]:', '# EMPLOY_AGE seems to be most correlated with TARGET of our new variables', '# we can plot EMPLOY_AGE relative to TARGET using KDE', '# # Adding Features from the Other Data Files', '# In this section, I will begin adding to my data, incorporating the information from the other data files we were provided.', '# In[ ]:', '# the first file we will investigate is bureau', '# In[ ]:', '# In[ ]:', '# I will need to determine how to incorporate each of these items into the data.  Generally, for time measurements (numbers of days since something), I will want to use the max or min.  For other items, like amounts, I will want to use the mean instead.  I may also want to use count or sum, depending on the item.', '# ', '# To get this data into the main dataset file, I will need to group the data in the new file by SK_ID_CURR.  I will then apply the max or mean (or other function) to the data and merge this into the dataset file.', '# ## COUNT', '# In[ ]:', '# the first item to look at is how many records are in this for each applicant', '# In[ ]:', '# ## DAYS_CREDIT', '# In[ ]:', '# review the data, and divide by -365.25 to turn this into positive years', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# merge with the dataset', '# In[ ]:', '# what is the correlation of our new variable with TARGET', '# In[ ]:', '# evaluate the new variable with a KDE plot', '# ## CREDIT_DAY_OVERDUE', '# In[ ]:', '# In[ ]:', '# this looks like virtually all are zero, but there are some outliers', '# In[ ]:', ""# let's take the max of this variable"", '# In[ ]:', '# In[ ]:', '# most of the data in this column is zero', '# how many non-zero items exist?', '# In[ ]:', ""# let's turn this into a flag, since 99% of the data is zero"", '# In[ ]:', '# drop the max variable and merge in the flag', '# In[ ]:', '# ## DAYS_CREDIT_ENDDATE', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# let's take the max of this variable"", '# In[ ]:', '# it appears that we have a few outliers around -41875', '# In[ ]:', '# merge both our max variable and outlier flag into the dataset', '# In[ ]:', '# ## DAYS_ENDDATE_FACT', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# let's take the average of this variable"", '# In[ ]:', '# In[ ]:', '# it appears that we have a few outliers around -8000 days that we can handle', '# In[ ]:', '# merge both our mean variable and outlier flag into the dataset', '# In[ ]:', '# In[ ]:', '# evaluate the new variable with a KDE plot', '# ## AMT_CREDIT_MAX_OVERDUE', '# In[ ]:', '# In[ ]:', ""# let's take the max of this variable"", '# In[ ]:', ""# I'm also curious on the average of this variable"", '# In[ ]:', ""# I'm not sure which of these two variables may work better in this case, so let's bring them both into the dataset for now"", '# In[ ]:', '# These two are very similarly correlated with our TARGET.  For now, we will keep both of these, but we may remove one later on.', '# ## CNT_CREDIT_PROLONG', '# In[ ]:', '# In[ ]:', ""# since these are counts, let's sum this variable"", '# In[ ]:', '# merge into our dataset', '# In[ ]:', '# ## AMT_CREDIT_SUM', '# In[ ]:', '# In[ ]:', ""# since these are amounts, let's average this variable"", '# In[ ]:', '# merge into our dataset', '# In[ ]:', '# ## AMT_CREDIT_SUM_DEBT', '# In[ ]:', '# In[ ]:', ""# since these are amounts, let's average this variable"", '# In[ ]:', '# merge into our dataset and look at the correlation', '# ## AMT_CREDIT_SUM_LIMIT', '# In[ ]:', '# In[ ]:', ""# since these are amounts, let's average this variable"", '# In[ ]:', '# merge into our dataset and look at the correlation', '# ## AMT_CREDIT_SUM_OVERDUE', '# In[ ]:', '# In[ ]:', ""# since these are amounts, let's average this variable"", '# In[ ]:', '# merge into our dataset and look at the correlation', '# ## DAYS_CREDIT_UPDATE', '# In[ ]:', '# divide by -365.25 to turn this into positive years', '# In[ ]:', ""# since this is a days variable, let's use max"", '# In[ ]:', '# merge into our dataset and look at the correlation', '# ## AMT_ANNUITY', '# In[ ]:', '# In[ ]:', ""# since these are amounts, let's average this variable"", '# In[ ]:', '# merge into our dataset and look at the correlation', '# ## Categoricals in Bureau data', '# ', ""# Now let's look at the categorical variables that are left in this data.  There are three of them that we need to deal with."", '# In[ ]:', '# In[ ]:', ""# let's use one-hot encoding on these variables"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# the next file we will investigate is bureau_balance', '# ## MONTHS_BALANCE', '# In[ ]:', '# this appears to be the number of months of balance relative to the application date', ""# let's start with the count"", '# In[ ]:', ""# let's also look at the mean"", '# In[ ]:', '# In[ ]:', ""# now let's get our categoricals"", '# In[ ]:', '# In[ ]:', ""# now let's merge the MONTHS_BAL with our categoricals by SK_ID_BUREAU, then merge with bureau to add in SK_ID_CURR"", '# Now we will take the mean when grouping by SK_ID_CURR for each of the above variables and then add them to our dataset.  There are more possibilities here (min, max, count, sum, etc.) but we will just do mean for now.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# let's free up some memory by deleting some of the dataframes we are done with"", '# In[ ]:', '# the next file we will investigate is credit_card_balance', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# now let's deal with our one categorical variable, NAME_CONTRACT_STATUS"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Now we have all of this data by previous ID, but we need to aggregate this on SK_ID_CURR.  We will repeat what we did above for the bureau balance data, averaging these variables for each applicant.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# let's free up some memory by deleting some of the dataframes we are done with"", '# In[ ]:', '# the next file we will investigate is installments_payments', ""# For this file, I think we want to look at creating a couple of additional variables.  For the dates of payments, I would like to see the difference between the due date and the actual payment date.  For the amounts of payments, I would like to see the difference between the amount owed and the actual amount paid.  So let's create a few new variables."", '# In[ ]:', '# create the additional variables', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# capture the mean, max, and min for DAYS_DIFF', '# In[ ]:', '# capture the mean, max, and min for AMT_DIFF', '# In[ ]:', '# Now we have all of this data by previous ID, but we need to aggregate this on SK_ID_CURR.  We will repeat what we did above for the credit data.  For most cases, I will use the average to combine.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# let's free up some memory by deleting some of the dataframes we are done with"", '# In[ ]:', '# the next file we will investigate is POS_CASH_balance', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# now let's deal with our one categorical variable, NAME_CONTRACT_STATUS, in this cash file"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# let's free up some memory by deleting some of the dataframes we are done with"", '# In[ ]:', '# # Feature Selection', '# In this next section, I will work on reducing the number of variables.  I will do this by getting rid of collinear variables, variables with too many missing values, and feature importance.  After completing these steps, we will have a dataset better suited for modeling.', '# In[ ]:', ""# let's review our dataset data types"", '# In[ ]:', '# it looks like we have a couple of objects still in our data?', '# In[ ]:', '# our outlier variables appear to still be in the format of True or False, so we need to fix this before continuing.', '# In[ ]:', '# check again', '# In[ ]:', '# ## Start with collinear variables', '# In[ ]:', ""# because the dataset file is so large, let's use a subsample of the data to evaluate the collinear variables"", '# In[ ]:', ""# let's first make the correlation matrix"", '# In[ ]:', '# In[ ]:', ""# let's drop any columns with correlations above 0.9"", '# In[ ]:', '# now we can drop these columns from the full dataset file', '# In[ ]:', ""# delete the dataframes we don't need anymore"", '# ## Next look at missing values', '# In[ ]:', '# missing values (in percent)', '# In[ ]:', ""# let's remove columns with more than 75% missing data"", '# In[ ]:', ""# let's drop these columns"", '# Next we will look at feature importance.  But before we do so, we will need to split our data back into test and train.', '# In[ ]:', '# separate training and testing data for modeling', '# In[ ]:', '# separate training data', '# In[ ]:', '# ## Feature Importance', '# To evaluate feature importance, I will use the LightGBM model.  I will run the model twice to capture the feature importances, and then average the results.', '# In[ ]:', '# create a dataframe of all zeroes to hold feature importance calculations', '# In[ ]:', '# create the model to use', '# for the parameters, objective is binary (as this is either default or no default that we are predicting),', '# boosting type is gradient-based one-side sampling (larger gradients contribute more to information gain so this keeps those ', '# with larger gradients and only randomly drops those with smaller), class weight is balanced', '# (automatically adjust the weights to be inversely proportional to the frequencies)', '# In[ ]:', '# we will fit the model twice and record the feature importances each time', '# note that we will use auc (area under the curve) for evaluation, as on this is what our model will be judged', '# In[ ]:', '# review features with most importance', '# In[ ]:', '# review features with zero importance', '# In[ ]:', ""# let's drop the features with zero importance"", '# In[ ]:', '# # Modeling', ""# Let's begin the modeling section now.  We will use LightGBM with cross validation."", '# In[ ]:', '# dataframe to hold predictions', '# dataframe for out of fold validation predictions', '# lists for validation and training scores', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# iterate through each of the five folds', '    # create the model, similar to the one used above for feature importances', '    # train the model', '    # record the best iteration', '    # test predictions', '    # out of fold predictions', '    # record scores', '    # Clean up memory', '# In[ ]:', '# scores', '# In[ ]:', '# In[ ]:', '# make submission file']",617
intro-to-model-tuning-grid-and-random-search.py,"['# coding: utf-8', '# # Introduction: Hyperparameter Tuning using Grid and Random Search', '# ', '# In this notebook, we will explore two methods for hyperparameter tuning a machine learning model. [In contrast](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) to model __parameters__ which are learned during training, model __hyperparameters__ are set by the data scientist ahead of training and control implementation aspects of the model. The weights learned during training of a linear regression model are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. Hyperparameters can be thought of as model settings. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will __not be__ the best across all datasets. The process of [hyperparameter tuning (also called hyperparameter optimization)](https://en.wikipedia.org/wiki/Hyperparameter_optimization) means finding the combination of hyperparameter values for a machine learning model that performs the best - as measured on a validation dataset - for a problem. ', '# ', ""# (__Quick Note__: a lot of data scientists use the terms _parameters_ and _hyperparameters_ interchangeably to refer to the model settings. While this is technically incorrect, it's pretty common practice and it's usually possible to tell when they are referring to parameters learned during training versus hyperparameters. I'll try to stick to using model hyperparameters or model settings and I'll  point out when I'm talking about a parameter that is learned during training. If you're still confused, [this article](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) may help you out!)"", '# ', '# __Additional Notebooks__ ', '# ', ""# If you haven't checked out my other work on this problem, here is a complete list of the notebooks I have completed so far:"", '# ', '# * [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)', '# * [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)', '# * [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)', '# * [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)', '# * [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)', '# * [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)', '# * [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)', '# * [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)', '# ', '# There are several approaches to hyperparameter tuning', '# ', '# 1. __Manual__: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results. ', '# 2. __Grid Search__: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!', '# 3. __Random search__: set up a grid of hyperparameter values and select _random_ combinations to train the model and score. The number of search iterations is set based on time/resources. ', '# 4. __Automated Hyperparameter Tuning__: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.', '# ', '# (This [Wikipedia Article](https://en.wikipedia.org/wiki/Hyperparameter_optimization) provides a good high-level overview of tuning options with links for more details)', '# ', '# In this notebook, we will implement approaches 2 and 3 for a Gradient Boosting Machine Learning Model. In a future notebook, we will implement automated hyperparameter tuning using Bayesian optimization, specifically the Hyperopt library. If you want to get an idea of how automated hyperparameter tuning is done, check out [this article](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). ', '# ', '# ## Model: Gradient Boosting Machine ', '# ', '# The [Gradient Boosting Machine (GBM)](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) has recently emerged as one of the top machine learning models. The GBM is extremely effective on structured data - where the information is in rows and columns - and medium sized datasets - where there are at most a few million observations. We will focus on this model because it is currently the top performing method for most competitions on Kaggle and because the performance is highly dependent on the hyperparameter choices. The basics you need to know about the GBM are that it is an ensemble method that works by training many individual learners, almost always decision trees. However, unlike in a random forest where the trees are trained in __parallel__, in a GBM, the trees are trained __sequentially__ with each tree learning from the mistakes of the previous ones. The hundreds or thousands of weak learners are combined to make a single strong ensemble learner with the contributions of each individual learned during training using Gradient Descent (the weights of the individual trees would therefore be a model _parameter_). ', '# ', '# The GBM [has many hyperparameters to tune](http://lightgbm.readthedocs.io/en/latest/Parameters.html) that control both the overall ensemble (such as the learning rate) and the individual decision trees (such as the number of leaves in the tree or the maximum depth of the tree). It is difficult to know which combination of hyperparameters will work best based only on theory because there are complex interactions between hyperparameters. Hence the need for hyperparameter tuning: the only way to find the optimal hyperparameter values is to try many different combinations on a dataset!', '# ', '# We will use the implementation of the Gradient Boosting Machine in the [LightGBM library](http://lightgbm.readthedocs.io/en/latest/). This is a much faster (and some say more accurate) implementation than that available in Scikit-Learn.', '# ', '# For more details of the Gradient Boosting Machine (GBM), check out this [high-level blog post](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/), or this [in depth technical article.](https://brage.bibsys.no/xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf) ', '# ', '# ### Getting Started', '# ', ""# With the necessary background out of the way, let's get started. For this notebook, we will work with a subset of the data consisting of 10000 rows. Hyperparameter tuning is extremely computationally expensive and working with the full dataset in a Kaggle Kernel would not be feasible for more than a few search iterations. However, the same ideas that we will implement here can be applied to the full dataset and while this notebook is specifically aimed at the GBM, the methods can be applied for any machine learning model. "", '# ', '# To ""test"" the tuning results, we will save some of the training data, 6000 rows, as a separate testing set. When we do hyperparameter tuning, it\'s crucial to __not tune the hyperparameters on the testing data__. We can only use the testing data __a single time__ when we evaluate the final model that has been tuned on the validation data. To actually test our methods from this notebook, we would need to train the best model on all of the training data, make predictions on the actual testing data, and then submit our answers to the competition. ', '# In[ ]:', '# Data manipulation', '# Modeling', '# Splitting data', '# Below we read in the data and separate into a training set of 10000 observations and a ""testing set"" of 6000 observations. After creating the testing set, we cannot do any hyperparameter tuning with it! ', '# In[ ]:', '# Sample 16000 rows (10000 for training, 6000 for testing)', '# Only numeric features', '# Extract the labels', '# Split into training and testing data', '# We will also use only the numeric features to reduce the number of dimensions which will help speed up the hyperparameter search. Again, this is something we would not want to do on a real problem, but for demonstration purposes, it will allow us to see the concepts in practice (rather than waiting days/months for the search to finish).', '# In[ ]:', '# # Cross Validation', '# ', '# To evaluate each combination of hyperparameter values, we need to score them on a validation set. The hyperparameters __can not be tuned on the testing data__. We can only use the testing data __once__ when we evaluate the final model. The testing data is meant to serve as an estimate of the model performance when deployed on real data, and therefore we do not want to optimize our model to the testing data because that will not give us a fair estimate of the actual performance. The correct approach is therefore to use a validation set. However, instead of splitting the valuable training data into a separate training and validation set, we use [KFold cross validation](https://www.youtube.com/watch?v=TIgfjmp-4BA). In addition to preserving training data, this should give us a better estimate of generalization performance on the test set than using a single validation set (since then we are probably overfitting to that validation set). The performance of each set of hyperparameters is determined by Receiver Operating Characteristic Area Under the Curve (ROC AUC) from the cross-validation.', '# ', '# In this example, we will use 5-fold cross validation which means training and testing the model with each set of hyperparameter values 5 times to assess performance. Part of the reason why hyperparameter tuning is so time-consuming is because of the use of cross validation. If we have a [large enough training set, we can probably get away with just using a single separate validation set](https://www.coursera.org/lecture/deep-neural-network/train-dev-test-sets-cxG1s), but cross validation is a safer method to avoid overfitting. ', '# ', '# To implement KFold cross validation, we will use the LightGBM cross validation function, `cv`, because this allows us to use a critical technique for training a GBM, early stopping. (For other machine learning models where we do not need to use early stopping, we can use the Scikit-Learn functions `RandomizedSearchCV` or `GridSearchCV`.)', '# ', '# ## Early Stopping', '# ', ""# One of the most important hyperparameters in a Gradient Boosting Machine is the number of estimators (the number of decision trees trained sequentially). We could set this as another hyperparameter in our search, but there's a better method: [early stopping](https://en.wikipedia.org/wiki/Early_stopping). Early stopping means training until the validation error does not decrease for a specified number of iterations. In the case of the GBM, this means training more decision trees, and in this example, we will use early stopping with 100 rounds, meaning that the training will continue until validation error has not decreased for 100 rounds. Then, the number of estimators that yielded the best score on the validation data will be chosen as the number of estimators to use in the final model."", '# ', ""# The concept of early stopping is commonly applied to the GBM and to deep neural networks so it's a great technique to understand. This is one of many forms of regularization that aims to improve generalization performance on the testing set by not overfitting to the training data. If we keep adding estimators, the training error will always decrease because the capacity of the model increases. Although this might seem positive, it means that the model will start to memorize the training data and then will not perform well on new testing data. The __variance__ of the model increases as we continue adding estimators because the model starts to rely too heavily on the training data (high variance means overfitting)."", '# ', '# Early stopping is simple to implement with the LightGBM library in the cross validation function. We simply need to pass in the number of early stopping rounds.', '# ', '# ### Example of Cross Validation and Early Stopping ', '# ', '# To use the `cv` function, we first need to make a LightGBM `dataset`. ', '# In[ ]:', '# Create a training and testing dataset', ""# We have to pass in a set of hyperparameters to the cross validation, so we will use the default hyperparameters in LightGBM. In the `cv` call, the `num_boost_round` is set to 10,000 (`num_boost_round` is the same as `n_estimators`), but this number won't actually be reached because we are using early stopping. As a reminder, the metric we are using is Receiver Operating Characteristic Area Under the Curve (ROC AUC)."", '# ', '# The code below carries out both cross validation with 5 folds and early stopping with 100 early stopping rounds. ', '# In[ ]:', '# Get default hyperparameters', '# Remove the number of estimators because we set this to 10000 in the cv call', '# Cross validation with early stopping', '# The `cv_results` is a dictionary with lists for the `metric` mean and the `metric` standard deviation. The last entry (index of -1) contains the best performing score. The length of each list in the dictionary will be the ""optimal"" number of estimators to train.', '# In[ ]:', '# We can use this result as a baseline model to beat. To find out how well the model does on our ""test"" data, we will retrain it on all the training data with the best number of estimators found during cross validation with early stopping.', '# In[ ]:', '# In[ ]:', '# Optimal number of esimators found in cv', '# Train and make predicions with model', '# This is the baseline score _before hyperparameter tuning_. The only difference we made from the default model was using early stopping to set the number of estimators (which by default is 100). ', '# ## Hyperparameter Tuning Implementation', '# ', '# Now we have the basic framework in place: we will use cross validation to determine the performance of model hyperparameters and early stopping with the GBM so we do not have to tune the number of estimators. The basic strategy for both grid and random search is simple: for each hyperparameter value combination, evaluate the cross validation score and record the results along with the hyperparameters. Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score, train the model on all the training data, and make predictions on the test data.', '# ', '# # Four parts of Hyperparameter tuning', '# ', ""# It's helpful to think of hyperparameter tuning as having four parts (these four parts also will form the basis of Bayesian Optimization):"", '# ', '# 1. Objective function: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize', '# 2. Domain: the set of hyperparameter values over which we want to search. ', '# 3. Algorithm: method for selecting the next set of hyperparameters to evaluate in the objective function.', '# 4. Results history: data structure containing each set of hyperparameters and the resulting score from the objective function.', '# ', '# Switching from grid to random search to Bayesian optimization will only require making minor modifications to these four parts. ', '# ', '# ## Objective Function', '# ', '# The objective function takes in hyperparameters and outputs a value representing a score. Traditionally in optimization, this is a score to minimize, but here our score will be the ROC AUC which of course we want to maximize. Later, when we get to Bayesian Optimization, we will have to use a value to minimize, so we can take $1 - \\text{ROC AUC}$ as the score. What occurs in the middle of the objective function will vary according to the problem, but for this problem, we will use cross validation with the specified model hyperparameters to get the cross-validation ROC AUC. This score will then be used to select the best model hyperparameter values. ', '# ', '# In addition to returning the value to maximize, our objective function will return the hyperparameters and the iteration of the search. These results will let us go back and inspect what occurred during a search. The code below implements a simple objective function which we can use for both grid and random search.', '# In[ ]:', '    # Number of estimators will be found using early stopping', '     # Perform n_folds cross validation', '    # results to retun', '# In[ ]:', '# # Domain', '# ', '# The domain, or search space, is all the possible values for all the hyperparameters that we want to search over. For random and grid search, the domain is a hyperparameter grid and usually takes the form of a dictionary with the keys being the hyperparameters and the values lists of values for each hyperparameter.', '# ', '# ## Hyperparameters for GBM', '# ', ""# To see which settings we can tune, let's make a model and print it out. You can also refer to the [LightGBM documentation](http://lightgbm.readthedocs.io/en/latest/Parameters.html) for the description of all the hyperparameters."", '# In[ ]:', '# Create a default model', '# Some of these we do not need to tune such as `silent`, `objective`, `random_state`, and `n_jobs`, and we will use early stopping to determine perhaps the most important hyperparameter, the number of individual learners trained, `n_estimators` (also referred to as `num_boost_rounds` or the number of iterations). Some of the hyperparameters do not need to be tuned if others are: for example, `min_child_samples` and `min_child_weight` both limit the complexity of individual decision trees by adjusting the minimum leaf observation requirements and therefore we will only adjust one. However, there are still many hyperparameters to optimize, and we will choose 10 to tune. ', '# ', ""# Choosing a hyperparameter grid is probably the most difficult part of hyperparameter tuning: it's nearly impossible ahead of time to say which values of hyperparameters will work well and the optimal settings will depend on the dataset. Moreover, the hyperparameters have complex interactions with each other which means that just tuning one at a time doesn't work because when we start changing other hyperparameters that will affect the one we just tuned! "", '# ', ""# If we have prior experience with a model, we might know where the best values for the hyperparameters typically lie, or what a good search space is. However, if we don't have much experience, we can simply define a large search space and hope that the best values are in there somewhere. Typically, when first using a method, I define a wide search space centered around the default values. Then, if I see that some values of hyperparameters tend to work better, I can concentrate the search around those values. "", '# ', '# A complete grid for the 10 hyperparameter is defined below. Each of the values in the dicionary must be a list, so we use `list` combined with `range`, `np.linspace`, and `np.logspace` to define the range of values for each hyperparameter. ', '# In[ ]:', '# Hyperparameter grid', '# One aspect to note is that if `boosting_type` is `goss`, then we cannot use `subsample` (which refers to training on only a fraction of the rows in the training data, a technique known as [stochastic gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting#Stochastic_gradient_boosting)). Therefore, we will need a line of logic in our algorithm that sets the `subsample` to 1.0 (which means use all the rows) if `boosting_type=goss`. As an example below, if we randomly select a set of hyperparameters, and the boosting type is ""goss"", then we set the `subsample` to 1.0.', '# In[ ]:', '# Randomly sample a boosting type', '# Set subsample depending on boosting type', '# The `boosting_type` and `is_unbalance` domains are pretty simple because these are categorical variables. For the hyperparameters that must be integers (`num_leaves`, `min_child_samples`), we use `range(start, stop, [step])` which returns a range of numbers from start to stop spaced by step (or 1 if not specified). `range` always returns integers, which means that if we want evenly spaced values that can be fractions, we need to use `np.linspace(start, stop, [num])`.  This works the same way except the third argument is the number of values (by default 100).', '# ', '# Finally, `np.logspace(start, stop, [num = 100], [base = 10.0])` returns values evenly spaced on a logarithmic scale. According to the [the docs](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html) ""In linear space, the sequence starts at $base^{start}$ (base to the power of start) and ends with $base ^{stop}$ "" This is useful for values that differ over several orders of magnitude such as the learning rate.', '# In[ ]:', '# Learning rate histogram', '# ### Learning Rate Domain', '# ', '# The learning rate domain is from 0.005 to 0.5. Using a logarithmic uniform distribution allows us to create a domain where there are as many values from 0.005 to 0.05 as from 0.05 to 0.5. In a linear space, there would be far more values from 0.05 to 0.5 because this represents a larger distance in linear space but in logarithmic space each of these two intervals is the same width because they are multiples of 10 of each other. (Think about going from 1 to 10 and then from 10 to 100. On a logarithmic scale, these intervals are the same size, but on a linear scale the latter is 10 times the size of the former). In other words, a logarithmic uniform distribution lets us sample more evenly from a domain that varies over several orders of magnitude. ', '# ', ""# If that's a little confusing, perhaps the graph above makes it clearer. We can also do a sanity check to make sure the spacing is correct by counting the number of values in each interval."", '# In[ ]:', '# Check number of values in each category', '    # Check values', '# As an example of a simple domain, the `num_leaves` is a uniform distribution. This means values are evenly spaced on a linear scale.', '# In[ ]:', '# number of leaves domain', '# # Algorithm for selecting next values', '# ', ""# Although we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random. "", '# ', '# We will implement these algorithms very shortly, as soon as we cover the final part of hyperparameter tuning.', '# # Results History', '# ', '# The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually _uses the past results to decide on the next hyperparmeters_ to evaluate. Random and grid search are _uninformed_ methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best! ', '# ', '# A dataframe is a useful data structure to hold the results.', '# In[ ]:', '# Dataframes for random and grid search', '# # Grid Search Implementation', '# ', '# Grid search is best described as exhuastive guess and check. We have a problem: find the hyperparameters that result in the best cross validation score, and a set of values to try in the hyperparameter grid - the domain. The grid search method for finding the answer is to try all combinations of values in the domain and hope that the best combination is  in the grid (in reality, we will never know if we found the best settings unless we have an infinite hyperparameter grid which would then require an infinite amount of time to run).', '# ', ""# Grid search suffers from one limiting problem: it is extremely computationally expensive because we have to perform cross validation with every single combination of hyperparameters in the grid! Let's see how many total hyperparameter settings there are in our simple little grid we developed."", '# In[ ]:', ""# Until Kaggle upgrades the kernels to quantum computers, we are not going to be able to run evan a fraction of the combinations! Let's assume 100 seconds per evaluation and see how many years this would take:"", '# In[ ]:', ""# I think we're going to need a better approach! Before we discuss alternatives, let's walk through how we would actually use this grid and evaluate all the hyperparameters."", '# ', '# The code below shows the ""algorithm"" for grid search. First, we [unpack the values](https://www.geeksforgeeks.org/packing-and-unpacking-arguments-in-python/) in the hyperparameter grid (which is a Python dictionary) using the line `keys, values = zip(*param_grid.items())`.  The key line is `for v in itertools.product(*values)` where we iterate through all the possible combinations of values in the hyperparameter grid one at a time.  For each combination of values, we create a dictionary `hyperparameters = dict(zip(keys, v))` and then pass these to the objective function defined earlier. The objective function returns the cross validation score from the hyperparameters which we record in the dataframe. This process is repeated for each and every combination of hyperparameter values. By using `itertools.product` (from [this Stack Overflow Question and Answer](https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists)), we create a [generator](http://book.pythontips.com/en/latest/generators.html) rather than allocating a list of all possible combinations which would be far too large to hold in memory. ', '# In[ ]:', '    # Dataframe to store results', '    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists', '    # Iterate through every possible combination of hyperparameters', '        # Create a hyperparameter dictionary', '        # Set the subsample ratio accounting for boosting type', '        # Evalute the hyperparameters', '        # Normally would not limit iterations', '    # Sort with best score on top', '# Normally, in grid search, we do not limit the number of evaluations. The number of evaluations is set by the total combinations in the hyperparameter grid (or the number of years we are willing to wait!). So the lines ', '# ', '# ```', '#         if i > MAX_EVALS:', '#             break', '# ```', '# ', '# would not be used in actual grid search. Here we will run grid search for 5 iterations just as an example. The results returned will show us the validation score (ROC AUC), the hyperparameters, and the iteration sorted by best performing combination of hyperparameter values.', '# In[ ]:', '# Now, since we have the best hyperparameters, we can evaluate them on our ""test"" data (remember not the real test data)!', '# In[ ]:', '# Get the best parameters', '# Create, train, test model', ""# It's interesting that the model scores better on the test set than in cross validation. Usually the opposite happens (higher on cross validation than on test) because the model is tuned to the validation data. In this case, the better performance is probably due to small size of the test data and we get very lucky (although this probably does not translate to the actual competition data). "", '# To get a sense of how grid search works, we can look at the progression of hyperparameters that were evaluated.', '# In[ ]:', '# Look at the `subsample` and the `is_unbalance` because these are the only hyperparameters that change. In fact, the effect of  changing these values is so small that validation scores literally did not change across runs (indicating this small of a change has no effect on the model). This is grid search trying every single value in the grid! No matter how small the increment between subsequent values of a hyperparameter, it will try them all. Clearly, we are going to need a more efficient approach if we want to find better hyperparameters in a reasonable amount of time. ', '# #### Application', '# ', '# If you want to run this on the entire dataset feel free to take these functions and put them in a script. However, I would advise against using grid search unless you have a very small hyperparameter grid because this is such as exhaustive method! ', '# Later, we will look at results from 1000 iterations of grid and random search run on the same small subset of data as we used above. I have not tried to run any form of grid search on the full data (and probably will not try this method).', '# # Random Search', '# ', '# Random search is surprisingly efficient compared to grid search. Although grid search will find the optimal value of hyperparameters (assuming they are in your grid) eventually, random search will usually find a ""close-enough"" value in far fewer iterations. [This great paper explains why this is so](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf): grid search spends too much time evaluating unpromising regions of the hyperparameter search space because it has to evaluate every single combination in the grid. Random search in contrast, does a better job of exploring the search space and therefore can usually find a good combination of hyperparameters in far fewer iterations. ', '# ', ""# As [this article](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881) lays out, random search should probably be the first hyperparameter optimization method tried because of its effectiveness. Even though it's an _uninformed_ method (meaning it does not rely on past evaluation results), random search can still usually find better values than the default and is simple to run."", '# ', '# Random search can also be thought of as an algorithm: randomly select the next set of hyperparameters from the grid! We can build a dictionary of hyperparameters by selecting one random value for each hyperparameter as follows (again accounting for subsampling):', '# In[ ]:', '# Randomly sample from dictionary', '# Deal with subsample ratio', '# Next, we define the `random_search` function. This takes the same general structure as `grid_search` except for the method used to select the next hyperparameter values. Moreover, random search is always run with a limit on the number of search iterations.', '# In[ ]:', '    # Dataframe for results', '    # Keep searching until reach max evaluations', '        # Choose random hyperparameters', '        # Evaluate randomly selected hyperparameters', '    # Sort with best score on top', '# In[ ]:', '# We can also evaluate the best random search model on the ""test"" data.', '# In[ ]:', '# Get the best parameters', '# Create, train, test model', '# Finally, we can view the random search sequence of hyperparameters.', '# In[ ]:', '# This time we see hyperparameter values that are all over the place, almost as if they had been selected at random! Random search will do a much better job than grid search of exploring the search domain (for the same number of iterations). If we have a limited time to evaluate hyperparameters, random search is a better option than grid search for exactly this reason.', '# ', '# ### Stacking Random and Grid Search', '# ', '# One option for a smarter implementation of hyperparameter tuning is to combine random search and grid search: ', '# ', '# 1. Use random search with  a large hyperparameter grid ', '# 2. Use the results of random search to build a focused hyperparameter grid around the best performing hyperparameter values.', '# 3. Run grid search on the reduced hyperparameter grid. ', '# 4. Repeat grid search on more focused grids until maximum computational/time budget is exceeded.', '# ', '# In a later notebook (upcoming), we will look at methods that use the past evaluation results to pick the next hyperparameter values to try in the objective function. These methods (including [Bayesian optimization](https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf)) are essentially doing what we would do in the strategy outlined above: adjust the next values tried in the search from the previous results. The overall objective of these _informed methods_ is to limit evaluations of the objective function by reasoning about the next values to try based on past evaluation results. These algorithms are therefore able to save time by evaluating more promising values of hyperparameters. This is a really cool topic and [Bayesian optimization](http://gpss.cc/gpmc17/slides/LancasterMasterclass_1.pdf) is fascinating so stay tuned for this upcoming notebook. ', '# ## Next Steps', '# ', '# We can now take these random and grid search functions and use them on the complete dataset or any dataset of our choosing. These search methods are very expensive, so expect the hyperparameter tuning to take a while.(I am currently running this script on a full set of features for 500 iterations and will make the results public when they are available. )', '# ', '# For now, we will turn to implementing random and grid search on the reduced dataset for 1000 iterations just to compare the results (I took the code below and already ran it because even with the small dataset, it takes a very long time. The results are available as part of the data in this kernel). ', '# ', '# ## Writing to File to Monitor Progress', '# ', ""# When we run these searches for a long time, it's natural to want to track the performance while the search is going on. We can print information to the command prompt, but this will grow cluttered after 1000 iterations and the results will be gone if we close the command prompt. A better solution (although not perfect) is to write a line to a csv (comma separated value) file on each iteration. Then, we can look at the file to track progress while the searching is running, and eventually, have the entire results saved when the search is complete."", '# ', '# ### Extremely Important Note about Checking Files', '# ', ""# When you want to check the csv file, __do not open it in Excel while the search is ongoing__. This will cause a permission error in Python and the search will be terminated. Instead, you can view the end of the file by typing `tail out_file.csv` from Bash where `out_file.csv` is the name of the file being written to. There are also some text editors, such as notepad or Sublime Text, where you can open the results safely while the search is occurring. However, __do not use Excel to open a file that is being written to in Python__. This is a mistake I've made several times so you do not have to! "", '# Below is the code we need to run before the search. This creates the csv file, opens a connection, writes the header (column names), and then closes the connection. This will overwrite any information currently in the `out_file`, so change to a new file name every time you want to start a new search.', '# In[ ]:', '# Create file and open connection', '# Write column names', '# Now we must slightly modify `random_search` and `grid_search` to write to this file every time. We do this by opening a connection, this time using the `""a""` option for append (the first time we used the `""w""` option for write) and writing a line with the desired information (which in this case is the cross validation score, the hyperparameters, and the number of the iteration). Then we close the connection until the function is called again.', '# In[ ]:', '    # Dataframe for results', '        # Choose random hyperparameters', '        # Evaluate randomly selected hyperparameters', '        # open connection (append option) and write results', '        # make sure to close connection', '    # Sort with best score on top', '# In[ ]:', '    # Dataframe to store results', '    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists', '    # Iterate through every possible combination of hyperparameters', '        # Select the hyperparameters', '        # Set the subsample ratio accounting for boosting type', '        # Evalute the hyperparameters', '        # open connection (append option) and write results', '        # make sure to close connection', '        # Normally would not limit iterations', '    # Sort with best score on top', '# To run these functions for 1000 iterations (or however many you choose) uncomment the cell below. Otherwise, I have run these functions on the reduced dataset and attached the results to this kernel.', '# In[ ]:', '# MAX_EVALS = 1000', '# # Create file and open connection', ""# out_file = 'grid_search_trials_1000.csv'"", ""# of_connection = open(out_file, 'w')"", '# writer = csv.writer(of_connection)', '# # Write column names', ""# headers = ['score', 'hyperparameters', 'iteration']"", '# writer.writerow(headers)', '# of_connection.close()', '# grid_results = grid_search(param_grid, out_file)', '# # Create file and open connection', ""# out_file = 'random_search_trials_1000.csv'"", ""# of_connection = open(out_file, 'w')"", '# writer = csv.writer(of_connection)', '# # Write column names', ""# headers = ['score', 'hyperparameters', 'iteration']"", '# writer.writerow(headers)', '# of_connection.close()', '# random_results = random_search(param_grid, out_file)', '# # Results on Limited Data', '# ', '# We can examine 1000 search iterations of the above functions on the reduced dataset. Later, we can try the hyperparameters that worked the best for the small versions of the data on a complete dataset to see if the best hyperparameters translate when increasing the size of the data 30 times! The 1000 search iterations were not run in a kernel, although they might be able to finish (no guarantees) in the 12 hour time limit. ', '# ', '# First we can find out which method returned the best results. ', '# In[ ]:', '# When we save the results to a csv, for some reason the dictionaries are saved as strings. Therefore we need to convert them back to dictionaries after reading in the results using the `ast.literal_eval` function.', '# In[ ]:', '# Convert strings to dictionaries', ""# Now let's make a function to parse the results from the hyperparameter searches. This returns a dataframe where each column is a hyperparameter and each row has one search result (so taking the dictionary of hyperparameters and mapping it into a row in a dataframe)."", '# In[ ]:', '    # Sort with best values on top', '    # Print out cross validation high score', '    # Use best hyperparameters to create a model', '    # Train and make predictions', '    # Create dataframe of hyperparameters', '    # Iterate through each set of hyperparameters that were evaluated', '    # Put the iteration and score in the hyperparameter dataframe', '# In[ ]:', '# In[ ]:', '# # Visualizations', '# ', ""# Visualizations are both enjoyable to make, and can give us an intuitive look into a technique. Here we will make a few simple plots using matplotlib, seaborn, and Altair! __Unfortunately, the Altair visualizations do not show up when the notebook is rendered. To view the Altair figures, you'll have to run the notebook yourself!__"", '# First we can plot the validation scores versus the iteration. Here we will use the [Altair](https://altair-viz.github.io/) visualization library to make some plots! First, we need to put our data into a long format dataframe.', '# In[ ]:', '# In[ ]:', '# Combine results into one dataframe', '# In[ ]:', '# Below, we make the same plot using seaborn because the Altair visualizations do not show up in the rendered notebook. ', '# In[ ]:', '# In[ ]:', '# Plot of scores over the course of searching', '# In[ ]:', '# The grid cross validation score increases over time. This indicates that whatever hyperparameters are changing in grid search are gradually increasing the score. The random cross validation scores on the other hand are all over the place as expected. This grid search appears to be stuck in a relatively low-performing region of the search space, and because it is constrained to try all the values in the grid, it is not able to try significantly different hyperparameter values that would perform better (as occurs in random search). The random search method does a very good job of exploring the search space as we will see when we look at the hyperparameter values searched. ', '# ## Distribution of Search Values', '# ', ""# We can show the distribution of search values for random search (grid search is very uninteresting). Even though we expect these to be _random_, it's always a good idea to check our code both quantitatively and visually. "", '# In[ ]:', '# Create bar chart', '# Add text for labels', '# Display', '# The boosting type should be evenly distributed for random search. ', '# ', '# Again, we have to remake this chart in seaborn to have the visualization appear in the rendered notebook (if anyone knows how to address this issue, please tell me in the comments!)', '# In[ ]:', '# Bar plots of boosting type', ""# Next, for the numeric hyperparameters, we will plot both the sampling distribution (the hyperparameter grid) and the results from random search in a kernel density estimate (KDE) plot. (The grid search results are completely uninteresting). As random search is just drawing random values, we would expect the random search distribution to align with the sampling grid (although it won't be perfectly aligned because of the limited number of searches). "", '# ', '# As an example, below we plot the distribution of learning rates from both the sampling distribution and the random search results. The vertical dashed line indicates the optimal value found from random search.', '# In[ ]:', '# In[ ]:', '# Density plots of the learning rate distributions ', '# The following code repeats this plot for all the of the numeric hyperparameters. ', '# In[ ]:', '# Iterate through each hyperparameter', '        # Plot the random search distribution and the sampling distribution', '# ## Sequence of Search Values', '# ', '# Finally, we can plot the sequence of search values against the iteration for random search. Clearly there will not be any order, but this can let us visualize what happens in a random search!', '# ', '# The star indicates the best value of the hyperparameter that was found.', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '# In[ ]:', '# Scatterplot of next four hyperparameters', '# ## Score versus Hyperparameters', '# As a final plot, we can show the score versus the value of each hyperparameter. We need to keep in mind that the hyperparameters are not changed one at a time, so if there are relationships between the values and the score, they do not mean that particular hyperparameter is influencing the score. However, we might be able to identify values of hyperparameters that seem more promising. Mostly these plots are for my own interest, to see if there are any trends! ', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '# Scatterplot of next four hyperparameters', '# We want to avoid placing too much emphasis on any of these relationships because we were not changing one hyperparameter at a time (although we could carry out experiments where we only change one hyperparameter and observes the effects on the score) and so the trends are not due solely to the single hyperparameter we show. If we could plot this in higher dimensions, it might be interesting to see if there are more promising regions of the search space but here we are limited to one dimension (a single hyperparameter versus the score).  If we want to observe the effects of one hyperparameter on the cross validation score, we could alter only that hyperparameter while holding all the others constant. However, the hyperparameters do not act by themselves and there are complex interactions between the model settings.', '# # Testing Results on Full Data', '# ', '# We can take the best hyperparameters found from the 1000 iterations of random search on the reduced training data and try these on an entire training dataset. Here, we will use the features from the `[Updated 0.792 LB] LightGBM with Simple Features', '# `(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) kernel (I did not develop these features and want to give credit to the numerous people, including [Aguiar](https://www.kaggle.com/jsaguiar) and [olivier](https://www.kaggle.com/ogrellier),  who have worked on these features. Please check out their [kernels](https://www.kaggle.com/ogrellier/lighgbm-with-selected-features)!). ', '# ', '# The code below uses the best random search hyperparameters to build a model, train on the full features from `[Updated 0.792 LB] LightGBM with Simple Features', '# `(https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features), and test on the testing features. The test data is the actual competition data, so we can then submit these and see how well the score translates to a full dataset! ', '# In[ ]:', '# Read in full dataset', '# Extract the test ids and train labels', '# First we will test the cross validation score using the best model hyperparameter values from random search. This can give us an idea of the generalization error on the test set. We will delete the number of estimators found from the smaller dataset and use early stopping to find the best number of decision trees to train. ', '# In[ ]:', '# Cross validation with n_folds and early stopping', '# In[ ]:', '# The public leaderboard score is only calculated on 10% of the test data, so the cross validation score might actually give us a better idea of how the model will perform on the full test set. Usually we expect the cross validation score to be higher than on the testing data, but because of the small size of the testing data, this might be reversed for this problem.', '# ', '# Next, we will make predictions on the test data that can be submitted to the competition. ', '# In[ ]:', '# Train the model with the optimal number of estimators from early stopping', '# Predictions on the test data', '# In[ ]:', '# The score when submitting to the test competition is __0.782__. The original score from the kernel where I got these features was 0.792, so we can conclude that the results from random search on the smaller dataset to not translate to a full dataset. I currently am running random search with 500 iterations on the full dataset, and will make those results publicly available when the search is complete! ', '# ## Model Tuning Next Steps', '# ', ""# From here, we might want to take the functions we wrote and apply them to a complete dataset. The results are likely to be different because we were only using a random subset of the training data. However, this will take much longer (300000+ observations instead of 10000). I'm currently running the random search on the full dataset from the Kernel referenced above, and will see how the results turn out. (Sampling some of the observations is not inherently negative, and it can help us get reasonable answers in a much shorter time frame. However, if we are using such a small portion of the data that is not representative of the entire dataset, then we should not expect the tuning to translate to the full dataset.)"", '# ', ""# In an upcoming notebook, we will turn to automated hyperparameter tuning, in particular, Bayesian Optimization. We will implement automated optimization of machine learning hyperparameters step-by-step using the Hyperopt open-source Python library. I'll provide the link here as soon as this notebook is finished, but if you want to get an idea of Bayesian optimization, you can check out [this introductory article](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0), or [this article on automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). These topics are pretty neat and it's incredible that they are available in an easy-to-use format for anyone to take advantage of. I'll see you in the next notebook! "", '# # Conclusions', '# ', '# Model tuning is the process of finding the best machine learning model hyperparameters for a particular problem. Random and grid search are two uniformed methods for hyperparameter tuning that search by selecting hyperparameter values from a grid domain. ', '# The four parts of hyperparameter tuning are:', '# ', '# 1. Objective function: takes in hyperparameters and returns the cross validation score we want to maximize or minimize', '# 2. Domain of hyperparameters: values over which we want to search', '# 3. Algorithm: method for selecting the next hyperparameter values to evaluate in the objective function', '# 4. Results: history of hyperparameters and cross validation scores', '# ', ""# These four parts apply to grid and random search as well as to Bayesian optimization, a form of automated hyperparameter tuning. In this notebook, we implemented both random and grid search on a reduced dataset, inspected the results, and tried to translate the optimal hyperparameters to a full dataset (from [this kernel](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features)). As a small note, it's important to remember that we tune the hyperparameters to the training data - using cross validation - so the hyperparameter values we find are only optimal for the training data. Although the best hyperparameters from the smaller dataset did not work that well on the full dataset, we were still able to see the ideas behind these two tuning methods. Moreover, we can take the functions developed here and apply them to any dataset or to any machine learning model, not just the gradient boosting machine. "", '# ', '# Random search turns out to work pretty well in practice (because it is good at exploring the search domain), but it still is not a reasoning method because it does not use past evaluation results to choose the next hyperparameter values. A better approach would be to use the past results to reason about the best values to try next in the objective function, especially because as we saw, evaluating the objective function is time-consuming! In future work, we will look at [implementing automated hyperparameter tuning](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a) using Bayesian optimization. ', '# ', ""# Hyperparameter tuning is a crucial part of the machine learning pipeline because the performance of a model can depend strongly on the choices of the hyperparameter values. Random and grid search are two decent methods to start tuning a model (at least they are better than manual tuning) and are important tools to have in the data science skillset. Thanks for reading and I'll see you in the next notebook!"", '# ', '# As always, I welcome feedback and constructive criticism. I write for Towards Data Science at https://medium.com/@williamkoehrsen/ and can be reached on Twitter at https://twitter.com/koehrsen_will', '# ', '# Will', '# In[ ]:']",423
introduction-to-feature-selection.py,"['# coding: utf-8', '# # Introduction: Feature Selection', '# ', '# In this notebook we will apply feature engineering to the manual engineered features built in two previous kernels. We will reduce the number of features using several methods and then we will test the performance of the features using a fairly basic gradient boosting machine model. ', '# ', '# The main takeaways from this notebook are:', '# ', '# * Going from 1465 total features to 536 and an AUC ROC of 0.783 on the public leaderboard', '# * A further optional step to go to 342 features and an AUC ROC of 0.782', '# ', '# The full set of features was built in [Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) and [Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2) of Manual Feature Engineering', '# ', '# We will use three methods for feature selection:', '# ', '# 1. Remove collinear features', '# 2. Remove features with greater than a threshold percentage of missing values', '# 3. Keep only the most relevant features using feature importances from a model', '# ', '# We will also take a look at an example of applying PCA although we will not use this method for feature reduction. ', '# Standard imports for data science work. The LightGBM library is used for the gradient boosting machine.', '# In[1]:', '# pandas and numpy for data manipulation', '# featuretools for automated feature engineering', '# matplotlit and seaborn for visualizations', '# Suppress warnings from pandas', '# modeling ', '# utilities', '# memory management', '# * `train_bureau` is the training features built manually using the `bureau` and `bureau_balance` data', '# * `train_previous` is the training features built manually using the `previous`, `cash`, `credit`, and `installments` data', '# ', '# We first will see how many features we built over the manual engineering process. Here we use a couple of set operations to find the columns that are only in the `bureau`, only in the `previous`, and in both dataframes, indicating that there are `original` features from the `application` dataframe. Here we are working with a small subset of the data in order to not overwhelm the kernel. This code has also been run on the full dataset (we will take a look at some of the results).', '# In[2]:', '# Read in data', '# All columns in dataframes', '# In[3]:', '# Bureau only features', '# Previous only features', '# Original features will be in both datasets', '# That gives us the number of features in each dataframe. Now we want to combine the data without creating any duplicate rows. ', '# In[4]:', '# Merge the dataframes avoiding duplicating columns by subsetting train_previous', ""# Next we want to one-hot encode the dataframes. This doesn't give the full features since we are only working with a sample of the data and this will not create as many columns as one-hot encoding the entire dataset would. Doing this to the full dataset results in 1465 features."", '# ', '# An important note in the code cell is where we __align the dataframes by the columns.__ This ensures we have the same columns in the training and testing datasets.', '# In[5]:', '# One hot encoding', '# Match the columns in the dataframes', '# When we do this to the full dataset, we get __1465__ features. ', '# ### Admit and Correct Mistakes!', '# ', '# When doing manual feature engineering, I accidentally created some columns derived from the client id, `SK_ID_CURR`. As this is a unique identifier for each client, it should not have any predictive power, and we would not want to build a model trained on this ""feature"". Let\'s remove any columns built on the `SK_ID_CURR`.', '# In[6]:', '# After applying this to the full dataset, we end up with __1416 __ features. More features might seem like a good thing, and they can be if they help our model learn. However, irrelevant features, highly correlated features, and missing values can prevent the model from learning and decrease generalization performance on the testing data. Therefore, we perform feature selection to keep only the most useful variables.', '# ', '# We will start feature selection by focusing on collinear variables.', '# # Remove Collinear Variables', '# ', ""# Collinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set. Clearly, these are three things we want to increase, so removing collinear variables is a useful step. We will establish an admittedly arbitrary threshold for removing collinear variables, and then remove one out of any pair of variables that is above that threshold. "", '# ', '# The code below identifies the highly correlated variables based on the absolute magnitude of the Pearson correlation coefficient being greater than 0.9. Again, this is not entirely accurate since we are dealing with such a limited section of the data. This code is for illustration purposes, but if we read in the entire dataset, it would work (if the kernels allowed it)! ', '# ', '# This code is adapted from [work by Chris Albon](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/).', '# ### Identify Correlated Variables', '# In[7]:', '# Threshold for removing correlated variables', '# Absolute value correlation matrix', '# In[8]:', '# Upper triangle of correlations', '# In[9]:', '# Select columns with correlations above threshold', '# #### Drop Correlated Variables', '# In[10]:', '# Applying this on the entire dataset __results in 538  collinear features__ removed.  ', '# ', ""# This has reduced the number of features singificantly, but it is likely still too many. At this point, we'll read in the full dataset after removing correlated variables for further feature selection."", '# ', '# The full datasets (after removing correlated variables) are available in `m_train_combined.csv` and `m_test_combined.csv`.', '# ### Read in Full Dataset', '# ', '# Now we are ready to move on to the full set of features. These were built by applying the above steps to the entire `train_bureau` and `train_previous` files (you can do the same if you want and have the computational resources)!', '# In[ ]:', '# In[ ]:', '# # Remove Missing Values', '# ', '# A relatively simple choice of feature selection is removing missing values. Well, it seems simple, at least until we have to decide what percentage of missing values is the minimum threshold for removing a column. Like many choices in machine learning, there is no right answer, and not even a general rule of thumb for making this choice. In this implementation, if any columns have greater than 75% missing values, they will be removed. ', '# ', ""# Most models (including those in Sk-Learn) cannot handle missing values, so we will have to fill these in before machine learning. The Gradient Boosting Machine ([at least in LightGBM](https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst)) can handle missing values. Imputing missing values always makes me a little uncomfortable because we are adding information that actually isn't in the dataset. Since we are going to be evaluating several models (in a later notebook), we will have to use some form of imputation. For now, we will focus on removing columns above the threshold."", '# In[33]:', '# Train missing values (in percent)', '# In[34]:', '# Test missing values (in percent)', '# In[35]:', '# Identify missing values above threshold', ""# Let's drop the columns, one-hot encode the dataframes, and then align the columns of the dataframes."", '# In[36]:', '# Need to save the labels because aligning will remove this column', '# In[37]:', '# # Feature Selection through Feature Importances', '# ', '# The next method we can employ for feature selection is to use the feature importances of a model. Tree-based models (and consequently ensembles of trees) can determine an ""importance"" for each feature by measuring the reduction in impurity for including the feature in the model. I\'m not really sure what that means (any explanations would be welcome) and the absolute value of the importance can be difficult to interpret. However, the relative value of the importances can be used as an approximation of the ""relevance"" of different features in a model. Moreover, we can use the feature importances to remove features that the model does not consider important. ', '# ', '# One method for doing this automatically is the [Recursive Feature Elimination method](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) in Scikit-Learn. This accepts an estimator (one that either returns feature weights such as a linear regression, or feature importances such as a random forest) and a desired number of features. In then fits the model repeatedly on the data and iteratively removes the lowest importance features until the desired number of features is left. This means we have another arbitrary hyperparameter to use in out pipeline: the number of features to keep! ', '# ', ""# Instead of doing this automatically, we can perform our own feature removal by first removing all zero importance features from the model. If this leaves too many features, then we can consider removing the features with the lowest importance. We will use a Gradient Boosted Model from the LightGBM library to assess feature importances. If you're used to the Scikit-Learn library, the LightGBM library has an API that makes deploying the model very similar to using a Scikit-Learn model. "", '# Since the LightGBM model does not need missing values to be imputed, we can directly `fit` on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features.', '# In[38]:', '# Initialize an empty array to hold feature importances', '# Create the model with several hyperparameters', '# In[39]:', '# Fit the model twice to avoid overfitting', '    # Split into training and validation set', '    # Train using early stopping', '    # Record the feature importances', '# In[40]:', '# Make sure to average feature importances! ', '# In[48]:', '# Find the features with zero importance', ""# We see that one of our features made it into the top 5 most important! That's a good sign for all of our hard work making the features. It also looks like many of the features we made have literally 0 importance. For the gradient boosting machine, features with 0 importance are not used at all to make any splits. Therefore, we can remove these features from the model with no effect on performance (except for faster training). "", '# In[41]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    # Set the yticks and labels', '    # Plot labeling', '    # Cumulative importance plot', '# In[42]:', ""# Let's remove the features that have zero importance."", '# In[44]:', '# At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function.', '# In[45]:', '    # Initialize an empty array to hold feature importances', '    # Create the model with several hyperparameters', '    # Fit the model multiple times to avoid overfitting', '        # Split into training and validation set', '        # Train using early stopping', '        # Record the feature importances', '    # Find the features with zero importance', '# In[46]:', ""# There are now no 0 importance features left (I guess we should have expected this). If we want to remove more features, we will have to start with features that have a non-zero importance. One way we could do this is by retaining enough features to account for a threshold percentage of importance, such as 95%. At this point, let's keep enough features to account for 95% of the importance. Again, this is an arbitrary decision! "", '# In[47]:', ""# We can keep only the features needed for 95% importance. This step seems to me to have the greatest chance of harming the model's learning ability, so rather than changing the original dataset, we will make smaller copies. Then, we can test both versions of the data to see if the extra feature removal step is worthwhile. "", '# In[49]:', '# Threshold for cumulative importance', '# Extract the features to keep', '# Create new datasets with smaller features', '# In[50]:', '# # Test New Featuresets', '# ', '# The last step of feature removal we did seems like it may have the potential to hurt the model the most. Therefore we want to test the effect of this removal. To do that, we can use a standard model and change the features. ', '# ', ""# We will use a fairly standard LightGBM model, similar to the one we used for feature selection. The main difference is this model uses five-fold cross validation for training and we  use it to make predictions. There's a lot of code here, but that's because I included documentation and a few extras (such as feature importances) that aren't strictly necessary. For now, understanding the entire model isn't critical, just know that we are using the same model with two different datasets to see which one performs the best."", '# In[52]:', '    # Extract the ids', '    # Extract the labels for training', '    # Remove the ids and target', '    # One Hot Encoding', '        # Align the dataframes by the columns', '        # No categorical indices to record', '    # Integer label encoding', '        # Create a label encoder', '        # List for storing categorical indices', '        # Iterate through each column', '                # Map the categorical features to integers', '                # Record the categorical indices', '    # Catch error if label encoding scheme is not valid', '    # Extract feature names', '    # Convert to np arrays', '    # Create the kfold object', '    # Empty array for feature importances', '    # Empty array for test predictions', '    # Empty array for out of fold validation predictions', '    # Lists for recording validation and training scores', '    # Iterate through each fold', '        # Training data for the fold', '        # Validation data for the fold', '        # Create the model', '        # Train the model', '        # Record the best iteration', '        # Record the feature importances', '        # Make predictions', '        # Record the out of fold predictions', '        # Record the best score', '        # Clean up memory', '    # Make the submission dataframe', '    # Make the feature importance dataframe', '    # Overall validation score', '    # Add the overall scores to the metrics', '    # Needed for creating dataframe of validation scores', '    # Dataframe of validation scores', '# ### Test ""Full"" Dataset', '# ', '# This is the expanded dataset. To recap the process to make this dataset we:', '# ', '# * Removed collinear features as measured by the correlation coefficient greater than 0.9', '# * Removed any columns with greater than 80% missing values in the train or test set', '# * Removed all features with non-zero feature importances', '# In[53]:', '# In[54]:', '# In[55]:', '# The full features after feature selection score __0.783__ when submitted to the public leaderboard. ', '# ### Test ""Small"" Dataset', '# ', '# The small dataset requires one additional step over the ful l dataset:', '# ', '# * Keep only features needed to reach 95% cumulative importance in the gradient boosting machine', '# In[56]:', '# In[57]:', '# In[58]:', '# The smaller featureset scores __0.782__ when submitted to the public leaderboard.', '# # Other Options for Dimensionality Reduction', '# ', '# We only covered a small portion of the techniques used for feature selection/dimensionality reduction. There are many other methods such as:', '# ', '# * PCA: Principle Components Analysis (PCA)', '# * ICA: Independent Components Analysis (ICA)', '# * Manifold learning: [also called non-linear dimensionality reduction](https://stats.stackexchange.com/questions/247907/what-is-the-difference-between-manifold-learning-and-non-linear-dimensionality-r)', '# ', ""# PCA is a great method for reducing the number of features provided that you do not care about model interpretability. It projects the original set of features onto a lower dimension, in the process, eliminating any physical representation behind the features. Here's a pretty thorough introduction to the math for anyone interested. PCA also assumes that the data is Gaussian distributed, which may not be the case, especially when dealing with real-world human generated data. "", '# ', '# ICA representations also obscure any physical meaning behind the variables and presevere the most ""independent"" dimensions of the data (which is different than the dimensions with the most variance). ', '# ', '# Manifold learning is more often used for low-dimensional visualizations (such as with T-SNE or LLE) rather than for dimensionality reduction for a classifier. These methods are heavily dependent on several hyperparameters and are not deterministic which means that there is no way to apply it to new data (in other words you cannot `fit` it to the training data and then separately `transform` the testing data). The learned representation of a dataset will change every time you apply manifold learning so it is not generally a stable method for feature selection.', '# ## PCA Example', '# ', '# We can go through a quick example to show how PCA is implemented. Without going through too many details, PCA finds a new set of axis (the principal components) that maximize the amount of variance captured in the data. The original data is then projected down onto these principal components. The idea is that we can use fewer principal components than the original number of features while still capturing most of the variance. PCA is implemented in Scikit-Learn in the same way as preprocessing methods. We can either select the number of new components, or the fraction of variance we want explained in the data. If we pass in no argument, the number of principal components will be the same as the number of original features. We can then use the `variance_explained_ratio_` to determine the number of components needed for different threshold of variance retained.', '# In[59]:', '# Make sure to drop the ids and target', '# Make a pipeline with imputation and pca', '# Fit and transform on the training data', '# transform the testing data', '# In[ ]:', '# Extract the pca object', '# Plot the cumulative variance explained', '# We only need a few prinicipal components to account for the majority of variance in the data. We can use the first two principal components to visualize the entire dataset. We will color the datapoints by the value of the target to see if using two principal components clearly separates the classes.', '# In[ ]:', '# Dataframe of pca results', '# Plot pc2 vs pc1 colored by target', '# In[ ]:', '# Even though we have accounted for most of the variance, that does not mean the pca decomposition makes the problem of identifying loans repaid vs not repaid any easier. PCA does not consider the value of the label when projecting the features to a lower dimension. Feel free to try a classifier on top of this data, but when I have done so, I noticed that it was not very accurate. ', '# # Conclusions', '# ', '# In this notebook we employed a number of feature selection methods. These methods are necessary to reduce the number of features to increase model interpretability, decrease model runtime, and increase generalization performance on the test set. The methods of feature selection we used are:', '# ', '# 1. Remove highly collinear variables as measured by a correlation coefficient greater than 0.9', '# 2. Remove any columns with more than 75% missing values.', '# 3. Remove any features with a zero importance as determined by a gradient boosting machine.', '# 4. (Optional) keep only enough features to account for 95% of the importance in the gradient boosting machine.', '# ', '# Using the first three methods, we reduced the number of features from __1465__ to __536__ with a 5-fold cv AUC ROC score of 0.7838 and a public leaderboard score of 0.783.', '# ', '# After applying the fourth method, we end up with 342 features with a 5-fold cv AUC SCORE of 0.7482 and a public leaderboard score of 0.782. ', '# ', '# Going forward, we might actually want to add _more_ features except this time, instead of naively applying aggregations, think about what features are actually important from a domain point of view. There are a number of kernels that have created useful features that we can add to our set here to improve performance. The process of feature engineering - feature selection is iterative, and it may require several more passes before we get it completely right! ', '# In[ ]:']",256
kernal-for-credit.py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", '# to get a better understanding of kaggle and how to use different frameworks/languages ', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# sklearn preprocessing for dealing with categorical variables', '# File system manangement', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[ ]:', '# Missing values statistics', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# In[ ]:', '# one-hot encoding of categorical variables', '# In[ ]:', '# In[ ]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Replace the anomalous values with nan', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Display correlations', '# In[ ]:', '# In[ ]:', '# Set the style of plots', '# Plot the distribution of ages in years', '# In[ ]:', '# KDE plot of loans that were repaid on time', '# KDE plot of loans which were not repaid on time', '# Labeling of plot', '# In[ ]:', '# Bin the age data', '# In[ ]:', '# In[ ]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# In[ ]:', '# In[ ]:', '# Heatmap of correlations', '# In[ ]:', '# iterate through the sources', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# In[ ]:', '# Copy the data for plotting', '# Add in the age of the client in years', '# Drop na values and limit to first 100000 rows', '# Function to calculate correlation coefficient between two columns', '# Create the pairgrid object', '# Upper is a scatter plot', '# Diagonal is a histogram', '# Bottom is density plot', '# In[ ]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[ ]:', '# Make the model with the specified regularization parameter', '# Train on the training data', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",112
kernel24647bb75c.py,"['# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",37
kernel7c8856c435.py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Function to calculate missing values by column# Funct ', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Number of unique classes in each object column', '# In[ ]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# In[ ]:', '# one-hot encoding of categorical variables', '# In[ ]:', '# Align the training and testing data, keep only columns present in both dataframes', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# In[ ]:', '# In[ ]:', '# Find correlations with the target and sort', '# Display correlations', '# In[ ]:', '# Find the correlation of the positive days since birth and target', '# In[ ]:', '# Set the style of plots', '# Plot the distribution of ages in years', '# In[ ]:', '# KDE plot of loans that were repaid on time', '# KDE plot of loans which were not repaid on time', '# Labeling of plot', '# In[ ]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[ ]:', '# Group by the bin and calculate averages', '# In[ ]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '# In[ ]:', '# Extract the EXT_SOURCE variables and show correlations', '# In[ ]:', '# Heatmap of correlations', '# In[ ]:', '# iterate through the sources', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# In[ ]:', '# Copy the data for plotting', '# Add in the age of the client in years', '# Drop na values and limit to first 100000 rows', '# Function to calculate correlation coefficient between two columns', '# Create the pairgrid object', '# Upper is a scatter plot', '# Diagonal is a histogram', '# Bottom is density plot', '# In[ ]:', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# Need to impute missing values', '# Create the polynomial object with specified degree', '# In[ ]:', '# Train the polynomial features', '# Transform the features', '# In[ ]:', '# In[ ]:', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# In[ ]:', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# iterate through the new features', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# In[ ]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '# Median imputation of missing values', '# Scale each feature to 0-1', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[ ]:', '# Make the model with the specified regularization parameter', '# Train on the training data', '# In[ ]:', '# Make predictions', '# Make sure to select the second column only', '# In[ ]:', '# Submission dataframe', '# In[ ]:', '# Save the submission to a csv file', '# In[ ]:']",135
kernelf68f763785.py,"['# coding: utf-8', '# In[1]:', '# Any results you write to the current directory are saved as output.', '# In[2]:', '# In[3]:', '    # \xad', '        # objectd\xa0\xa0\xa0\xad', '# In[4]:', '# One-hot encoding for categorical columns with get_dummies', '# Create a label encoder object', '# In[5]:', '    # Read data and merge', '    # Remove some rows with values not present in test set', '    # Remove some empty features', '     # Replace some outliers', '    # Some simple new features (percentages)', '    # Some new features', '# In[6]:', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '    # Time based aggregations: last x months', '    # Last loan max overdue', '    # Ratios: total debt/total credit and active loans debt/ active loans credit', '# In[7]:', '    # Replace some outliers', '    # Some new features', '    # Categorical features with One-Hot encode', '    # Aggregations for application set', '    # Previous Applications: Approved Applications', '    # Previous Applications: Refused Applications', '# In[8]:', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[9]:', '    # prepare data', '    # parameters', '    # range ', '    # optimize', '    # output optimization process', '    # return best parameters', '# In[10]:', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '# def kfold_lightgbm(df, num_folds, stratified = False, debug= False):', '#     # Divide in training/validation and test data', ""#     train_df = df[df['TARGET'].notnull()]"", ""#     test_df = df[df['TARGET'].isnull()]"", '#     print(""Starting LightGBM. Train shape: {}, test shape: {}"".format(train_df.shape, test_df.shape))', '#     del df', '#     gc.collect()', '#     # Cross validation model', '#     if stratified:', '#         folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=47)', '#     else:', '#         folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)', '#     # Create arrays and dataframes to store results', '#     oof_preds = np.zeros(train_df.shape[0])', '#     sub_preds = np.zeros(test_df.shape[0])', '#     feature_importance_df = pd.DataFrame()', ""#     feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]"", ""#     for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):"", ""#         train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]"", ""#         valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]"", '#         # LightGBM parameters found by Bayesian optimization', '#         clf = LGBMClassifier(', '#             nthread=4,', '#             #is_unbalance=True,', '#             n_estimators=10000,', '#             learning_rate=0.02,', '#             num_leaves=32,', '#             colsample_bytree=0.9497036,', '#             subsample=0.8715623,', '#             max_depth=8,', '#             reg_alpha=0.04,', '#             reg_lambda=0.073,', '#             min_split_gain=0.0222415,', '#             min_child_weight=40,', '#             silent=-1,', '#             verbose=-1,', '#             #scale_pos_weight=11', '#             )', '#         clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], ', ""#             eval_metric= 'auc', verbose= 400, early_stopping_rounds= 200)"", '#         oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]', '#         sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits', '#         fold_importance_df = pd.DataFrame()', '#         fold_importance_df[""feature""] = feats', '#         fold_importance_df[""importance""] = clf.feature_importances_', '#         fold_importance_df[""fold""] = n_fold + 1', '#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)', ""#         print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))"", '#         del clf, train_x, train_y, valid_x, valid_y', '#         gc.collect()', ""#     print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))"", '#     # Write submission file and plot feature importance', '#     if not debug:', ""#         test_df['TARGET'] = sub_preds"", ""#         test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)"", '#     display_importances(feature_importance_df)', '#     return feature_importance_df', '# # Display/plot feature importance', '# def display_importances(feature_importance_df_):', '#     cols = feature_importance_df_[[""feature"", ""importance""]].groupby(""feature"").mean().sort_values(by=""importance"", ascending=False)[:40].index', '#     best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]', '#     plt.figure(figsize=(8, 10))', '#     sns.barplot(x=""importance"", y=""feature"", data=best_features.sort_values(by=""importance"", ascending=False))', ""#     plt.title('LightGBM Features (avg over folds)')"", '#     plt.tight_layout()', ""#     plt.savefig('lgbm_importances01.png')"", '#     with timer(""Run LightGBM with kfold""):', '#         #print(df.shape)', '#         #df.drop(features_with_no_imp_at_least_twice, axis=1, inplace=True)', '#         #gc.collect()', '#         print(df.shape)', '#         feat_importance = kfold_lightgbm(df, num_folds= 5, stratified= False, debug= debug)', '#     submission_file_name = ""submission_kernel.csv""']",127
kernelfadb0573fe.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '#  -   .    ', '# Add target back in to the data', '# In[6]:', '#      ', '#   ', '#     3', '#   ', '#  ', '# In[7]:', '# In[8]:', '#     ', '#  ', '#  ', '#     ', '# In[9]:', '#     ', '#   ', '#   ', '#  ', '#  ', '# In[10]:', '#     ', '#   ', '#    ', '# ', '#   ', '#     ', '#     ', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '#  ', '#  ', '#    ', '#    ', '# ', '# In[22]:', '# In[23]:', '#  ', '#    ', '#    ', '#    ', '# ', '# In[24]:', '#        ', '# ', '#    ', '# ', '#   ', '#  ', '# In[25]:', '#   ', '#  ', '# In[26]:', '# In[27]:']",75
kfold-lightgbm.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# One-hot encoding for categorical columns with get_dummies', '# In[ ]:', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# In[ ]:', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[ ]:', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# In[ ]:', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# In[ ]:', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# In[ ]:', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[ ]:', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# In[ ]:', '# Display/plot feature importance', '# In[ ]:', '# In[ ]:', '# In[ ]:']",54
lanasore-ver2.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# ###  app   ', '# In[2]:', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# In[3]:', '# ### Application_train  EDA ', '# In[4]:', '# app_train \xa0   122 ,       67 ', '# In[5]:', '# In[6]:', '# In[7]:', '# ####   feature  TARGET  0 1 Histogram ', '# -  feature  TATGET   ', '# In[8]:', '# In[9]:', '# Feature Importances     \xa0', '# In[10]:', '# DAYS_BIRTH \xa0 TARGET ', '# In[11]:', '# \xa0   \xa0  ', '# In[12]:', '# \xa0   \xa0  plot (TARGET=0)', '# \xa0   \xa0  plot (TARGET=1)', '# ###     3  EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 ', '# In[13]:', '#', '# In[14]:', '# In[15]:', '# TARGET  EXT_SOURCE  EXT_SOURCE   ', '# In[16]:', '# In[17]:', '#  EXT_SOURCE  TARGET  \xa0  ', '# In[18]:', '# iterate through the sources', '# ### category (object ) TARGET \xa0  Count ', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '#         -->  value_counts() ', '# ###   \xa0     ', '# In[23]:', '# ###  Application    feature engineering ', '# In[24]:', '# In[25]:', '# In[26]:', '# \xa0 mean std  ', '# max, min    ,        \xa0    mean std  ', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# AMT_CREDIT  Feature ', '# In[31]:', '# AMT_INCOME_TOTAL  Feature ', '# AMT_INCOME_TOTAL    \xa0  ', '#  \xa0\xa0    . ', '# In[32]:', '# DAYS_BIRTH, DAYS_EMPLOYED  Feature ', '# DAYS_BIRTH, DAYS_EMPLOYED  / \xa0 Feature . ', '# In[33]:', '#  \xa0 , NULL LightGBM     ', '# ###      ', '# In[34]:', '# ####     \xa0 LGBM Classifier  ', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# ####  Classifier    \xa0  Kaggle Submit ', '# In[39]:', '# In[40]:', '# In[ ]:', '# In[ ]:']",85
level-3-home-credit-a-gentle-introduction.py,"['# coding: utf-8', '# # Level 3. Home Credit Default Risk', '# ### - Start Here: A Gentle Introduction -', '# # **   \xa0 **', '# ## \xa0 : ', '# ---', '# ', '#  -  : https://kaggle-kr.tistory.com/32?category=868318', '# ', '#  -  : https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction', '# ---', '# # ', '# 1. ', '# ', '# ', '# 2. \xa0 \xa0,  ', '# 2-1. \xa0 \xa0', '# 2-2.  \xa0', '# 2-3.  ', '# ', '# ', '# 3. EDA ( \xa0   )', '# ', '# 4. Feature Engineering', '# 4-1. Polynomial Features', '# 4-2. Domain Knowledge Features', '# ', '# 5.  ', '# # 1. ', '# >  \xa0   \xa0  \xa0     . ', '# ', '#           .', '# ', '#   \xa0  \xa0\xad  \xa0   ', '#      \xa0\xad  .', '# ', '#       \xa0   .', '# \xa0      .', '#  - 0 ( \xa0   )', '#  - 1 ( \xa0   \xa0 )', '# # 2. \xa0 \xa0,  \xa0', '# ## 2-1. \xa0 \xa0', '#  \xa0   \xa0.', '# ', '# > ""  \xa0\xad   \xa0  \xa0 .""', '# ', '#  Home Credit \xa0 .', '# ', '# > 1997     . ', '# \xa0 \xa0 \xa0     \xa0 \xa0 .', '#   Home Credit  \xa0\xa0\xa0 \xa0   \xa0 \xa0 \xa0 \xa0.', '#   \xa0\xa0      Home Credit \xa0  \xa0   ', '# \xa0   \xa0    .     \xa0  \xa0 ', '#  \xa0     . Home Credit      ', '#    \xa0   \xa0 , Kagglers      \xa0.', '# ## 2-2.  \xa0', '# ', '#  Home Credit   \xa0,  7 .', '# ', '#  - application_train/application_test ', '#  Home Credit   \xa0\xad  \xa0  ', '#  \xa0 \xad   , SK_ID_CURR feature ,  ', '#  0 :  , 1 :     .', '#  - bureau ', '#     \xa0 \xa0  \xad', '#  - bureau_balance ', '#  bureau   ', '#  - previous_application ', '#  Home Credit  \xa0 \xa0  \xad', '#  SK_ID_PREV feature  .', '#  - POS_CASH_balance ', '#  application data    \xa0 \xa0  \xa0\xad \xad', '#  - credit_card_balance ', '#  Home Credit   \xa0 \xa0  \xad,  ', '#  - installments_payment ', '#  Home Credit   \xa0   \xad', '#  ', '#    \xa0    .', '# ', '# ![home_credit](https://user-images.githubusercontent.com/53182751/69926570-4f1d8100-14f8-11ea-8111-1f6dfc90f867.png)', '# ', '#    6    .', '# 1. \xa0 ', '# 2.    \xa0 ', '# 3.   ', '# 4.  \xa0 ', '# 5.  \xa0 ', '# 6. \xa0\xad  ', '# ', '#         ', '#     ,   ', '# application_train application_test  \xa0 .', '# ## 2-3.  ', '# ', '#   numpy, pandas, sklearn, matplotlib  \xa0 \xa0.', '# In[1]:', '#  ', '#   ', '# In[2]:', '# print(os.listdir(""../input/""))', '#  ', '# In[3]:', '# Training data ', '# Training data  307,511   122 (feature) \xa0 .', '# In[4]:', '# Testing data ', '# Test data  Training   \xa0, (TARGET)  .', '# # 3. EDA (\xa0  )', '# ', '# EDA   \xa0  , ,      .', '#      \xa0  \xa0    .', '# ', '#     .', '# ### Target   ', '# In[5]:', '# In[6]:', '#  0( \xa0 ) 1( \xa0  x)     ', '#    \xa0  2 \xa0 Porto    ', '# \xa0 \xa0    .', '# ###  ', '# In[7]:', '        #  ', '        #  ', '        #   \xa0', '        #   \xa0', '        #   \xa0\xa0', '        # ', '        # return', '# In[8]:', '#  ', '# \xa0  \xa0 ,     .', '#    feature  \xa0 \xa0   \xa0 .', '# ', '#  XGBoost   \xa0      .', '# ###    ', '# In[9]:', '#    ', '# In[10]:', '# Number of unique classes in each object column', '# object ( ) \xa0\xa0 \xad  ', '# ###   ', '# ', '#   \xa0,     .', '# \xa0       . (LightGBM  \xa0)', '#       \xa0     .', '#    2 .', '# ', '#  - Label encoding ', '#  ![label_encoding](https://user-images.githubusercontent.com/53182751/69928041-ba1d8680-14fd-11ea-91c6-c502b8638dc1.png)', '# ', '#  - one-hot encoding ', '#  ![one_hot_encoding](https://user-images.githubusercontent.com/53182751/69928068-d4576480-14fd-11ea-8ba6-ff8e93312f6e.png)', '#  ', '#  \xa0  (/ ) 2 \xa0\xa0  \xa0 ', '#  Lable encoding  , 2  \xa0\xa0  \xa0 ', '#  One-hot encoding      ', '#  2  \xa0    Label encoding', '#  2   \xa0    one-hot encoding  .', '# ', '# ### Label Encoding and One-Hot Encoding', '# In[11]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '            # Train on the training data', '            # Transform both training and testing data', '            # Keep track of how many columns were label encoded', '# In[12]:', '# one-hot encoding of categorical variables', '# ### Train, Test   \xa0\xa0', '# ', '# Train, Test     \xa0  .', '# ', '# one-hot encoding    dummy      Test    .', '# In[13]:', '# TARGET    ', '#  \xa0 Train, Test  \xa0\xa0 (    )', '# TARGET ', '# ###  EDA ', '# ', '# ####  \xa0 ', '# ', '# EDA \xa0  \xad  \xa0    .', '#  \xa0  , \xa0 \xa0 \xa0  .', '# ', '# DAYS_BIRTH  ', '#  DAYS_BIRTH ,  .', '#   DAYS_EMPLOYED  DAYS    ,', '#   "" \xa0\xad \xa0 -    \xa0""  ', '# (-)      .', '# ', '#       \xa0  , \xa0 DAYS_BIRTH ', '#  \xa0 365    \xa0,  10   ', '# \xa0     .', '# ', '# \xa0    \xa0 .', '# In[14]:', '#  ', '#     \xa0 .', '#       .', '# ', '#  \xa0 ', '# In[15]:', '#   1000    \xa0   .', '# In[16]:', '#  \xa0 \xa0 \xa0 \xa0  ', '#  \xa0   .', '# In[17]:', '#  \xa0  \xa0 \xa0 \xa0     !', '# In[18]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# In[19]:', '# ## ', '# ', '#     EDA .', '#    feature    .', '# ', '#  \xa0  \xa0   .', '# ', '#  - .00-.19   ', '#  - .20-.39  ', '#  - .40-.59  ', '#  - .60-.79  ', '#  - .80-1.0   ', '# In[20]:', '# Find correlations with the target and sort', '# Display correlations', '# ###      ', '# In[21]:', '# Find the correlation of the positive days since birth and target', '#   \xa0     ', '#      \xa0\xa0 \xa0  .', '# In[22]:', '# Set the style of plots', '# Plot the distribution of ages in years', '# In[23]:', '# \xa0    ', '# \xa0     ', '# Labeling of plot', '# \xa0     \xa0     .', '# ', '#   \xa0    \xa0.', '# In[24]:', '# Age information into a separate dataframe', '# Bin the age data', '# In[25]:', '# Group by the bin and calculate averages', '# In[26]:', '# Graph the age bins and the average of the target as a bar plot', '# Plot labeling', '#    \xa0  ', '#        .', '# ###   ', '# ', '# TARGET       3 ', '# EXT_SURCE_1, EXT_SURCE_2, EXT_SURCE_3  ', '#  feature  \xa0 .', '# In[27]:', '# Extract the EXT_SOURCE variables and show correlations', '# In[28]:', '# Heatmap of correlations', '# 3  EXT_SOURCE   TARGET    \xa0 ', '# EXT_SOURCE     \xa0  \xa0 ', '#   .', '# ', '#  DayS_BIRTH EXT_SOURCE_1      \xa0', '#    \xa0 \xa0   .', '# In[29]:', '# iterate through the sources', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '#    EXT_SOURCE_3      ,', '#   \xa0\xad  \xa0       .', '# ## Pairs Plot', '# ', '#  \xa0 , EXT_SOURCE  DatS_BIRTH     .', '# In[30]:', '# Copy the data for plotting', '# Add in the age of the client in years', '# Drop na values and limit to first 100000 rows', '# Function to calculate correlation coefficient between two columns', '# Create the pairgrid object', '# Upper is a scatter plot', '# Diagonal is a histogram', '# Bottom is density plot', '# # 4. Feature Engineering', '# ', '# ## 4-1. Polynomial Features', '# In[31]:', '#        ', '#  ', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# Need to impute missing values', '# Create the polynomial object with specified degree', 'poly_transformer = PolynomialFeatures(degree = 3) #  degree  \xad ', '# In[32]:', '# Train the polynomial features', '# Transform the features', '# In[33]:', '# In[34]:', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# \xad        TARGET  ', '#     .  Train, Test  ', '#            . ', '# ', '# \xad     ,    \xa0 merge .', '#  app_train    SK_ID_CURR \xa0  poly_feature \xa0', '#  . \xa0  \xa0\xa0.', '# ', '# In[35]:', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# ## 4-2. Domain Knowledge Features', '# ', '#     \xa0     ', '#   feature    .', '# ', '#  - CREDIT_INCOME_PERCENT ', '#  \xa0   \xa0  ', '#  - ANNUILY_INCOME_PERCENT ', '#  \xa0     ', '#  - CREDIT_TERM ', '#   ', '#  - DAYS_EMPLOYEM_PERCENT ', '#  \xa0 \xa0  \xa0  ', '# In[36]:', '# In[37]:', '# \xad   TARGET   ', '# In[38]:', '# iterate through the new features', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# \xad  feature  \xa0     .', '# ## 5.  ', '# ', '# ###  \xa0', '# In[39]:', '# Drop the target from the training data', '# Feature names', '# Copy of the testing data', '#   ', '#  0~1 \xa0', '# Fit on the training data', '# Transform both training and testing data', '# Repeat with the scaler', '# In[40]:', '# Make the random forest classifier', '# In[41]:', '# Train on the training data', '# Extract feature importances', '# Make predictions on the test data', '# In[42]:', '# \xa0  \xa0', '# In[43]:', '# Make a submission dataframe', '# Save the submission dataframe', '# In[ ]:', '#  \xa0     ', '# feature_importances_ ', '# In[44]:', '# In[45]:', '# In[ ]:']",374
lgbm-credit-default-prediction.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# One of the main problems that I faced is the large amount of data that needs to be handled, so I process the data one portion at a time in order to always keep under control the memory usage, applying similar transformations multiple times.', '# Application data', '# For all the csv files I consider XNA and XAP as nan (along with the default nan).', '# In[6]:', '# Cleaning the application data', '# In[7]:', '# In[8]:', '# In[9]:', '# I drop a few rows in the train data where there are less than .01% missing values in columns where the test data has got no missing values and then I join the two datasets adding one column IS_TRAIN to identify where each record belongs.', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# Analysis of columns with more than 60% of missing values', '# In[17]:', '# In[18]:', '# OWN_CAR_AGE can be handled with FLAG_OWN_CAR when using the tree based gradient boosting.', '# EXT_SOURCE_1 will be imputed when the external bureau data is added.', '# The rest is data relative to the housing and it is mostly missing, so I drop all those columns.', '# In[19]:', '# In[20]:', '# In the final submission I decided to also leave the housing features which slightly improve the score.', '# In[21]:', '# Label encoding for binary categorical features', '# In[22]:', '# Next I handle the three categorical features with missing values: ORGANIZATION_TYPE, NAME_TYPE_SUITE, OCCUPATION_TYPE. The approach that I prefer to follow is to create a new categorical value for all of them called Nan in order to avoid messing up the existing data, which will be handled by the get_dummies function used for the one hot encoding. I will use the same approch for all the following data.', '# One hot encoding of all the other categorical features', '# In[23]:', '# I check the correlation of the features with the target to see if I can drop the remaining columns with missing values', '# In[24]:', '# In[25]:', '# In[26]:', '# In[27]:', '# In[28]:', '# In[29]:', '# EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 are the best features, which are scores obtained from external sources. I tried to impute them using the rest of the data, but I only end up lowering the correlation with the target feature so I prefer to let them as they are since the the lgbm algorithm can handle missing data.', '# The 6 AMT_REQ_CREDIT_BUREAU features are missing because these clients are not present in the bureau credit data. So I add a new feature IS_IN_BUREAU.', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# As for the CNT_SOCIAL_CIRCLE features, not knowing how this data was gathered, I decided to add a feature HAS_SOCIAL_CIRCLE.', '# In[34]:', '# In[35]:', '# In[36]:', '# Hand crafted features. The few application data features that actually regard the entity of the loan are only slightly correlated to the target. So I tried to build a few new features', '# In[37]:', '# In[38]:', '# In[39]:', '# In[40]:', '# In[41]:', '# In[42]:', '# Only AMT_GOODS_FRAC is actually a very good feature. I kept all the others leaving the machine learning algorithm to decide how to use them.', '# Bureau Balance data', '# In[43]:', '# I drop the data of loans that are not related to clients in the application data.', '# In[44]:', '# In[45]:', '# One-hot encoding', '# In[46]:', '# Aggregation', '# In[47]:', '# In[48]:', '# In[49]:', '# In[50]:', '# In[51]:', '# In[52]:', '# In[53]:', '# Bureau data', '# In[54]:', '# In[55]:', '# In[56]:', '# In[57]:', '# In[58]:', '# In[59]:', '# In[60]:', '# In[61]:', '# In[62]:', '# In[63]:', '# In[64]:', '# In[65]:', '# In[66]:', '# Previous applications', '# In[67]:', '# In[68]:', '# In[69]:', '# In[70]:', '# In[71]:', '# In[72]:', '# In[73]:', ""prev_application = prev_application.groupby('SK_ID_CURR').agg(['max', 'sum', 'mean']) # last"", '# In[74]:', '# In[75]:', '# In[76]:', '# In[77]:', '# In[78]:', '# In[79]:', '# POS Cash Balance', '# In[80]:', '# In[81]:', '# In[82]:', '# In[83]:', '# In[84]:', ""pos_cash_balance = pos_cash_balance.groupby(['SK_ID_PREV', 'SK_ID_CURR']).agg(['sum', 'mean', 'max']) # last"", '# In[85]:', '# In[86]:', '# In[87]:', '# In[88]:', '# In[89]:', '# In[90]:', '# In[91]:', '# In[92]:', '# In[93]:', '# In[94]:', '# In[95]:', '# In[96]:', '# Credit Card Balance', '# In[97]:', '# In[98]:', '# In[99]:', '# In[100]:', '# In[101]:', ""credit_card_balance = credit_card_balance.groupby(['SK_ID_PREV', 'SK_ID_CURR']).agg(['sum', 'mean', 'max']) # last"", '# In[102]:', '# In[103]:', '# In[104]:', '# In[105]:', '# In[106]:', '# In[107]:', '# In[108]:', '# In[109]:', '# In[110]:', '# In[111]:', '# Installment payments', '# In[112]:', '# In[113]:', '# In[114]:', '# In[115]:', '# In[116]:', '# In[117]:', '# In[118]:', '# In[119]:', '# In[120]:', '# In[121]:', '# In[122]:', '# In[123]:', '# In[124]:', '# In[125]:', '# In[126]:', '# In[127]:', '# In[128]:', '# Feature selection', '# In order to reduce the number of features before starting the training and evaluation, I use the lgbm algorithm to select the most important features based on the number of times the feature is used in a model. ', '# In[129]:', '# In[130]:', '# In[131]:', '# In[132]:', '# In[133]:', '# In[134]:', '# In[135]:', '# In[136]:', '# In[137]:', '# In[138]:', '# In[139]:', '# In[140]:', '# In[141]:', '# In[142]:', '# In[143]:', '# In[144]:', '# In[145]:', '# Hyperparameter Tuning', '# The approach that I follow for the hyperparameter tuning is the random search on the interval around the main default parameters of the lgbt classifier.', '# In[146]:', '# In[147]:', '# In[148]:', '# remove the comment to do hyperparameter tuning', '# In[149]:', '# Training and evaluation', '# In[150]:', '# In[151]:', '# In[152]:', '# Submission', '# In[153]:', '# In[154]:', '# References', '# ', '# 1. Guolin Ke Qi Meng Thomas Finely Taifeng Wang Wei Chen Weidong Ma Qiwei Ye Tie-Yan Liu. 2017. [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)']",196
lgbm-with-bayesian-optimization.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', ' #  n_estimators=1327, n_jobs=-1, num_leaves=106, objective=None,random_state=None, reg_alpha=0.5129992714397862, reg_lambda=0.38268769901820565, silent=True, subsample=0.7177561548329953, subsample_for_bin=80000,', '  #     subsample_freq=0, verbose=1)', '# In[7]:', '# In[8]:', '# Save the submission dataframe']",12
lighgbm_with_selected_features.py,"['# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features', '# From Kaggler : https://www.kaggle.com/jsaguiar', '# Just added a few features so I thought I had to make release it as well...', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', ""            'learning_rate': 0.02,  # 02,"", ""            'min_child_weight': 60, # 39.3259775,"", '    # Write submission file and plot feature importance', '# Display/plot feature importance']",44
lighgbm_with_selected_features_impr.py,"['# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features', '# From Kaggler : https://www.kaggle.com/jsaguiar', '# Just added a few features so I thought I had to make release it as well...', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', ""            'learning_rate': 0.02,  # 02,"", ""            'min_child_weight': 60, # 39.3259775,"", '    # Write submission file and plot feature importance', '# Display/plot feature importance']",44
light-gbm-easy-peasy.py,"['# coding: utf-8', '# # Home Credit Solution ', '# ## This Kernal is made for beginners learning purpose.', '# ### Feel free to fork and use it. ', '# ### You will learn how to handle large dataset without being confused.', '# ![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkr6pXKnRSPy5tYRwUBMr0nZwjAzDVAzVFAigd6bauuwvHoQTf)', '# ', '# # 1. Introduction', '# In this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan.', '# # 2. Importing requires packages', '# In[ ]:', '# numpy and pandas for data manipulation', '# sklearn preprocessing for dealing with categorical variables', '# File system manangement', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# # 3. Retrieving the Data', '# In[ ]:', '# List files available', '# # 4. Exploration of Application Train/Test Data.', '# In[ ]:', '# Training data', '# ## 4.1 Merge both train and test dataset', '# In[ ]:', '# We can see there are some anamolies in  `DAYS_EMPLOYED`.', '# ', '# `DAYS_EMPLOYED` at max seems to have very large positive value.', '# ', '# ## 4.2 Removing Anamolies / Outlier', '# In statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses.', '# ', '# ', '# ', '# In[ ]:', '# 365243 days is somewhat around 1000 years, which is impossible.', '# ', '# So, its best for us to replace it by NaN.', '# In[ ]:', '# NaN values for DAYS_EMPLOYED: 365.243 -> nan', '# ## 4.3 Categorical Variables ', '# In[ ]:', '# Number of unique classes in each object column', ""# `CODE_GENDER` has unknown value 'XNA'. Its better to remove these 4 rows."", '# In[ ]:', '# Remove the rows with XNA value in CODE_GENDER', '# ## 4.4 Label Encoder and One Hot Encoding', '# ', '# **Label Encoder **: As you might know by now, we cant have text in our data if were going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model.', '# ', '# And to convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.', '# ', '# Suppose, we have a feature State which has 3 category i.e India , France, China . So, Label Encoder will categorize them as 0, 1, 2.', '# ', '# **One Hot Encoding** : One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. ', '# ', '# If we applied OHE on say Gender column which has category male,female. OHE will create two new column Gender_male, Gender_female and store the value 0 and 1 according to the main category value.', '# In[ ]:', '# Create a label encoder object', '# Iterate through the columns', '        # If 2 or fewer unique categories', '# One Hot Encoding ', '# ## 4.5 Add some feature variables', ""# Since i don't consider myself as a credit expert. I have used this features from [this](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features/code#L261) script by Aguiar."", '# ', ""# - DAYS_EMPLOYED_PERC: the percentage of the days employed relative to the client's age."", ""# - INCOME_CREDIT_PERC: the percentage of the credit amount relative to a client's income."", '# - INCOME_PER_PERSON : the percentage of income per person.', ""# - ANNUITY_INCOME_PERC: the percentage of the loan annuity relative to a client's income."", '# - PAYMENT_RATE : the percentage of rate of payment annually.', '# In[ ]:', '# ## Target Column Distribution ', '# The target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category.', '# In[ ]:', '# In[ ]:', '# There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance.', '# ## Garbage Collection', '# Pythons memory allocation and deallocation method is automatic. The user does not have to preallocate or deallocate memory similar to using dynamic memory allocation in languages such as C or C++.', '# ', '# Here we are using **Manual Garbage Collection**', '# ', '# Invoking the garbage collector manually during the execution of a program can be a good idea on how to handle memory being consumed by reference cycles.', '# The garbage collection can be invoked manually in the following way:', '# In[ ]:', '# If you wish not to use GC then better watch your RAM. It is likely to over-exceed and Kernel error may come up.', '# ## Handling Categorical Features', '# In[ ]:', '# function to obtain Categorical Features', '# function to factorize categorical features', '# function to create dummy variables of categorical features', '# In[ ]:', '# # factorize the categorical features from train and test data', '# df_cats = _get_categorical_features(df)', '# df = _factorize_categoricals(df, df_cats)', '# # 5. Exploration of Bureau and Bureau_data', '# In[ ]:', '# ## 5.1 One Hot Encoding', '# In[ ]:', '# ## 5.2 Feature Engineering - Bureau Data', '# In[ ]:', '# Average Values for all bureau features ', '# # 6. Exploration of Previous Application', '# In[ ]:', '# ## 6.1 Handling Outliers', '# In[ ]:', '# ## 6.2 One Hot Encoding', '# In[ ]:', '# ## 6.3 Feature Engineering - Previous Application', '# In[ ]:', '# # 7. Exploration of POS Cash Balance', '# In[ ]:', '# ## 7.1 One Hot Encoding', '# In[ ]:', '# ## 7.2 Feature Engineering - POS Cash Balance', '# In[ ]:', '# # 8. Exploration of Installment Payments', '# In[ ]:', '# ## 8.1 One Hot Encoding', '# In[ ]:', '# ## 8.2 Adding some new features', '# In[ ]:', '# Percentage and difference paid in each installment (amount paid and installment value)', '# Days past due and days before due (no negative values)', '# ## 8.3 Feature Engineering - Installment Payments', '# In[ ]:', '# # 9. Exploration of Credit Card', '# In[ ]:', '# ## 9.1 One Hot Encoding', '# In[ ]:', '# ## 9.2 Feature Engineering - Credit Card', '# In[ ]:', '# # 10. LightGBM', '# ', '# - It is a gradient boosting framework that uses **tree based learning algorithm**.', '# - It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise.', '# -  It is prefixed as Light because of its** high speed**.', '# - It can handle the **large size of data** and takes **lower memory to run.**', '# - This is popular because it focuses on **accuracy of results.**', '# -  LGBM also supports **GPU learning.**', '# ', '# **Why not to use Light GBM?**', '# ', '# - It is **not advisable** to use LGBM on **small datasets**.', '# - Light GBM is sensitive to overfitting and can easily **overfit small data**.', '# ', '# The only complicated thing is **parameter tuning.** Light GBM covers more than 100 parameters but dont worry, you dont need to learn all.', '# ', '# Lets learn about some of the parameters we used in our model :', '# ', '# - **n_estimators** : number of boosting iterations.', '# - **objective** : This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model.', '# ', '#     - regression: for regression', '#     - binary: for binary classification', '#     - multiclass: for multiclass classification problem', '#    ', '# - **learning_rate** : This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003', '# - **reg_alpha** : L1 regularization', '# -  **reg_lambda** : L2 regularization', '# ', '# In[ ]:', '# ## 10.1 Prepare Final Train and Test data', '# In[ ]:', '# Divide in training/validation and test data', '# ## 10.2 Cross Validation Model', '# **K-Fold Cross Validation**: To reduce variability, in most machine learning methods multiple rounds of cross-validation are performed using different partitions, and the validation are averaged at the end. This method is known as k-fold cross validation.', '# ', '# Increase the `n_splits` to make better prediction. But it may increase the time of processing.', '# In[ ]:', '# In[ ]:', '# Create arrays and dataframes to store results', '# ## 10.3 Fitting the model and Predicting', '# In[ ]:', '# Iterate through each fold', '    # Training data for the fold', '    # Validation data for the fold', '    # Create the model', '    # Train the model', '# ## 10.5 Submission', '# In[ ]:', '# **Thanks for taking time to go through the kernel. **', '# ', '# **Show your appreciation by upvoting or commenting about your feedbacks. **']",182
lightgbm-automated-feature-engineering-easy.py,"['# coding: utf-8', '# **Want to find out how to do the feature enigeering step automatically in a concise and compact tutorial, applying it on a binary classification problem using lightGBM?---look no further **', '# ', '# After finding out that data scientists created a tool that ""replaces"" data-scientists I had to try it out. Thank you [https://docs.featuretools.com/](http://)! Feature-engineering is tiresome, and takes the biggest amount of time do it. What if we can make it a one liner. Well now it seems we can. Even more appropriately we will be working on Home Credit Default Risk. A set of datasets where all of them are in a relationship with one-another and from all of them some information should be extracted. Featuretools makes it easy! Our goal in the end is simple. Predict whether the customer will default or not.', '# In[ ]:', '# Load in the data, NOTE: datasets are huge, working on them will be computationally costly. In order to avoid it we can introduce some limited sample size.', '# In[ ]:', '# Please note that you could have read it with simple read_csv, without using os (operating system commands...)', '# If we merge datasets now we can perofrm neccesary operations and seperate them later.', '# In[ ]:', '# Merge the datasets into a single one for training', '# **NOTE** This NaN handling is just for the sake of it. It is by no-means complete and there are lot of them underneath (function is built that shows us percentage). But there is a specific way that GBM (light and xBGM) handle missing values. So even tough it would be better we want to focus on algortihm and automatic feature engineering!', '# In[ ]:', '# A lot of the continuous days variables have integers as missing value indicators.', '# **NOTE** Even tough it is automatic, we can incorporate some manual features. IF we know some domain specific information.', '# In[ ]:', '# Amount loaned relative to salary', '# Number of overall payments (I think!)', '# Social features', '# ***Featuretools*** is an open-source Python library for automatically creating features out of a set of related tables using a technique called deep feature synthesis. Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.', '# ', '# There are a few concepts that we will cover along the way:', '# ', '# 1.  Entities and EntitySets', '# 2. Relationships between tables', '# 3. Feature primitives: aggregations and transformations', '# 4. Deep feature synthesis', '# In[ ]:', '# Create new entityset', '# Create an entity from the applications (app_both) dataframe', '# This dataframe already has an index', '# Create an entity from the bureau dataframe', '# This dataframe already has an index', '# Create an entity from the bureau balance dataframe', '# Create an entity from the installments dataframe', '# Create an entity from the previous applications dataframe', '# Create an entity from the credit card balance dataframe', '# Create an entity from the POS Cash balance dataframe', '# **2. Relationships betweeen the sets**', '# In[ ]:', '# Relationship between applications and credits bureau', '# Relationship between applications and credits bureau', '# Relationship between applications and credits bureau', '# Relationship between applications and previous applications', '# Relationship between applications and credit card balance', '# Relationship between applications and POS cash balance', '# **Feature primitives** Basically which functions are we going to use to create features. Since we did not specify it we will be using standard ones (check doc) There is a option to define own ones or to just select some of the standards.', '# In[ ]:', '# Create new features using specified primitives', '# In[ ]:', '# **Label encoding** Making it machine readable', '# In[ ]:', '    # Label encode categoricals', '# In[ ]:', '# **NaN imputation** will be skipped in this tutorial.', '# In[ ]:', '# Let us split the variables one more time.', '# In[ ]:', '# Separate into train and test', '# **Train** the model, predict, etc.', '# In[ ]:', '# In[ ]:', '# In[ ]:']",63
lightgbm-predictions-explained-with-shap-0-796.py,"['# coding: utf-8', '# This kernel shows how one can try to explain the predictions of a given boosted tree model using the lib SHAP https://github.com/slundberg/shap', '# ', '# The model is then retrained only with the best features to avoid fitting to noise and improve our LB score ;)', '# ', '# It is based on the best performing public script and some other variants: i.e. https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features et al.', '# In[ ]:', '# In[ ]:', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[ ]:', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance', '# Display/plot shapley values', '# In[ ]:', '# In[ ]:', '# ## Using SHAP (SHapley Additive exPlanations)', '# In[ ]:', ""# explain the model's predictions using SHAP values"", '# (same syntax works for LightGBM, CatBoost, and scikit-learn models)', '# In[ ]:', '# load JS visualization code to notebook', ""# visualize the first prediction's explanation (in log-odds scale)"", '# In[ ]:', '# Reconstructing the lgb output from shap_values:', '# In[ ]:', '# visualize the training set predictions', '# In[ ]:', '# summarize the effects of top features', '# ## Select Best Features', '# To avoid fitting to noise and improve our LB score ;)', '# In[ ]:']",66
lightgbm.py,"[""        #    df[col] = df[col].astype('category')""]",1
lightgbm_7th_place_solution (1).py,"['    # Add ratios and groupby between different tables', '    # CREDIT TO INCOME RATIO', '    # PREVIOUS TO CURRENT CREDIT RATIO', '    # PREVIOUS TO CURRENT ANNUITY RATIO', '    # PREVIOUS TO CURRENT CREDIT TO ANNUITY RATIO', '    # DAYS DIFFERENCES AND RATIOS', '# ------------------------- LIGHTGBM MODEL -------------------------', '    # Hold oof predictions, test predictions, feature importance and training/valid auc', '        # Feature importance by GAIN and SPLIT', '    # Get the average feature importance between folds', '    # Save feature importance, test predictions and oof predictions as csv', '        # Generate oof csv', '        # Save submission (test data) and feature importance', '# ------------------------- APPLICATION PIPELINE -------------------------', '    # Data cleaning', ""    df = df[df['CODE_GENDER'] != 'XNA']  # 4 people with XNA code gender"", ""    df = df[df['AMT_INCOME_TOTAL'] < 20000000]  # Max income in test is 4M; train has a 117M value"", '    # Flag_document features - count and kurtosis', '    # Categorical age - based on target=1 plot', '    # New features based on External sources', '    # Credit ratios', '    # Income ratios', '    # Time ratios', '    # Groupby: Statistics for applications in the same group', '    # Encode categorical features (LabelEncoder)', '    # Drop most flag document columns', '# ------------------------- BUREAU PIPELINE -------------------------', '    # Credit duration and credit/account end date difference', '    # Credit to debt ratio and difference', '    # One-hot encoder', '    # Join bureau balance features', '    # Flag months with late payments (days past due)', '    # Aggregate by number of months in balance and merge with bureau (loan length agg)', '    # General loans aggregations', '    # Active and closed loans aggregations', '    # Aggregations for the main loan types', '    # Time based aggregations: last x months', '    # Last loan max overdue', '    # Ratios: total debt/total credit and active loans debt/ active loans credit', '    # Calculate rate for each category with decay', '    # Min, Max, Count and mean duration of payments (months)', '# ------------------------- PREVIOUS PIPELINE -------------------------', '    # One-hot encode most important categorical features', '    # Feature engineering: ratios and difference', '    # Interest ratio on previous application (simplified)', '    # Active loans - approved and not complete yet (last_due 365243)', '    # Find how much was already payed in active loans (using installments csv)', '    # Active loans: difference of what was payed and installments', '    # Merge with active_df', '    # Perform aggregations for active applications', '    # Change 365.243 values to nan (missing)', '    # Days last due difference (scheduled x done)', '    # Categorical features', '    # Perform general aggregations', '    # Merge active loans dataframe on agg_prev', '    # Aggregations for approved and refused loans', '    # Aggregations for Consumer loans and Cash loans', '    # Get the SK_ID_PREV for loans with late payments (days past due)', '    # Aggregations for loans with late payments', '    # Aggregations for loans in the last x months', '# ------------------------- POS-CASH PIPELINE -------------------------', '    # Flag months with late payment', '    # Aggregate by SK_ID_CURR', '    # Sort and group by SK_ID_PREV', '    # Percentage of previous loans completed and completed before initial term', '    # Number of remaining installments (future installments) and percentage from total', '    # Group by SK_ID_CURR and merge', '    # Percentage of late payments for the 3 most recent applications', '    # Last month of each application', '    # Most recent applications (last 3)', '    # Drop some useless categorical features', '# ------------------------- INSTALLMENTS PIPELINE -------------------------', '    # Group payments and get Payment difference', '    # Payment Entry: Days past due and Days before due', '    # Flag late payment', '    # Percentage of payments that were late', '    # Flag late payments that have a significant amount', '    # Flag k threshold late payments', '    # Aggregations by SK_ID_CURR', '    # Installments in the last x months', '    # Last x periods trend features', '    # Last loan features', '# ------------------------- CREDIT CARD PIPELINE -------------------------', '    # Amount used from limit', '    # Current payment / Min payment', '    # Late payment', '    # How much drawing of limit', '    # Aggregations by SK_ID_CURR', '    # Last month balance of each credit card application', '    # Aggregations for last x months', '# ------------------------- UTILITY FUNCTIONS -------------------------', '                # Can use unsigned int here too', '# ------------------------- CONFIGURATIONS -------------------------', '# GENERAL CONFIGURATIONS', '# INSTALLMENTS TREND PERIODS', '# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS', '# AGGREGATIONS', '    # Categorical', '    # Group by loan duration features (months)', '    # Engineered features', '    # Engineered features', '    # Engineered features', '    # The following features are only for approved applications', '    # Engineered features', '    # Engineered features', '    # Engineered features']",106
lightgbm_harder_better_slower.py,"['# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage', '    # # Add new features', '    # Amount loaned relative to salary', '    # A lot of the continuous days variables have integers as missing value indicators.', '    # Number of overall payments (I think!)', '    # Social features', '    # Previous loan amounts & installments', '    # Misc', '    # # Aggregate and merge supplementary datasets', '    # Previous applications', '    # Average the rest of the previous app data', '    # Previous app categorical features', '    # Credit card data - numerical features', '    # Credit card data - categorical features', '    # Credit bureau data - numerical features', '    # Credit bureau categorical features', '    # Bureau balance data', '    # Pos cash data - weight values by recency when averaging', '    # Unweighted aggregations of numeric features', '    # Pos cash data data - categorical features', '    # Installments data', '    # Add more value counts', '    # # Further engineering of combined features', '    # Label encode categoricals', '# Merge the datasets into a single one for training', '# Separate metadata', '# Process the data set.', '# Capture other categorical features not as object data types:', '# Re-separate into train and test', '# Some parameters taken from https://www.kaggle.com/ogrellier/fork-lightgbm-with-simple-features/code                       ', '# Plot importances']",31
lightgbm_try.py,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '    # make sure that the number of features in train and test should be same', ""#                              application_train_ohe['TARGET'])"", '# You can use the full sample and do sample weighting in lightgbm using `is_unbalance` OR `scale_pos_weight` argument', '# But it makes the code to run 8x..10x slower, which is ok for the run with pre-optimised parametersm but is too slow for HP optimisation']",8
lightgbm_with_simple_features.py,"['# HOME CREDIT DEFAULT RISK COMPETITION', '# Most features are created by applying min, max, mean, sum and var functions to grouped tables. ', '# Little feature selection is done and overfitting might be a problem since many features are related.', '# The following key ideas were used:', '# - Divide or subtract important features to get rates (like annuity and income)', '# - In Bureau Data: create specific features for Active credits and Closed credits', '# - In Previous Applications: create specific features for Approved and Refused applications', '# - Modularity: one function for each table (except bureau_balance and application_test)', '# - One-hot encoding for categorical features', '# All tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).', '# You can use LightGBM with KFold or Stratified KFold.', '# Update 16/06/2018:', '# - Added Payment Rate feature', '# - Removed index from features', '# - Use standard KFold CV (not stratified)', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Some simple new features (percentages)', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance']",55
lightgbm_with_weighted_averages_dropout_787.py,"['    # # Add new features', '    # Amount loaned relative to salary', '    # Number of overall payments (I think!)', '    # Social features', '    # A lot of the continuous days variables have integers as missing value indicators.', '    # # Aggregate and merge supplementary datasets', '    # Previous applications', '    # Average the rest of the previous app data', '    # Previous app categorical features', '    # Credit card data - numerical features', '    # Credit card data - categorical features', '    # Credit bureau data - numerical features', '    # Bureau balance data', '    # Pos cash data - weight values by recency when averaging', '    # Unweighted aggregations of numeric features', '    # Pos cash data data - categorical features', '    # Installments data', '    # Add more value counts', '    # Label encode categoricals', '# Merge the datasets into a single one for training', '# Separate metadata', '# Process the data set.', '# Capture other categorical features not as object data types:', '# Re-separate into train and test']",24
ligthgbm-0-781-home-credit-default-risk.py,"['# coding: utf-8', '# # Home Credit Default Risk - Competition', '# ## LigthGBM [0.781]', '# # 1. Load Data and Modules', '# In[1]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# # Preparation for modeling', '# In[ ]:', '# In[ ]:', '        # n_estimators=1000,', '        # num_leaves=20,', '        # colsample_bytree=.8,', '        # subsample=.8,', '        # max_depth=7,', '        # reg_alpha=.1,', '        # reg_lambda=.1,', '        # min_split_gain=.01', '# In[ ]:', '# In[ ]:', '# Plot feature importances', '# In[ ]:', '# Plot ROC curves', '    # Plot the roc curve', '# In[ ]:', '# Plot ROC curves']",43
machinelearningflow.py,"['# coding: utf-8', '# **[Problem 1] Cross Validation**', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# loading the csv of the dataset', '# cleaning the dataset by filling the empy data(null)', '# get only existing data with no missing values', '# separating them into variables', '# In[3]:', '# ', '# **[Problem 2] Grid search**', '# In[4]:', '# checking which model and params are best', '# defining an array to store the scores', '# **[Problem 3] Survey from Kaggle Notebooks**', '# Model Hyperparameter Optimization is the  points of choice or configuration that allow a machine learning model to be customized for a specific task or dataset.', '# ', '# Parameters are different from hyperparameters. Parameters are learned automatically; hyperparameters are set manually to help guide the learning process.', '# ', '# the Gradient Boosting Algorithm for Machine Learning The origin of boosting from learning theory and AdaBoost.', '# How gradient boosting works including the loss function, weak learners and the additive model.', '# How to improve performance over the base algorithm with various regularization schemes.', '# Modal explainability ', '# Early Stopping to Avoid Overtraining Neural Networks A compromise is to train on the training dataset but to stop training at the point when performance on a validation dataset starts to degrade. This simple, effective, and widely used approach to training neural networks is called early stopping.', '# **[Problem 4] Creating a model with high generalization performance**', '# In[5]:', '# creating an instance of the model', '# save the default params', '# number of folds', '# creating a dataset', '# Cross validation results when avoid overfitting', '# displaying the results', '# imported the whole dataset', '# created a subset of only numbers', '# split the data using kfold', '# used gridsearchCV to find the best model and params to fine tune my classiffiers', '# **[Problem 5] Final model selection**', '# In[6]:', '# loading the csv of the test dataset', '# cleaning the dataset by removing the empy data(null)', '# separating them into variables', '# standardizing the data', '# predicting', '# In[7]:', '# In[8]:', '# In[ ]:']",55
magic_of_weighted_average_rank_0_80.py,"['# USING WEIGHTED AVERAGE RANK METHOD', '# Plese refer this discussion for more detials - https://www.kaggle.com/c/home-credit-default-risk/discussion/60934']",2
micro-model-174-features-0-8-auc-on-home-credit (1).py,"['# coding: utf-8', '# ## a Micro Model Study on Home Credit', '# ', '# The Home Credit Default Risk dataset on the Kaggle is subjected as a final project of my DS/ML bootcamp, and I have spent a period of three weeks on this project. I developed various models and quite a large number of them having AUC scores better than 0.8 ( highest one +0.804). Unfortunately, I could not run any full version of my models on Kaggle because of insufficient RAM issue even though datasets are zipped to almost 4 times by integer/float dtype conversion on my datasets. In addition, I made a bleend boosting study to acheive highest AUC score (0.81128, much highers possible) on Kaggle (https://www.kaggle.com/hikmetsezen/blend-boosting-for-home-credit-default-risk).', '# ', '# Here I would like to share my micro model study with you. This micro model has only 174 features and is able to reach better than 0.8 AUC score. Micro model is developed on my base model via successive feature elimination and addition procedure, which is developed by myself. My ambition is that tremendously increasing number of feature is not always necessary to improve performance of model! ', '# ', '# Mostly I use Colab Pro to compute LigthGBM calculations with 5-fold CV on GPUs. My models have 900-1800 features. ', '# ', '# I have a limited knowledge about the credit finance, therefore, I combined many Kaggle notebooks for expending number of features as much as I desire and/or acceptance of my LigthGBM models harvesting further enhance scores. I would like to thank these contributors. Some of them are listed here:', '# * https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features <=-- my models are based on this study', '# * https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution', '# * https://www.kaggle.com/sangseoseo/oof-all-home-credit-default-risk <=-- in most cases these hyperparameters are used', '# * https://www.kaggle.com/ashishpatel26/different-basic-blends-possible <=-- thank for blending idea', '# * https://www.kaggle.com/mathchi/home-credit-risk-with-detailed-feature-engineering', '# * https://www.kaggle.com/windofdl/kernelf68f763785', '# * https://www.kaggle.com/meraxes10/lgbm-credit-default-prediction', '# * https://www.kaggle.com/luudactam/hc-v500', '# * https://www.kaggle.com/aantonova/aggregating-all-tables-in-one-dataset', '# * https://www.kaggle.com/wanakon/kernel24647bb75c', '# In[1]:', '# !pip install lightgbm==2.3.1', '# import lightgbm', '# lightgbm.__version__', '# In[2]:', '# load libraries', '# In[3]:', '# run functions and pre_settings', '# In[4]:', '    # general cleaning procedures', ""    df = df[df['AMT_INCOME_TOTAL'] < 20000000] # remove a outlier 117M"", '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', ""    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True) # set null value"", ""    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True) # set null value"", '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '    # Flag_document features - count and kurtosis', '    # Categorical age - based on target=1 plot', '    # New features based on External sources', '    # Some simple new features (percentages)', '    # Credit ratios', '    # Income ratios', '    # Time ratios', '    # EXT_SOURCE_X FEATURE', '    # AMT_INCOME_TOTAL : income', '    # CNT_FAM_MEMBERS  : the number of family members', ""    # DAYS_BIRTH : Client's age in days at the time of application"", '    # DAYS_EMPLOYED : How many days before the application the person started current employment', '    # other feature from better than 0.8', '# In[5]:', '    # Credit duration and credit/account end date difference', '    # Credit to debt ratio and difference', '    # CREDIT_DAY_OVERDUE :', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# In[6]:', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Feature engineering: ratios and difference', '    # Interest ratio on previous application (simplified)', '    # Days last due difference (scheduled x done)', '    # from off', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# In[7]:', '    # Flag months with late payment', ""    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) #  0) & (x < 120) else 0)"", '    # Features', '    # Count pos cash accounts', '    # Percentage of previous loans completed and completed before initial term', '    # Number of remaining installments (future installments) and percentage from total', '    # Group by SK_ID_CURR and merge', '    # Percentage of late payments for the 3 most recent applications', '    # Last month of each application', '    # Most recent applications (last 3)', '# In[8]:', '    # Group payments and get Payment difference', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Flag late payment', '    # Flag late payments that have a significant amount', '    # Flag k threshold late payments', '    # Features: Perform aggregations', '    # Count installments accounts', '    # from oof (DAYS_ENTRY_PAYMENT)', '# In[9]:', '    # Amount used from limit', '    # Current payment / Min payment', '    # Late payment  0 else 0)', '    # How much drawing of limit', '    # General aggregations', '    # Count credit card lines', '    # Last month balance of each credit card application', '# In[10]:', '    # loading predicted result ', '    # split train, and test datasets', '    # Expand train dataset with two times of test dataset including predicted results', '    # Cross validation model', '    # Create arrays and dataframes to store results', '    # limit number of feature to only 174!!!', '    # print final shape of dataset to evaluate by LightGBM', '    # create submission file', '# In[11]:']",108
minimal-pipeline-lightgbm-lb-744-auc.py,"['# coding: utf-8', '# # Minimal pipeline with LightGBM (~.744 AUC on Public Leaderboard)', '# ', '# - Still trying to make it shorter. Please do let me know if you have any suggestions/improvements! ', '# - No EDA, no custom/manual feature engineering (aside from generic numeric/categoricals handling in pipeline)', '# - (pipeline goal is to be as generic as possible)', '# - some warnings with LabelEnconder I coudnt fix yet, checking.', '# - note: sklearn .20 has some  solutions for OneHot/Categorical Imputation, anyone knows ow to update the package Kaggle kernels?', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Split X,y', '# Split kaggle train, reserve internal hold out test set', '# In[ ]:', '# Note: sklearn .20 has now SimpleImputer works for categorical also, workaround', '# In[ ]:', '# Some workarounds for sklearn 19.1 (.20 use OneHot, SimpleImputer)', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Full fit', '# In[ ]:', '# Prepare submission', '# In[ ]:', '# Check predictions ', '# In[ ]:']",27
ml-dive-week4.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# In[3]:', '# In[4]:', '# Check missing value ratios', '# In[5]:', '# Bring high cordinality columns to numerical pipeline', '# In[6]:', '    # Bring high cordinality columns to numerical pipeline', '# missing_df is sorted by missing_ratio', '# In[7]:', '# In[8]:', '# Column preprocessing', '# Model pipeline', '# In[9]:']",25
model-stacking.py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '    # Get all the models tested so far in DataFrame format', '    # Get current parameters and the best parameters    ', '    # Save all model results', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# prev.head(10)', '# In[ ]:', '# avg_prev.head(10)', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# bayes_cv_tuner = BayesSearchCV(', '#     estimator = lgb.LGBMClassifier(', ""#         boosting='dart',"", ""#         application='binary',"", ""#         metric='auc',"", '#         drop_rate=0.2,', '#         n_jobs=1,', '#         verbose=0', '#     ),', '#     search_spaces = {', ""#         'learning_rate': (0.01, 0.3, 'uniform'),"", ""#         'num_leaves': (1, 225),      "", ""#         'max_depth': (0, 8),"", ""#         'feature_fraction':(0.5, 1.0, 'uniform'),"", '#         ""min_data_in_leaf"":(20, 100),', ""# #         'min_child_samples': (0, 50),"", ""#         'max_bin': (100, 1000),"", ""#         'reg_lambda': (1e-9, 1.0, 'log-uniform'),"", ""#         'reg_alpha': (1e-9, 1.0, 'log-uniform'),"", ""#         'scale_pos_weight': (1,12, 'uniform'),"", '# },    ', ""#     scoring = 'roc_auc',"", '#     cv = StratifiedKFold(', '#         n_splits=3,', '#         shuffle=True,', '#         random_state=42', '#     ),', '#     n_jobs = 1,', '#     n_iter = 15,   ', '#     verbose = 0,', '#     refit = True,', '#     random_state = 42', '# )', '# # Fit the model', '# result = bayes_cv_tuner.fit(data, y, callback=status_print)', '# In[ ]:', '# # model = lgb.LGBMClassifier(lgbm_params)', '# Best ROC-AUC: 0.7618', ""# Best params: {'max_bin': 783, 'max_depth': 7, 'min_child_samples': 37, 'min_child_weight': 7, 'n_estimators': 94, 'num_leaves': 92, 'reg_alpha': 0.6654390259962506, 'reg_lambda': 8.076151891962533e-06, 'scale_pos_weight': 7.642490251593845, 'subsample': 0.25371759984574854, 'subsample_freq': 9}"", '# In[ ]:', '# Model #10', '# Best ROC-AUC: 0.7711', ""# Best params: {'learning_rate': 0.685534641629431, 'max_bin': 112, 'max_depth': 38, 'min_child_samples': 42, 'min_child_weight': 3, 'n_estimators': 60, 'num_leaves': 25, 'reg_alpha': 1.462442068214992e-06, 'reg_lambda': 3.5571385509488406e-07, 'scale_pos_weight': 0.0052366805641386495, 'subsample': 0.7074795557274224, 'subsample_freq': 10}"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '#                           min_data_in_leaf=lgbm_params[""min_data_in_leaf""],', '#                            feature_fraction=lgbm_params[""feature_fraction""],', '# In[ ]:', '# clf = lgb.train(train_set=lgbm_train,', '#                  params=lgbm_params,', '#                  num_boost_round=optimum_boost_rounds)', '# In[ ]:', '# model = lgb.LGBMClassifier(lgbm_params)', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# data = data.drop(columns = zero_features)', '# test = test.drop(columns = zero_features)', '# # ', '# In[ ]:', '        # Record the best score', '# In[ ]:', '# In[ ]:', '# out_df = pd.DataFrame({""SK_ID_CURR"":test[""SK_ID_CURR""], ""TARGET"":y_pred})', '# out_df.to_csv(""submissions.csv"", index=False)', '# In[ ]:']",127
model-tuning-results-random-vs-bayesian-opt.py,"['# coding: utf-8', '# # Introduction: Random vs Bayesian Optimization Model Tuning', '# ', '# In this notebook, we will compare random search and Bayesian optimization hyperparameter tuning methods implemented in two previous notebooks.', '# ', '# * [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)', '# * [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)', '# ', '# In those notebooks we saw results of the methods applied to a limited dataset (10000 observations) but here we will explore results on a complete dataset with 700 + features.  The results in this notebook are from 500 iterations of random search and 400 iterations of Bayesian Optimization (these took about 5 days to run each). We will thoroughly explore the results both visually and statistically, and then implement the best hyperparameter values on a full set of features. After all the hard work in the random search and Bayesian optimization notebooks, now we get to have some fun! ', '# ', '# # Roadmap', '# ', '# Our plan of action is as follows:', '# ', '# 1. High Level Overview', '#     * Which method did best? ', '# 2. Examine distribution of scores', '#     * Are there trends over the course of the search?', '# 3. Explore hyperparameter values', '#     * Look at values over the course of the search', '#     * Identify correlations between hyperparameters and the score', '# 4. Perform ""meta"" machine learning using these results', '#     * Fit a linear regression to results and look at coefficients', '# 5. Train a model on the full set of features using the best performing values', '#     * Try best results from both random search and bayesian optimization', '# 6.  Lay out next steps', '#     * How can we use these results for this _and other_ problems? ', '#     * Are there better methods for hyperparameter optimization', '#     ', '# At each step, we will use plenty of figures and statistics to explore the data. This will be a fun notebook (even though it may not land you at the top of the leaderboard)! ', '# ', '# ## Recap ', '# ', '# In the respective notebooks, we examined we performed 1000 iterations of random search and Bayesian optimization on a reduced sample of the dataset (10000 rows). We compared the cross-validation ROC AUC on the training data, the score on a ""testing set"" (6000 observations) and the score on the real test set when submitted to the competition leaderboard. Results are below:', '# ', '# | Method                               | Cross Validation Score | Test Score (on 6000 Rows) | Submission to Leaderboard | Iterations to best score |', '# |--------------------------------------|------------------------|---------------------------|---------------------------|--------------------------|', '# | Random Search                        | 0.73110                | 0.73274                   | 0.782                     | 996                      |', '# | Bayesian Hyperparameter Optimization | 0.73448                | 0.73069                   | 0.792                     | 596                      ', '# __Take these with some skepticism because they were performed on a very small subset of the data!__ ', '# ', '# For more rigorous results, we will turn to the evaluation metrics from running __500 iterations (with random search)__ and __400+ iterations (with Bayesian Optimization)__ on a full training dataset with about 700 features (the features are from [this notebook](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https://www.kaggle.com/jsaguiar)). These iterations took around 6 days on a machine with 128 GB of RAM so they will not run in a kernel! The Bayesian Optimization method is still running and I will update the results as they finish.', '# ', '# __In this notebook  we will focus only on the results and building the best model, so for the explanations of the methods, refer to the previous notebooks! __', '# ', '# # Overall Results', '# ', ""# First, let's start with the most basic question: which model produced the highest cross validation ROC AUC score (using 5 folds) on the training dataset?"", '# In[ ]:', '# pandas and numpy for data manipulation', '# Visualizations!', '# Plot formatting and default style', '# In[ ]:', '# Read in data and sort', '# Well, there you go! __Random search slightly outperformed  Bayesian optimization and found a higher cross validation model in far fewer iterations.__ However, as we will shortly see, this does not mean random search is the better hyperparameter optimization method. ', '# ', '# When submitted to the competition (at the end of this notebook):', '# ', '# * __Random search results scored 0.790__', '# * __Bayesian optimization results scored 0.791__', '# ', '# What were the best model hyperparameters from both methods?', '# ', '# ####  Random Search best Hyperparameters', '# In[ ]:', '# #### Bayesian Optimization best Hyperparameters', '# In[ ]:', '# If we compare the individual values, we actually see that they are fairly close together when we consider the entire search grid! ', '# ', '# ## Distribution of Scores', '# ', ""# Let's plot the distribution of scores for both models in a kernel density estimate plot."", '# In[ ]:', '# Kdeplot of model scores', '# Bayesian optimization did not produce the highest individual score, but it did tend to spend more time evaluating ""better"" values of hyperparameters. __Random search got lucky and found the best values but Bayesian optimization tended to ""concentrate"" on better-scoring values__. That\'s pretty much what we expect: random search does a good job of exploring the search space which means it will probably happen upon a high-scoring set of values (if the space is not extremely high-dimensional) while Bayesian optimization will tend to focus on a set of values that yield higher scores. __If all you wanted was the conclusion, then you\'re probably good to go. If you really enjoy making plots and doing exploratory data analysis and want to gain a better understanding of how these methods work, then read on!__ In the next few sections, we will thoroughly explore these results.', '# ', '# Our plan for going through the results is as follows:', '# ', '# * Distribution of scores', '#     * Overall distribution', '#     * Score versus the iteration (did scores improve as search progressed)', '# * Distribution of hyperparameters', '#     * Overall distribution including the hyperparameter grid for a reference', '#     * Hyperparameters versus iteration to look at _evolution_ of values', '# * Hyperparameter values versus the score', '#     * Do scores improve with certain values of hyperparameters (correlations)', '#     * 3D plots looking at effects of 2 hyperparameters at a time on the score', '# * Additional Plots', '#     * Time to run each evaluation for Bayesian optimization', '#     * Correlation heatmaps of hyperparameters with score', '#     ', '# There will be all sorts of plots: heatmaps, 3D scatterplots, density plots, bar charts (hey even bar charts can be helpful!)', '# ', '# After going through the results, we will do a little meta-machine learning, and implement the best model on the full set of features.', '# # Distribution of Scores', '# ', ""# We already saw the kernel density estimate plot, so let's go on to a bar plot. First we'll get the data in a long format."", '# In[ ]:', '# In[ ]:', ""# Keep in mind that random search ran for more iterations (as of now). Even so, we can see that Bayesian Optimization tended to produce much more higher cross validation scores. Let's look at the statistical averages:"", '# In[ ]:', '# If we are going by mean, then Bayesian optimization is the clear winner. If we go by high score, then random search just wins out. ', '# ## Score versus Iteration', '# ', '# Now, to see if either method improves over the course of the search, we need to plot the score as a function of the iteration. ', '# In[ ]:', '# Again keeping in mind that Bayesian optimization has not yet finished, we can see a clear upward trend for this method and no trend whatsoever for random search. ', '# ', '# ### Linear Regression of Scores versus Iteration', '# ', '# To show that Bayesian optimization improves over time, we can regress the score by the iteration. Then, we can use this to extrapolate into the future, __a wildly inappropriate technique in this case, but fun nonetheless!__', '# ', '# Here we use `np.polyfit` with a degree of 1 for the linear regression (you can compare the results with `LinearRegression`  from `sklearn.linear_model`.', '# In[ ]:', '# The random search slope is basically zero. ', '# In[ ]:', '# In[ ]:', '# The Bayesian slope is about 15 times greater than that of random search! What happens if we say run these methods for 10,000 iterations?', '# In[ ]:', '# In[ ]:', '# Incredible! I told you this was wildly inappropriate. Nonetheless, the slope does indicate that Bayesian optimization ""learns"" the hyperparameter values that do better over time. It then concentrates on evaluating these rather than spending time exploring other values as does random search. This means it can get stuck in a local optimum and can tend to __exploit__ values rather than continue to __explore__.', '# ', '# Now we will move on to the actual values of the hyperparameters.', '# # Hyperparameter Values', '# ', '# For each hyperparameter, we will plot the values tried by both searches as well as the reference distribution (which was the same in both cases, just a grid for random and distributions for Bayesian). We would expect the random search to almost exactly match the reference - it will converge on the reference given enough iterations.', '# ', '# First, we will process the results into a dataframe where each column is one hyperparameter. Saving the file converted the dictionary into a string, so we use `ast.literal_eval` to convert back to a dictionary before adding as a row in the dataframe.', '# In[ ]:', '    # Sort with best values on top', '     # Create dataframe of hyperparameters', '    # Iterate through each set of hyperparameters that were evaluated', '    # Put the iteration and score in the hyperparameter dataframe', '# In[ ]:', '# Next we define the hyperparameter grid that was used (the same ranges applied in both searches).', '# In[ ]:', '# Hyperparameter grid', '# # Distributions of Search Values', '# ', '# Below are the kernel density estimate plots for each hyperparameter. The dashed vertical lines indicate the ""optimal"" value found in the respective searches. ', '# ', '# We start with the learning rate:', '# In[ ]:', '# In[ ]:', '# Density plots of the learning rate distributions ', '# Even though the search domain extended from 0.005 to 0.2, both optimal values clustered around a lower value. Perhaps this tells us we should concentrate further searches in this area below 0.02?', '# ', ""# That code was a little tedious, so let's write a function that makes the same code for any hyperparameter (feel free to pick your own colors!)."", '# In[ ]:', '    # Density plots of the learning rate distributions ', '# In[ ]:', '# We can do this for all of the hyperparameters. These results can be used to inform further searches. They can even be used to define a grid search over a concentrated region. The problem with grid search is the insane compuational and time costs involved, and a smaller hyperparameter grid will help immensely! ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# The `reg_alpha` and `reg_lambda` best scores seem to complement one another for Bayesian optimization. In other words, if either `reg_lambda` or `reg_alpha` is high (say greater than 0.5), then the other should be low (below 0.5). These hyperparameters control a penalty placed on the weights of the trees and thus are meant to control overfitting. It might make sense if only one needs to be high then.', '# In[ ]:', '# In[ ]:', '# ### Boosting Type', '# ', ""# The boosting type deserves its own section because it is a categorical variable, and because as we will see, it has an outsized effect on model performance. First, let's calculate statistics grouped by boosting type for each search method."", '# In[ ]:', '# In[ ]:', ""# In both search methods, the `gbdt` (gradient boosted decision tree) and `dart` (dropout meets additive regression tree) do much better than `goss` (gradient based one-sided sampling). `gbdt` does the best on average (and for the max), so it might make sense to use that method in the future! Let's view the results as a barchart:"", '# In[ ]:', '# __`gbdt` (or `dart`) it should be! Notice that random search tried `gbdt` about the same number of times as the other two (since it selected with no reasoning) while Bayesian optimization tried `gbdt` much more often. __', '# ', ""# Since `gbdt` supports `subsample` (using on a sample of the observations to train on in every tree) we can plot the distribution of `subsample` where `boosting_type=='gbdt'`. We also show the reference distribution."", '# In[ ]:', '# Density plots of the learning rate distributions ', '# There is a significant disagreement between the two methods on the optimal value for `subsample`. Perhaps we would want to leave this as a wide distribution in any further searches (although some subsampling does look to be beneficial).', '# Finally, we can look at the instance of `is_unbalance`, a hyperparameter that tells LightGBM whether or not to treat the problem as unbalance classification.', '# In[ ]:', '# In[ ]:', ""# __According to the average score, it pretty much does not matter if this hyperparameter is `True` or `False`.__ To be honest, I'm not sure what difference this is supposed to make, so anyone who wants can fill me in!"", '# # Hyperparameters versus Iteration', '# ', '# Next we will take a look at the __evolution__ of the Bayesian search (random search shows no pattern as expected) by graphing the values versus the iteration. This can inform us the direction in which the search was heading in terms of where the values tended to cluster. Given these graphs, we might then be able to extrapolate values that lead to even higher scores (or maybe not, _extrapolation is dangerous_!)', '# ', '# The black star in the plots below signifies the best scoring value.', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '# We want to be careful about placing too much value in these results, because remember, the Bayesian optimization could have found a local minimum of the cross validation loss that it is exploting. Moreover, the trends here are generally pretty small. It is encouraging that the best value was found close to the end of the search indicating cross validation scores were continuing to improve. ', '# ', '# Next, we can look at the values of the score as a function of the hyperparameter values. This is again a dangerous area! ', '# # Plots of Hyperparameters vs Score', '# ', '# ![](http://)These next plots show the value of a single hyperparameter versus the score. We want to avoid placing too much emphasis on these graphs, because we are not changing one hyperparameter at a time. Therefore, if there are trends, it might not be solely due to the single hyperparameter we show. A truly accurate grid would be 10-dimensional and show the values of __all__ hyperparameters and the resulting score. If we could understand a __10-dimensional__ graph, then we might be able to figure out the optimal combination of hyperparameters! ', '# In[ ]:', '# Append the two dataframes together', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '        # Scatterplot', ""# __The only clear distinction is that the score decreases as the learning rate increases.__ Of course, we cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of esimators shortly). The learning rate domain was on a logarithmic scale, so it's most accurate for the plot to be as well (unfortunately I cannot get this to work yet)."", '# In[ ]:', ""# hyper = 'learning_rate'"", '# fig, ax = plt.subplots(1, 1, figsize = (6, 6))', '# random_hyp[hyper] = random_hyp[hyper].astype(float)', '# # Scatterplot', ""# sns.regplot(hyper, 'score', data = random_hyp, ax = ax, color = 'b', scatter_kws={'alpha':0.6})"", ""# ax.scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'b', edgecolor = 'k')"", '# opt_hyp[hyper] = opt_hyp[hyper].astype(float)', '# # Scatterplot', ""# sns.regplot(hyper, 'score', data = opt_hyp, ax = ax, color = 'g', scatter_kws={'alpha':0.6})"", ""# ax.scatter(best_opt_hyp[hyper], best_opt_hyp['score'], marker = '*', s = 200, c = 'g', edgecolor = 'k')"", ""# ax.set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper))"", ""# ax.set(xscale = 'log');"", '# Now for the next four hyperparameters versus the score.', '# In[ ]:', '# Plot of four hyperparameters', '        # Scatterplot', '        # Scatterplot', '# There are not any strong trends here. Next we will try to look at two hyperparameters simultaneously versus the score in a 3-dimensional plot. This makes sense for hyperparameters that work in concert, such as the learning rate and the number of esimators or the two regularization values.', '# ## 3D Plots ', '# ', '# To try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. 3D plots can be made in matplotlib by import `Axes3D` and specifying the `3d` projection in a call to `.add_subplot`', '# In[ ]:', '# First up is `reg_alpha` and `reg_lambda`. These control the amount of regularization on each decision tree and help to prevent overfitting to the training data.', '# In[ ]:', '# In[ ]:', ""# It's a little difficult to tell much from this plot. If we look at the best values and then look at the plot, we can see that scores do tend to be higher around 0.9 for `reg_alpha`and 0.2 for `reg_lambda`.  Later, we'll make the same plot for the Bayesian Optimization for comparison."", '# The next plot is learning rate and number of estimators versus the score. __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__. The number of estimators __was not__ a hyperparameter in the grid that we searched over. Early stopping is a more efficient method of finding the best number of estimators than including it in a search (based on my limited experience)!', '# In[ ]:', '# Here there appears to be a clear trend: a lower learning rate leads to higher values! What does the plot of just learning rate versus number of estimators look like?', '# In[ ]:', '# This plot is very easy to interpret: the lower the learning rate, the more estimators that will be trained. From our knowledge of the model, this makes sense: each individual decision trees contribution is lessened as the learning rate is decreased leading to a need for more decision trees in the ensemble. Moreover, from the previous graphs, it appears that decreasing the learning rate increases the model score.', '# ### Function for 3D plotting', '# ', ""# Any time you write code more than twice, it should be encoded into a function! That's what the next code block is for: putting this code into a function that we can use many times! This function can be used for __any__ 3d plotting needs."", '# In[ ]:', '    # 3d scatterplot', '    # Plot labeling', '# The bayesian optimization results are close in trend to those from random search: lower learning rate leads to higher cross validation scores.', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Again, we probably want one of the regularization values to be high and the other to be low. This must help to ""balance"" the model between bias and variance. ', '# # Correlations between Hyperparameters and Score', '# ', '# Time for another dangerous act: finding correlations between the hyperparameters and the score. These are not going to be accurate because again, we are not varying one value at a time! Nonetheless, we may discover useful insight about the Gradient Boosting Machine model.', '# ### Correlations for Random Search', '# In[ ]:', '# As expected, the `learning_rate` has one of the greatest correlations with the score. The `subsample` rate might be affected by the fact that 1/3 of the time this was set to 1.0.', '# In[ ]:', '# ### Correlations for Bayesian Optimization', '# In[ ]:', '# The `learning_rate` again appears to be moderately correlated with the score. This should tell us again that a lower learning rate tends to co-occur with a higher cross-validation score, but not that this is nexessarily the cause of the higher score. ', '# In[ ]:', '# ## Correlation Heatmap', '# ', '# Now we can make a heatmap of the correlations. I enjoy heatmaps and thankfully, they are not very difficult to make in `seaborn`.', '# In[ ]:', '# Heatmap of correlations', ""# That's a lot of plot for not very much code! We can see that the number of estimators and the learning rate have the greatest magnitude correlation (ignoring subsample which is influenced by the boosting type)."", '# In[ ]:', '# Heatmap of correlations', '# Feel free to use this code for your own heatmaps! (Also send me color recommendations because I am not great at picking out a palette).', '# # Meta-Machine Learning', '# ', '# So we have a labeled set of data: the hyperparameter values and the resulting score. Clearly, the next step is to use these for machine learning? Yes, here we will perform _meta-machine learning_ by fitting an estimator on top of the hyperparameter values and the scores. This is a supervised regression problem, and although we can use any method for learning the data, here we will stick to a linear regression. This will let us examine the coefficients on each hyperparameter and will help reduce overfitting. ', '# In[ ]:', '# Create training data and labels', '# In[ ]:', '# Create the lasso regression with cv', '# Train on the data', '# In[ ]:', '# If we wanted, we could treat this as _another optimization problem_ and try to maximize the linear regression in terms of the score! However, for now I think we have done enough optimization. ', '# ', ""# It's time to move on to implementing the best hyperparameter values from random and Bayesian optimization on the full dataset."", '# # Implementation', '# ', '# The full set of features on which these results come are from [this notebook](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https://www.kaggle.com/jsaguiar)). Here, we will load in the same features, train on the full training features and make predictions on the testing data. These can then be uploaded to the competition.', '# In[ ]:', '# In[ ]:', '# First we need to format the data and extract the labels.', '# In[ ]:', '# We can also save the features to later use for plotting feature importances.', '# In[ ]:', '# ### Random Search', '# In[ ]:', '# In[ ]:', '# ### Bayesian Optimization', '# In[ ]:', '# In[ ]:', '# ### Competition Results', '# ', '# * __Random search results scored 0.790__', '# * __Bayesian optimization results scored 0.791__', '# ', ""# If we go by best score on the public leaderboard, Bayesian Optimization wins! However, the public leaderboard is based only on 10% of the test data, so it's possible this is a result of overfitting to this particular subset of the testing data. Overall, I would say the complete results suggest that both methods produce similar outcomes especially when run for enough iterations. Either method is better than hand-tuning! "", '# #### Feature Importances', '# ', '# As a final step, we can compare the feature importances between the models from the best hyperparameters. It would be interesting to see if the hyperparameter values has an effect on the feature importances.', '# In[ ]:', '# In[ ]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    # Set the yticks and labels', '    # Plot labeling', '# In[ ]:', '# In[ ]:', '# The feature importances look to be relatively stable across hyperparameter values. This is what I expected, but at the same time, we can see that the _absolute magnitude_ of the importances differs significantly but not the _relative ordering_.', '# In[ ]:', '# # Conclusions', '# ', '# Random search narrowly beat out Bayesian optimization in terms of finding the hyperparameter values that resulted in the highest cross validation ROC AUC. That single number does not tell the whole story though as the Bayesian method average ROC AUC was much higher than that of random search. We expect this to be the case because Bayesian optimization should focus on higher scoring values based on the surrogate model of the objective function it constructs. Morevoer, this tells us Bayesian optimization is a valuable technique, but random search can still happen upon better values in fewer search iterations if we are lucky. ', '# ', '# * Random search slightly outperformed Bayesian optimization in terms of cv ROC AUC ', '# * Bayesian optimization average scores were much higher than random search indicating it spends more time evaluating ""better"" hyperparameters', '# * Bayesian scored 0.791 when submitted and random search scored 0.790 indicating that with enough iterations, the methods deliver similar results', '# * Boosting type ""gdbt"" did much better than ""goss"" with ""dart"" nearly as good', '# * A lower learning rate resulted in higher model scores: lower than 0.02 looks to be optimal', '# * `reg_alpha` and `reg_lambda` should complement one another: if one is high (above 0.5), than the other should be lower (below 0.5)', '# * Some subsampling appears to increase the model scores', '# * The other hyperparameters either did not have a significant effect, or their effects are intertwined and hence could not be disentangled in this study', '# ', ""# Feel free to build upon these results! I'm curious if the best hyperparameters for this dataset will translate to other datasets, either for this problem, or for vastly different data science problems. The best way to find out is to try them! "", '# ', ""# If you're looking for more work on this problem, I have a series of notebooks documenting my work:"", '# ', '# __Additional Notebooks__ ', '# ', '# * [A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)', '# * [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)', '# * [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)', '# * [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)', '# * [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)', '# * [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)', '# * [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)', '# * [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)', '# ', '# Thanks for reading and feel free to share any constructive criticism or feedback. ', '# ', '# Best,', '# ', '# Will', '# In[ ]:']",344
my-try.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# ## Import Libraries', '# In[3]:', '# ## The Data', '# ', '# > Reading train/test csv files into the pandas dataframe.', '# In[4]:', '# In[5]:', '# In[6]:', '# Target Distribution', '# Result: this is an imbalanced class problem', '# In[7]:', '# Testing data features', '# ## Check missing data', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# ## Categorical data', '# In[20]:', '# dtypes: float64(65), int64(41), object(16)', '# In[21]:', '# In[22]:', '# for example:', '# In[23]:', '# In[24]:', '# ## EDA', '# In[25]:', '# Target 1: client with payment difficulties', '# Result: Cash loans is the most popular contract type and no difficulties for Revolving contract type', '# In[26]:', '# Target 1: client with payment difficulties', '# ## CORR with TARGET', '# In[27]:', '# In[28]:', '# In[29]:', '# Display Neg', '# In[30]:', '# Display Pos', '# In[31]:', '# Make a new dataframe for features', '# In[32]:', '# THIS IS GOING TO BE A VERY LARGE PLOT', '# In[33]:', '# imputer for handling missing values', '# Feature names', '# Median imputation of missing values', '# Fit on the training data', '# In[34]:', '# Transform both training and testing data', '# In[35]:', '# Scale each feature to 0-1', '# Repeat with the scaler', '# In[36]:', '# In[37]:', '# In[38]:', '# ## Logistic Regression Implementation', '# In[39]:', '# Make the model', '# logmodel = LogisticRegression()', '# Train on the training data', '# logmodel.fit(traindf, train_labels)', '# predict_lr = logmodel.predict_proba(testdf)', '# ## KNN Implementation', '# ### Train Test Split', '# In[40]:', '# In[41]:', '# ### Using KNN', '# In[42]:', '# In[43]:', '# ### Predictions and Evaluations', '# In[44]:', '# In[45]:', '# In[46]:', '# ### Choosing a K Value', '# In[47]:', '# Will take some time', '# In[48]:', '# In[49]:', '# NOW WITH K=30', '# ### Pred on Testing dataset', '# In[50]:', '# In[51]:', '# In[52]:', '# Submission dataframe', '# In[53]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",108
notebook721481d097.py,"['# coding: utf-8', '# In[1]:', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '#   ', '# In[2]:', '# ---', '# #    ', '#   `HomeCredit_columns_description.csv`      ', '# In[3]:', '#    (      [https://www.kaggle.com/c/home-credit-default-risk/data](https://www.kaggle.com/c/home-credit-default-risk/data)),    -   ,    ,       `TARGET`, 1   ,       .      -     : , ,  ///, ,    . ', '#     ', '# In[4]:', '#    307511 ', '#     :', '# In[5]:', '# In[6]:', '#    48744 ', '#     :', '# In[7]:', ""#    `int64`       `TARGET`.   `object` -  ,    `string`'"", '#       `TARGET`   ', '# In[8]:', '# ,   0   ,  1,          ', '# ---', '# #  ', '#  ,         ', '# In[9]:', '# In[10]:', '# In[11]:', '#    ,           ,  33%      ', '#       ', '# In[12]:', '# In[13]:', '#       ,    ,          ', '#          ', '# ---', '# #  ', '#    ,     16   `object`,    .      .', '#  :', '# In[14]:', '#  :', '# In[15]:', '#   ,   2  ,   0/1,   -   one-hot ', '# In[16]:', '# one-hot', '# In[17]:', '#       ,   ,          ,    (   `TARGET`),   ', '# In[18]:', '# ---', '# #   ', '#           .    `DAYS_BIRTH`  `DAYS_EMPLOYED`,         ,          ,         .', '# In[19]:', '#  ', '# In[20]:', '# In[21]:', '# In[22]:', ""# ,  `train['DAYS_EMPLOYED']`   55  ,   -1000 "", '# In[23]:', '# ,         """"  ,   """".      ,  ...', '#   `DAYS_EMPLOYED`     (,   ,    )  `nan`,         . ,  ,  ,     .', '# In[24]:', '# In[25]:', '#      ,   ,       40-50  (     )', '# In[26]:', '#   ,     ,         ', '# In[27]:', '# In[28]:', '# ,       ,      ', '# In[29]:', '# ---', '# #   ', '#      `TRAIN`     ', '# In[30]:', '# 10    ', '# In[31]:', '#      `REGION_RATING_CLIENT_W_CITY`  `REGION_RATING_CLIENT`,       /,   .', '# In[32]:', ""# 10  '  "", '# In[33]:', ""# ,   `EXT_SOURCE`, ,  ,  Normalized score from external data source,   '  .    `DAYS_BIRTH`  `DAYS_EMPLOYED`:       ,     ."", '# ,      `CODE_GENDER_F`  `CODE_GENDER_M`,     .    `TARGET`    ', '# In[34]:', '# ,     ,      ,  ,   ', '#              ', '# In[35]:', '# , ,    ,      ,     ,    .   , ,          ', '#      `DAYS_EMPLOYED`', '# In[36]:', '#        ,   ,  ,  .', '#    ,  `DAYS_EMPLOYED/DAYS_BIRTH` -         (   ) ', '# In[37]:', '# In[38]:', '#    ,       , ,   ,     ,       ,  ,    .', '#      `EXT_SOURCE_1`, `EXT_SOURCE_2`, `EXT_SOURCE_3`: ,    ', '# In[39]:', ""# ,  `DAYS_EMPLOYED_FRAC`    `EXT_SOURCE` '   `TARGET`,   -  `DAYS_BIRTH`"", '# ---', '# # , ', '#   `SK_ID_CURR`, ,  ,   ID of loan in our sample,      ', '# In[40]:', '# In[41]:', '# In[42]:', '#  `train`  `train_lables`   train-  validation-', '# In[43]:', '#  grid search       ', '# In[44]:', '#  `RandomForestClassifier`   ROC-AUC  validation-', '# In[45]:', '# In[46]:', '#   feature importance, RandomForestClassifier    ', '# In[47]:', '# In[48]:', '# ,  `EXT_SOURCE_2`, `EXT_SOURCE_3`, `DAYS_BIRTH`, `DAYS_EMPLOYED`  `DAYS_EMPLOYED_FRAC`      ,      .        ', '# In[49]:', '#      ,     Kaggle', '# In[50]:']",117
olivier_lightgbm_parameters_by_bayesian_opt.py,"['# Based on excellent script by @olivier', '#', '# https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm', '#', '# My additions and changes:', '#', '# StratifiedKFold instead of KFold', '# LightGBM parameters found by Bayesian optimization ( https://github.com/fmfn/BayesianOptimization )', '# Out-of-fold file saved for downstream use in ensembling', '#', '        # LightGBM parameters found by Bayesian optimization', '    # Plot feature importances', '    # Plot ROC curves', '        # Plot the roc curve', '    # Plot ROC curves', '        # Plot the roc curve', '    # Build model inputs', '    # Create Folds', '    # Train model and get oof and test predictions', '    # Save test predictions', '    # Display a few graphs']",21
oof-all-home-credit-default-risk (1).py,"['# coding: utf-8', '# ## Calculated the running avg of predicted probability with OOF Prediction', '# ### Load Package', '# In[1]:', '# #####  \xa0 Google Drive   ', '# In[2]:', '# In[3]:', '    # google drive', '# ### load data as pandas DataFrame', '# In[4]:', '# ### Modulization for Feature Engineering', '# In[5]:', '# In[6]:', '    # EXT_SOURCE_X FEATURE', '    # AMT_ANNUITY - amount should be paid per month.', '    # AMT_CREDIT  - total amount of loan.', '    # AMT_GOODS_PRICE : consumer loadn.eg) car purchase installment.', '    # AMT_INCOME_TOTAL : income ', '    # CNT_FAM_MEMBERS  : the number of family members', ""    # DAYS_BIRTH : Client's age in days at the time of application"", '    # DAYS_EMPLOYED : How many days before the application the person started current employment', '    # AMT_APPLICATION : For how much credit did client ask on the previous application', '    # AMT_GOODS_PRICE : Goods price of good that client asked for (if applicable) on the previous application', ""    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']"", '    # data cleansing', '    # important features for determine days overdue', '    # 1.PREV_INTERESTS_RATE : interest ratio', '    #       \xa0   . ', '    # AMT_ANNUITY : Annuity of previous application', '    # \xa0    AMT_CREDIT  \xa0      . ', '         #   aggregation. ', '        #   aggregation', ""    # multi index  '_'   "", '    # rename column', '    # NaN  0 . ', '    # Refused or Approved previous credit', '    # prev_amt_agg . ', '    # SK_ID_CURR    APPROVED_COUNT  REFUSED_COUNT  . ', ""    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'  drop "", '    # DAYS_CREDIT_ENDDATE : CB \xa0    (\xa0\xad ) Remaining duration of CB credit (in days) at the time of application in Home Credit', '    # DAYS_ENDDATE_FACT : CB \xa0    \xa0 (\xa0\xad ,  Close) Days since CB credit ended at the time of application in Home Credit (only for closed credit)', '    # DAYS_CREDIT :   \xa0\xad     \xa0\xad  (How many days before current application did client apply for Credit Bureau credit)', '    #   /     ', '    #    120    ', '    # CREDIT_DAY_OVERDUE :  \xa0\xad  CB \xa0  ', '    # Number of days past due on CB credit at the time of application for related loan in our sample', '    #   ', '    #   SK_ID_CURR reset_index()  ', '        #   ', '    #   SK_ID_CURR reset_index()  ', '    # Status of Credit Bureau loan during the month (active, closed, DPD0-30,?[C means closed, X means status unknown, 0 means no DPD, ', '    # 1 means maximal did during month between 1-30, 2 means DPD 31-60,?5 means DPD 120+ or sold or written off ] )', '    # SK_ID_CURR \xa0  MONTHS_BALANCE aggregation  ', '    #   SK_ID_CURR reset_index()  ', '    # bureau_day_amt_agg bureau_active_agg .  ', '    # STATUS ACTIVE IS_DPD RATIO\xa0  . ', '    # bureau_agg bureau_bal_agg . ', '    # ,   0~ 120  ,   120   ', '    #   \xa0  SK_ID_CURR \xa0 \xa0 aggregation  ', '        #  . ', '    # MONTHS_BALANCE (20 )    . ', '    # MONTHS_BALANCE < 20 : having tendency to fall into DPD', '        #  . ', '    #   ', '    # SK_ID_CURR reset_index()   ', '    # AMT_INSTALMENT : \xa0', '    # AMT_PAYMENT - ', '    # DAYS_ENTRY_PAYMENT : When was the installments of previous credit paid actually (relative to application date of current loan -  \xa0\xad     \xa0)', '    # DAYS_INSTALMENT - When the installment of previous credit was supposed to be paid (relative to application date of current loan- \xa0\xad  \xa0  \xa0)', '    # ,   30~ 120  ,   100    . ', '    #   \xa0  SK_ID_CURR \xa0 \xa0 aggregation  . ', '        #   ', '    # \xa0  (DAYS_ENTRY_PAYMENT) \xa0 (1 )   ', '        #   ', '    #     \xa0    ', '    # DPD    .', '    #     SK_ID_CURR \xa0 aggregation \xa0  . ', '        #   ', '    # MONTHS_BALANCE \xa0  ( 3 )  .  ', '# In[7]:', '    #  prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg apps   /  . ', '# ### Creation of the final train and test datasets', '# In[8]:', '# application, previous, bureau, bureau_bal \xa0    . ', '# Category   Label  . ', '#    . ', '# In[9]:', '# ### Ouf Of Fold Prediction', '# In[10]:', 'np.zeros(10)  # note that 1 dim', '# In[11]:', '# In[12]:', ""    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)  # feature dateset"", ""    target_app = apps_all_train['TARGET']                           # target datasets"", '    # nfolds  cross validatin fold set  KFold  ', '    #  Out of Folds   validation set   \xa0  array .', '    # validation set n_split   ftr_app   . ', '    # Ouf of Folds   test dataset   \xa0  array . ', '    # n_estimators 4000 . ', '    #      .', '    # nfolds  cross validation Iteration  OOF      ', '        #            /  ', '        #  /    . early_stopping 200 . ', '        #     \xa0 \xa0  . ', '        #       \xa0 . ', '        # nfolds    \xa0 \xa0         \xa0    \xa0 \xa0 . ', '        # num_iteration  - stops at the time of early stopping', '        # [0 - normal , 1 - overdue] : fetch the probability of overdue', '        # Predecited probabily should be mean value ', '# In[13]:', '# ### CVS for predicted result', '# In[14]:', '# In[15]:', '# In[16]:']",114
preprocessing-feature-engineering-models.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Suppress warnings ', '# matplotlib and seaborn for plotting', '# # Read in Training Data', '# In[2]:', '# Training data', '# # Read in Testing Data', '# In[3]:', '# Testing data features', '# # Check if Dataset is Unbalanced', '# In[4]:', '# In[5]:', '# # Data Types', '# In[6]:', '# Number of each type of column', '# In[7]:', '# Number of unique classes in each object column', '# # One-Hot Encoding', '# In[8]:', '# one-hot encoding of categorical variables', '# # Align Training and Testing Data', '# In[9]:', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# # Preprocessing: Handle outliers and awkward data', '# In[10]:', '# # Helpful Functions', '# ## Function for Kernel Density Estimate Plots', '# The kernel density estimate plot shows the distribution of a single variable.', '# In[11]:', '# Plots the disribution of a variable colored by value of the target', '    # Calculate the correlation coefficient between the new variable and the target', '    # Calculate medians for repaid vs not repaid', '    # Plot the distribution for target == 0 and target == 1', '    # label the plot', '    # print out the correlation', '    # Print out average values', '# ## Function for Numeric Aggregations', '# Aggregates numeric values for count, mean, max, min, sum', '# In[12]:', '    # Remove id variables other than grouping variable', '    # Group by the specified variable and calculate the statistics', '    # Need to create new column names', '    # Iterate through the variables names', '        # Skip the grouping variable', '            # Iterate through the stat names', '                # Make a new column name for the variable and stat', '# ## Function to Calculate Correlation with Target', '# Function to calculate correlations with the target', '# In[13]:', '    # List of correlations', '    # Iterate through the columns ', '        # Skip the target column', '            # Calculate correlation with the target', '            # Append the list as a tuple', '    # Sort by absolute magnitude of correlations', '# ## Function to Handle Categorical Variables', '# This will calculate the counts and normalized counts of each category for all categorical variables in the dataframe.', '# In[14]:', '    # Select the categorical columns', '    # Make sure to put the identifying id on the column', '    # Groupby the group var and calculate the sum and mean', '    # Iterate through the columns in level 0', '        # Iterate through the stats in level 1', '            # Make a new column name', '# In[15]:', '    # Aggregate the numeric columns', '    # If there are categorical variables', '        # Count the categorical columns', '        # Merge the numeric and categorical', '        # Merge to get the client id in dataframe', '        # Remove the loan id', '        # Aggregate numeric stats by column', '    # No categorical variables', '        # Merge to get the client id in dataframe', '        # Remove the loan id', '        # Aggregate numeric stats by column', '# # Preprocessing', '# Handle missing values and outliers', '# ## Outliers', '# Remove or replace awkward values that were a mistake', '# In[16]:', '# ## Missing Values', '# Remove columns from training and testing datasets with greater than 50 percent of values missing.', '# In[17]:', '# Function to calculate missing values by column# Funct ', '        # Total missing values', '        # Percentage of missing values', '        # Make a table with the results', '        # Rename the columns', '        # Sort the table by percentage of missing descending', '        # Print some summary information', '        # Return the dataframe with missing information', '# In[18]:', '# Missing values statistics', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# Drop the missing columns', '# # Correlations', '# Examine the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data.', '# In[24]:', '# Calculate all correlations in dataframe', '# In[25]:', '# Ten most positive correlations', '# In[26]:', '# Ten most negative correlations', '# ## Colinear Variables', '# Calculate  the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. Look for any variables that have a greather than 0.8 correlation with other variables.', '# In[27]:', '# Set the threshold', '# Empty dictionary to hold correlated variables', '# For each column, record the variables that are above the threshold', '# Track columns to remove and columns already examined', '# Iterate through columns and correlated columns', '    # Keep track of columns already examined', '            # Only want to remove one in a pair', '# In[28]:', '# remove columns from training and testing sets']",129
project-of-datascience-homecreditdefaultrisk.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# # Distribution of AMT_CREDIT', '# In[18]:', '# # Distribution of AMT_INCOME_TOTAL', '# In[19]:', '# # Distribution of AMT_GOODS_PRICE', '# In[20]:', '# # Distribution of target (balanced or imbalanced)', '# In[21]:', '# # Distribution of Occupaiton Type', '# In[22]:', '# # Types of Organizations', '# In[23]:', '# # Contract product type', '# In[24]:', '# # Old or New Client', '# In[25]:', '# # Did the client requested insurance?', '# In[26]:', '# # Correlation', '# In[27]:', '# # Feature Importance using xgboost', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# # Preprocessing and Feature Engineering', '# In[32]:', ""# df = df[df['CODE_GENDER'] != 'XNA']"", '# In[33]:', '# for c in bureau_balance.columns:', '#     if bureau_balance[c].dtypes==object:', ""#         bb_aggregations[c] = ['mean']"", '# In[34]:', ""# df = df.join(bureau, how='left', on='SK_ID_CURR')"", '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[ ]:', '# # KFold cross validation lgb modeling', '# In[39]:', '# In[40]:', '# In[ ]:']",61
pure-gp-with-logloss.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",41
pure-gp-with-mean-squared-error.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",41
simple-blending-788-lb.py,"['# coding: utf-8', '# **Before you begin please upvote the original authors. Its all there effort not mine.**', '# **Links to original kernels-->**', '# 1.[Lightgbm with simple features by jsaguiar](http://www.kaggle.com/jsaguiar/lightgbm-with-simple-features-0-785-lb)', '# 2.[tidy_xgb -all tables by kxx](http://www.kaggle.com/kailex/tidy-xgb-all-tables-0-782/code)', '# In[2]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[6]:', '# In[7]:', '# In[8]:', '# In[ ]:']",18
simple-eda-and-lgb-model.py,"['# coding: utf-8', '# # Home credit default risk', '# ## All the information used in this kernel has been inspired by this amazing kernel: [here](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/notebook) ', '# ## so grateful for all the information', '# ### If you have any suggestions, critics or feedback, feel free to leave me a heads up here...', '# In[1]:', '# Python 3.5 is required', '# Scikit-Learn 0.20 is required', '# Common imports', '# Ignore useless warnings (see SciPy issue #5998)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# * Loading the application train, this is gonna be the base on which we are going to add more information from other datasets that has been provided to us.', '# In[2]:', '# * checking the shape of the data', '# * calling head to see the first entries', '# In[3]:', '# In[4]:', '# * Loading the testing set, checking the shape as well', '# In[5]:', '# In[6]:', '# #### Implementing a function to check for null values and plot a countplot', '# In[7]:', '# In[8]:', '    # Calculate missing stats for train and test (remember to calculate a percent!)', '    # list of missing columns for train and test', '    # Combine the two lists together', '    # Print information', '    # Drop the missing columns and return', '# In[9]:', '# #### A bunch of null values that need to be imputed, gonna address it soon', '# In[10]:', '# * We have 3 different types in our dataset(float64, int64, object), Gonna implement a function that reduce the amount of memory used.', '# In[11]:', '# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage', '# #### looking for outliers or some anomalies...', '# In[12]:', '# > The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:', '# In[13]:', '# * no issues here, the max and min are reasonable... ', '# * what about days employed', '# In[14]:', '# * as can be seen, the max value is not reasonable at all, here we have an outlier that might be a mistake when the data was created', '# In[15]:', ""# * well, we have 55374 values where our 'days employed' column is more than 300000, which is not a correct value. However, with such a high amount of entries showing this issue, one good approach would be to change the values to NAN and then, afterwards, impute those values with a median value"", '# In[16]:', ""# **Taking a look at the pearson's correlation using the .corr() method**"", '# In[17]:', '# In[18]:', '# > DAYS_BIRTH is the age in days of the client at the time of the loan in negative days. Meaning, the older the client gets, it is less likely that the loan defaults', '# #### gonna plot the KDE distribution relative to target, trying to show the relationship described above', '# In[19]:', '# KDE plot of loans that were repaid on time', '# KDE plot of loans which were not repaid on time', '# Labeling of plot', ""# ##### it's clear that the older the client gets, it's more likely that he is gonna pay the credit"", '# * the higher amount of credit default can be found within ages 20-25', '# ### I would like to plot a heat map with the top positive correlation feature(DAYS BIRTH) and all the top 3 negativaly correlated(all External Sources)', '# In[20]:', '# Extract the EXT_SOURCE variables and show correlations', '# Heatmap of correlations', '# ** As we have plotted the KDE for days birth, gonna do the same for all the external sources features **', '# In[21]:', '# iterate through the sources', '    # create a new subplot for each source', '    # plot repaid loans', '    # plot loans that were not repaid', '    # Label the plots', '# ***', '# ## Before diving into feature engineering and creating features that helps our models learn more from our data, I am gonna implement pipelines and a function to help us test our first models and see how they perform.', '# In[22]:', ""# * We have created a validation dataset in order to test our models, now let's take a look at the shape before implementing the functions to help us test our models"", '# In[23]:', '# In[24]:', '    # Sort features according to importance', '            # Extract feature importances', '        # Normalize the feature importances to add up to one', '        # Make a horizontal bar chart of feature importances', '        # Need to reverse the index to plot most important on top', '        # Set the yticks and labels', '        # Plot labeling', '    # Clean up memory', '# # Second Part - Improving our model -', '# ## it was provided extra datasets with plenty of information that can be added to our training set, help us engineering some extra features and improve our model', '# ### Gonna start loading bureau.csv, which contains information of previous loans of each client', '# In[25]:', ""# #### let's have an overall picture using .head()"", '# In[26]:', ""# #### We have the ID's of each client, which is the SK_ID_CURR, and information like when the loan was taken, end date, is it overdue, days credit and so forth....so we have here a credit history"", '# In[27]:', '# ** Nice, now we have the number of past credits each client have**', ""# * let's add this information to our training dataset"", '# In[28]:', ""# * Let's see the correlation with our target:"", '# In[29]:', '# Heatmap of correlations', '# * The correlation is really low, and negatively correlated, the higher this value(more past loans), less likely to default(remember, past loans that have been paid)', '# ', ""# ## as it was used previously, the KDE plot is gonna also show the relationship between one variable and other variables. It's gonna be implemented a function to help us plot using different variables"", '# In[30]:', '# Plots the disribution of a variable colored by value of the target', '    # Calculate the correlation coefficient between the new variable and the target', '    # Calculate medians for repaid vs not repaid', '    # Plot the distribution for target == 0 and target == 1', '    # label the plot', '    # print out the correlation', '    # Print out average values', ""# **Now, let's plot the KDE correlation of each of the columns I used before and see what the KDE plot show us **"", '# In[31]:', '# #### I am repeating the KDE graphs for the other columns to a base for comparison', '# ##### Unfortunately, tho, our new feature is not really helpful...', '# ', '# * gonna continue with feature engineering', '# ***', '# * Creating a funtion that gets a dataframe, column name and dataframe name, groups by the column name and aggregate using mean, count,min, max and sum.', '# In[32]:', '    # Remove id variables other than grouping variable', '    # Group by the specified variable and calculate the statistics', '    # Need to create new column names', '    # Iterate through the variables names', '        # Skip the grouping variable', '            # Iterate through the stat names', '                # Make a new column name for the variable and stat', '# In[33]:', '# In[34]:', ""# ** Now we have 61 new columns, that we're gonna merge to our training set **"", '# In[35]:', '# In[36]:', ""# ## that function is truly amazing to help us manage the numerical values of a given dataset, now, it's possible to create a very similar funtion using pd.getdummies and peform on categorical features."", '# In[37]:', '    # Select the categorical columns', '    # Make sure to put the identifying id on the column', '    # Groupby the group var and calculate the sum and mean', '    # Iterate through the columns in level 0', '        # Iterate through the stats in level 1', '            # Make a new column name', '# In[38]:', '# In[39]:', '# In[40]:', '# ** Great! Now we have processed the bureau dataframe and extracted information that might be useful for our models. We are gonna check correlation later **', '# ***', '# ### Now, with those functions is gonna be easy to process another dataframe, like the bureau balance', '# In[41]:', '# ### Previous application', '# In[42]:', '# In[43]:', '# In[44]:', '# In[45]:', '# In[46]:', ""# Counts of each type of status for each previous loan, it's a categorical feature"", '# In[47]:', '# Calculate value count statistics for each `SK_ID_CURR`', '# In[48]:', '# Dataframe grouped by the loan', '# Merge to include the SK_ID_CURR', '# Aggregate the stats for each client', '# In[49]:', '# In[50]:', '# In[51]:', '# Merge with the monthly information grouped by client', '# In[52]:', ""# ** We have increased the number of features quite significantly, it's advisable to clean up the memory, freeing up space, deleting objects that won't be used anymore, but before that, let's prepare our code for our models, creating dummies for both the training and testing set and align them**"", '# In[53]:', '# In[54]:', '# In[55]:', '# In[56]:', '# In[57]:', '# ### we have an expressive number of missing values, I am gonna use the function implemented before to drop all the columns with 73% or more of null values', '# In[58]:', '# In[59]:', ""# #### at least using pearson's correlation, it seems that some of the features that has been created has a positive (linear) correlation with our target."", '# In[60]:', '# In[61]:', '# ### The 3 most negativaly correlated features continue to be the 3 EXT source... some of the features create appears here as well..', ""# ### Dropping id columns and the target, as it's not gonna help our model"", '# ***', '# # Fitting our models...', ""# ## Notice that I am using only a small part of the dataset for models that won't give us a good score, to increase performance."", '# ', '# 1. Logistic Reression', '# In[62]:', '# Train on the training data', '# 2. Random Forest', '# In[63]:', '# Make the random forest classifier', '# 3. LGB model', '# In[64]:', '# In[65]:', '# In[66]:', '# In[67]:', '# In[68]:', '# In[69]:']",193
simple-exploration-pipeline-imputer-0-7.py,"['# coding: utf-8', '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[ ]:', 'import pandas as pd # package for high-performance, easy-to-use data structures and data analysis', 'import numpy as np # fundamental package for scientific computing with Python', 'import matplotlib.pyplot as plt # for plotting', 'import seaborn as sns # for making plots with seaborn', '# from plotly import tools', '# import plotly.tools as tls', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# **********POS_group-by**', '# In[ ]:', '# In[ ]:', ""# pos_grp_by_2 = POS_CASH_balance.groupby('SK_ID_CURR')['NAME_CONTRACT_STATUS'].max()"", '# pos_grp_by_2 = pos_grp_by_2.reset_index()', '# pos_grp_by_2.head(2)', '# pos_grp_by_1.head(2)', '# print (pos_grp_by_1.shape)', '# print (pos_grp_by_2.shape)', ""# pos_final_df = pos_grp_by_1.merge(pos_grp_by_2,on='SK_ID_CURR')"", '# print (pos_final_df.shape) ########## 1 data yet to join with credit data ', '# pos_final_df.head(2)', ""# data_2 = pos_final_df.merge(previous_application,on='SK_ID_CURR')"", ""# data_1 = data_2.merge(application_train,on='SK_ID_CURR')"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# **CATEGORICAL SEGREGATION**', '# In[ ]:', '# **NUMERICAL SEGREGATION**', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# **CHECKING FOR MISSING VALUES **', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# separate dataset into train and test', '# > **2 WAYS- (1) WITH ALL FEATURES  (2)SELECT FEATURES BASED ON ROC-AUC **', '# **> *1)WITH ALL FEATURES *******', '# Creating a PipeLine ', '# In[ ]:', '# Create a boolean mask for categorical columns', '# In[ ]:', '# In[ ]:', '# Apply categorical imputer( it will effect all categorocal columns)', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# **Before going to put everything into Pipeline - Do LabelEncoding to categorical vars ', '# **', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# **Now do Select Features based on ROC-AUC **', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Now check the Cross-validation Score ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Perform RandomizedSearchCV', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",117
simple-features.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', ' #  n_estimators=1327, n_jobs=-1, num_leaves=106, objective=None,random_state=None, reg_alpha=0.5129992714397862, reg_lambda=0.38268769901820565, silent=True, subsample=0.7177561548329953, subsample_for_bin=80000,', '  #     subsample_freq=0, verbose=1)', '# In[7]:', '# In[8]:', '# Save the submission dataframe']",12
simple-home-default-credit-lb-score-0-789.py,"['# coding: utf-8', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# Create an anomalous flag column', '# Replace the anomalous values with nan', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Align train and test', '# Align the training and testing data, keep only columns present in both dataframes', '# Add the target back in', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Make a new dataframe for polynomial features', '# imputer for handling missing values', '# Need to impute missing values', '# Create the polynomial object with specified degree', '# In[ ]:', '# Train the polynomial features', '# Transform the features', '# In[ ]:', '# In[ ]:', '# Create a dataframe of the features ', '# Add in the target', '# Find the correlations with the target', '# Display most negative and most positive', '# In[ ]:', '# Put test features into dataframe', '# Merge polynomial features into training dataframe', '# Merge polnomial features into testing dataframe', '# Align the dataframes', '# Print out the new shapes', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# check and remove constant columns', '# remove constant columns in the training set', '# remove constant columns in the test set', '# In[ ]:', '# In[ ]:', '# create temp DF', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', ' #   np.random.permutation(app_train_domain.index))', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '    # Extract the ids', '    # Extract the labels for training', '    # Remove the ids and target', '    # One Hot Encoding', '        # Align the dataframes by the columns', '        # No categorical indices to record', '    # Integer label encoding', '        # Create a label encoder', '        # List for storing categorical indices', '        # Iterate through each column', '                # Map the categorical features to integers', '                # Record the categorical indices', '    # Catch error if label encoding scheme is not valid', '    # Extract feature names', '    # Convert to np arrays', '    # Create the kfold object', '    # Empty array for feature importances', '    # Empty array for test predictions', '    # Empty array for out of fold validation predictions', '    # Lists for recording validation and training scores', '    # Iterate through each fold', '        # Training data for the fold', '        # Validation data for the fold', '        # Create the model', '        # Train the model', '        # Record the best iteration', '        # Record the feature importances', '        # Make predictions', '        # Record the out of fold predictions', '        # Record the best score', '        # Clean up memory', '    # Make the submission dataframe', '    # Make the feature importance dataframe', '    # Overall validation score', '    # Add the overall scores to the metrics', '    # Needed for creating dataframe of validation scores', '    # Dataframe of validation scores', '# In[ ]:', '# In[ ]:', '    # Sort features according to importance', '    # Normalize the feature importances to add up to one', '    # Make a horizontal bar chart of feature importances', '    # Need to reverse the index to plot most important on top', '    # Set the yticks and labels', '    # Plot labeling', '# In[ ]:', '# In[ ]:', '# In[ ]:']",112
simple-lightgbm.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# In[2]:', '# \xad', '# In[3]:', '# ', '# In[4]:', '# 5', '# In[5]:', '# ', '# In[6]:', '# target01', '# In[7]:', '# EXTtrain', '# In[8]:', '# ytrainTARGET', '# traintest3\xa0', '# In[9]:', '# train5', '# test5', '# In[10]:', '# import', '# In[11]:', '# train0.8:0.2', '# In[12]:', '# ', '# \xad', '# X_test', '# In[13]:', '# In[14]:', '# test', '# \xad', '# TARGETtarget', '# In[15]:', '# In[16]:', '# sample_submitsimple_light_gbm.csv', '# # ', '# * [Kaggle \\- kurupical\xad](http://kurupical.hatenablog.com/entry/2018/09/10/221420)', '# * [validation\xad](https://twitter.com/currypurin/status/1147841378040205314)', '# In[ ]:']",48
soumission-rc.py,"['# coding: utf-8', '# In[ ]:', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# In[ ]:']",5
stacking_test_sklearn_xgboost_catboost_lightgbm.py,"[""# Stacking Starter based on Allstate Faron's Script"", '# Preprocessing from ogrellier', ""# print('%-55s | %7s | %10s | %10s | %10s' "", ""#       % ('FEATURES', 'TYPE', 'NB VALUES', 'NB NaNS', 'NaNs (%)'))"", '# for f_ in data: # .dtypes', '#     print(""%-55s | %7s | %10s | %10s |    %5.2f""', '#           % (f_, str(data[f_].dtype), ', '#              str(len(data[f_].value_counts(dropna=False))), ', '#              str(data[f_].isnull().sum()),', '#              100 * data[f_].isnull().sum() / data.shape[0]', '#             )', '#          )', ""# PLEASE DON'T DO THIS AT HOME LOL"", '# Averaging factorized categorical features defeats my own reasoning']",14
target-variables.py,"['# coding: utf-8', '# In[1]:', '# In[2]:', '# HomeCredit_columns_description = pd.read_csv(""../input/home-credit-default-risk/HomeCredit_columns_description.csv"")', '# In[3]:', '# In[4]:', '# In[5]:', '# - secondary', '# \xad\xad', '# - higher education', '# \xad', '# - incomplete higher', '# \xa0\xad', '# - lower secondary', '# \xad\xad\xa0', '# - academic degree', '# \xad', '# In[6]:', '# In[7]:', '# In[8]:', '# - working', '# ', '# - State servant', '# ', '# - Commercial associate', '# ', '# - Pensioner', '# ', '# - unemployed', '# \xad', '# - student', '# \xad', '# - Businessman', '# ', '# - Maternity leave', '# \xad\xad', '# In[9]:', '# In[10]:', '# In[11]:', '# In[12]:', '# In[13]:', '# In[14]:', '# In[15]:', '# log', '# take the logarithm because the graph above is affected by outliers', '# In[16]:', '# range\xad', '# In[17]:', '# In[18]:', '# In[19]:', 'y_pred = gbm.predict(X_test)   # , num_iteration=gbm.best_iteration', 'y_pred_proba = gbm.predict_proba(X_test)  # , num_iteration=gbm.best_iteration', '# Feature Importance', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# X = X.fillna(X.mean())', 'y_pred = gbm.predict(X_test)   # , num_iteration=gbm.best_iteration', 'y_pred_proba = gbm.predict_proba(X_test)  # , num_iteration=gbm.best_iteration', '# Feature Importance', '# In[24]:', '# X = X.fillna(X.mean())', 'y_pred = gbm.predict(X_test)   # , num_iteration=gbm.best_iteration', 'y_pred_proba = gbm.predict_proba(X_test)  # , num_iteration=gbm.best_iteration', '# Feature Importance', '# In[25]:', '# In[26]:', '# X = X.fillna(X.mean())', 'y_pred = gbm.predict(X_test)   # , num_iteration=gbm.best_iteration', 'y_pred_proba = gbm.predict_proba(X_test)  # , num_iteration=gbm.best_iteration', '# Feature Importance', '# In[27]:', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# In[ ]:']",80
thank-you (1).py,"['# coding: utf-8', ""# **Thank you everyone for showing your appreciation and support. It's my first gold medal in kernels and I hope to publish far better kernels than this in near future.**"", '# In[ ]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# ## Blend with one rank weighted submission [0.8 LB]', '# In[ ]:', '# In[ ]:', '# Function for merging dataframes efficiently ', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Diversified blend [0.799 LB]', '# ', '# ', '# **The blending ingredients are taken from three different type of models.**', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Blending lowest correlated models', '# In[ ]:', 'print(Corr_Mat) # Correlation matrix of five submission files', '# In[ ]:', '# In[ ]:', '# ', '# In[ ]:']",34
the-first-baseline.py,"['# coding: utf-8', '# In[1]:', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# In[2]:', '# In[3]:', '# In[4]:', '# In[5]:', '# In[6]:', '# In[7]:', '# In[8]:', '# ### 1.Data preprocessing', '# 1.1 Numerique columns', '# In[9]:', '# In[10]:', '# In[11]:', '# Put the negative values to positive', '# In[12]:', '# Outliers', '# In[13]:', '# In[14]:', '# In[15]:', '# In[16]:', '# In[17]:', '# In[18]:', '# In[19]:', '# In[20]:', '# In[21]:', '# In[22]:', '# In[23]:', '# In[24]:', '# In[25]:', '# In[26]:', '# In[27]:', '# 1.2 Object columns', '# In[28]:', '# In[29]:', '# In[30]:', '# In[31]:', '# In[32]:', '# In[33]:', '# In[34]:', '# In[35]:', '# In[36]:', '# In[37]:', '# In[38]:', '# In[39]:', '# In[40]:', '# In[41]:', '# In[42]:', '# simple_impute ', '# In[43]:', '# In[44]:', '# In[45]:', '# In[46]:', '# ### 3.Model', '# In[47]:', '# In[48]:', '# 3.1 Rdf', '# In[49]:', '# In[50]:', '# In[51]:', '# In[52]:', '# In[53]:', '# In[54]:', '# In[55]:']",73
the-model-3.py,"['# coding: utf-8', '# - Still Working on this notebook :-)', '# ', '# -[Reference](https://www.kaggle.com/dromosys/fork-of-fork-lightgbm-with-simple-features-cee847)', '# In[ ]:', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# LightGBM GBDT with KFold or Stratified KFold', '# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        # LightGBM parameters found by Bayesian optimization', '    # Write submission file and plot feature importance', '# Display/plot feature importance', '# In[ ]:', '# In[ ]:']",46
tidy_xgb_all_tables_0_796.R,[],0
ultimate-guide.py,"['# coding: utf-8', '# # Home Credit Default Prediction Models', '# ### About Home Credit', '# Home Credit is a non-banking financial institution, founded in 1997 in the Czech Republic. ', '# * The company operates in 14 countries (including United States, Russia, Kazahstan, Belarus, China, India) and focuses on lending primarily to people with little or no credit history which will either not obtain loans or became victims of untrustworthly lenders. Home Credit group has over 29 million customers, total assests of 21 billions Euro, over 160 millions loans.', ""# * It uses of a variety of alternative data - including telco and transactional information - to predict their clients' repayment abilities."", '# ## 0. Setup', '# In[ ]:', ""plt.style.use('ggplot') # overall 'ggplot' style"", '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## 1. Some more EDA', '# #### Unbalanced Classes', '# In[ ]:', '# #### Income', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# Missings', '# ## 2. Feature Engineering', '# In[ ]:', '# ## Borrower characteristics', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# ## Employment', '# In[ ]:', '# In[ ]:', '# In[ ]:', ""# application_train[['OCCUPATION_TYPE', 'NAME_EDUCATION_TYPE', 'SK_ID_CURR']].groupby(['NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE']).count()"", '# In[ ]:', '# In[ ]:', '# ## Assets', '# In[ ]:', '# In[ ]:', '# ## Credit', '# In[ ]:', '# In[ ]:', '    i[""AMT_CREDIT_LOG""] = np.log(i[""AMT_CREDIT""]) # this is the amount of the loan', '    i[""AMT_GOODS_PRICE_LOG""] =np.log(i[\'AMT_GOODS_PRICE\']) # FOR CONS LOANS-- half million usd???', '# In[ ]:', ""# application_train['DOCS']"", '# In[ ]:', '# In[ ]:', '# ## 3. Modeling', '# ### Logistic Regression', '# In[ ]:', '# In[ ]:', '# for i in sorted(application_train.columns):', '#     print(i)', '# #### Train', '# In[ ]:', '# In[ ]:', '# #### Test', '# In[ ]:', '# In[ ]:', '# ### Logistic', '# In[ ]:', '# transform the test & train data', '# In[ ]:', '# In[ ]:', '# classification_report(y_train,p)', '# In[ ]:', '# In[ ]:', '# ## 4. Submission ', '# In[ ]:', '# In[ ]:', '# In[ ]:']",70
v0-only-main-dataset.py,"['# coding: utf-8', '# In[ ]:', '# %load /home/felipe/firstcell.py', '# In[ ]:', '# In[ ]:', '# one row = one loan', '# In[ ]:', '# In[ ]:', '# one row = one loan', '# ### merge names in both train+test sets for all categorical variables', '# ## v0 just using the stuff on the main table', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# take the second column because the classifier outputs scores for', '# the 0 class as well', '# fpr means false-positive-rate', '# tpr means true-positive-rate', ""# it's helpful to add a diagonal to indicate where chance "", '# scores lie (i.e. just flipping a coin)', '# ### final train', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",32
v1-main-dataset-and-bureau-only.py,"['# coding: utf-8', '# In[ ]:', '# %load /home/felipe/firstcell.py', '# ### common utils', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# one row = one loan', '# In[ ]:', '# In[ ]:', '# one row = one loan', '# In[ ]:', '# ### merge names in both train+test sets for all categorical variables', ""# only need to do this for the main dataset because it's split in train and test"", '# In[ ]:', '# In[ ]:', '# ### start to work', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# num_cols = 3', '# num_rows = math.ceil(len(ParameterGrid(param_grid)) / num_cols)', '# # create a single figure', '# plt.clf()', '# fig,axes = plt.subplots(num_rows,num_cols,sharey=True)', '# fig.set_size_inches(num_cols*5,num_rows*5)', '# for i,g in tqdm(enumerate(ParameterGrid(param_grid))):', '#     clf = XGBClassifier()', '#     clf.set_params(**g)', '#     clf.fit(X_train,y_train)', '#     y_preds = clf.predict_proba(X_test)', '#     # take the second column because the classifier outputs scores for', '#     # the 0 class as well', '#     preds = y_preds[:,1]', '#     # fpr means false-positive-rate', '#     # tpr means true-positive-rate', '#     fpr, tpr, _ = metrics.roc_curve(y_test, preds)', '#     auc_score = metrics.auc(fpr, tpr)', '#     ax = axes[i // num_cols, i % num_cols]', ""#     # don't print the whole name or it won't fit"", '#     ax.set_title(str([r""{}:{}"".format(k.split(\'__\')[1:],v) for k,v in g.items()]),fontsize=9)', ""#     ax.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc_score))"", ""#     ax.legend(loc='lower right')"", ""#     # it's helpful to add a diagonal to indicate where chance "", '#     # scores lie (i.e. just flipping a coin)', ""#     ax.plot([0,1],[0,1],'r--')"", '#     ax.set_xlim([-0.1,1.1])', '#     ax.set_ylim([-0.1,1.1])', ""#     ax.set_ylabel('True Positive Rate')"", ""#     ax.set_xlabel('False Positive Rate')"", '# plt.gcf().tight_layout()', '# plt.show()', '# ### final train', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:', '# In[ ]:']",72
xgb_lgbm_vote.py,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# -*- coding: utf-8 -*-', '#', '#', '#                              indices=True)', '#                                 n_estimators=10000,', ""#                                boosting_type='dart',"", ""##        # criterion = 'gini' 'entropy'"", ""#        'gradboost':GradientBoostingClassifier(random_state=111,n_estimators=200, "", '#                                               learning_rate=0.1,', '#                                               max_depth =3)', ""#        'forest':RandomForestClassifier(n_estimators = 200, criterion = 'gini', "", '#                                        random_state = 100, n_jobs=-1)', '    #    for train_index, test_index in skf.split(df_train.index, y_target):', '#del a, b, c # dfs still in list', 'del lst     # memory release now']",23
xgb_simple_features.py,"['# Forked from excellent kernel : https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features', '# From Kaggler : https://www.kaggle.com/jsaguiar', '# Just added a few features so I thought I had to make release it as well...', '# One-hot encoding for categorical columns with get_dummies', '# Preprocess application_train.csv and application_test.csv', '    # Read data and merge', '    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)', '    # NaN values for DAYS_EMPLOYED: 365.243 -> nan', '    # Categorical features with Binary encode (0 or 1; two categories)', '    # Categorical features with One-Hot encode', '    # Some simple new features (percentages)', ""    # df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']"", ""    # df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']"", ""    # df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']"", ""    # df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']"", ""    # df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']"", '# Preprocess bureau.csv and bureau_balance.csv', '    # Bureau balance: Perform aggregations and merge with bureau.csv', '    # Bureau and bureau_balance numeric features', '    # Bureau and bureau_balance categorical features', '    # Bureau: Active credits - using only numerical aggregations', '    # Bureau: Closed credits - using only numerical aggregations', '# Preprocess previous_applications.csv', '    # Days 365.243 values -> nan', '    # Add feature: value ask / value received percentage', '    # Previous applications numeric features', '    # Previous applications categorical features', '    # Previous Applications: Approved Applications - only numerical features', '    # Previous Applications: Refused Applications - only numerical features', '# Preprocess POS_CASH_balance.csv', '    # Features', '    # Count pos cash accounts', '# Preprocess installments_payments.csv', '    # Percentage and difference paid in each installment (amount paid and installment value)', '    # Days past due and days before due (no negative values)', '    # Features: Perform aggregations', '    # Count installments accounts', '# Preprocess credit_card_balance.csv', '    # General aggregations', '    # Count credit card lines', '# XGB GBDT with KFold or Stratified KFold', '    # Divide in training/validation and test data', '    # Cross validation model', '    # Create arrays and dataframes to store results', '        if n_fold == 0: # REmove for full K-fold run', '            sub_preds += clf.predict_proba(test_df[feats])[:, 1] # / folds.n_splits # - Uncomment for K-fold ', '            # feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)', ""    # print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))"", '    # Write submission file and plot feature importance', '# Display/plot feature importance']",50
