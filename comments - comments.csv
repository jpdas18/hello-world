file_name,comments,num_comm
aashishpotnuru_titanic-dataset.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
abinayajayaprakash_titanic-data-analysis.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
aditsrivastava4_titanic-prediction-by-adit-srivastava.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', ""#     if(str(x[3])=='nan'):"", '#         x[3] = np.random.randint(5,100)', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",18
akashadesai_eda-predicting-titanic-survivors-using-ml.html,"[' #   Column       Non-Null Count  Dtype  ', '# 3rd class people not survuved mostly', '# most of parents dont have child .i.e indicates in 0', '# 88% accuracy model']",4
akkidhanani_titanic-dataset-apply-logistic-regression.html,[],0
alagappan_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79
alexeydudchenko_titanic-1.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
alissecian_titanic-crash-course-decision-tree-visualization.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Loading data', '# Cleaning and preparing data', '# Training the tree', '# https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c', '# Parameters that we will pass to the library that draws the tree, to make it understandable', '# Create DOT data               ', '# Convert to png using system command hdddrequires Graphviz)', '# Display in jupyter notebook', '# Training a tree with modified parameters']",17
altrev_titanic.html,[],0
amigosmomo_titanic-data-science-xgboost-with-tunning.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# æ•¸æ“šåˆ†æžçš„ Class', '# åœ–åƒåŒ–çš„ Class', '# æ©Ÿå™¨å\xad¸ç¿’çš„ Class', '# ç”¨ä»¥é¡¯ç¤ºå‘Šè¨´çš„ Class', '# ç”¨ä»¥ç„¡è¦–ç¨‹å¼è\xad¦å‘Š', '# è¼‰å…¥æ•¸æ“š', '# Convert Sex', '# Filling missing values', '# Making Bins', '# cross tab', '# plots', '# close FacetGrid object', '# Filling missing values', '# Making Bins', '# cross tab', '# plots', '# close FacetGrid object', '# Ticket', '# the same ticket family or friends', ""df_data['Connected_Survival'] = 0.5 # default "", '# Masks', '# Plot', '# extracted title using name', '# Filling the missing age', ' # 0 1 2 3 4 5', '# split training set the testing set', '# åˆ†é–‹è¿” training set åŒ test set', '# Training set and labels', '# Using default parameter', '# Submission with hyperparameter tunning', '# Using default parameter']",39
amyhurd_titanic-data-science-solutions-d22403.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
anandrikka_project-titanic.html,"['# Checking for kaggle/input/titanic/****.csv', '# Column datatypes', '# Columns', '# Combine dataset for exploratory analytics', '# Change Pclass, Sex, Embarked to type category', '# Check for null count', '# Describe gives a very quick brief 5-point summary of the data. Take a peek look into numerical data.', '# changing back to original data types.', '# Categorical plots (Bar)', '    _ = sns.barplot(x=_.index, y=_, ax=ax[i]) # returns ax of matplotlib', '        label_x = patch.get_x() + patch.get_width()/2 # Mid point in x', '        label_y = patch.get_y() + patch.get_height() + 10 # Mid point in y', '    ax_ = sns.barplot(x=cols[i][""col""], y=""Age"", ax=ax[i], data=train_data) # returns ax of matplotlib', '    ax_ = sns.barplot(x=cols[i][""col""], y=""Survived"", ax=ax[i], data=train_data) # returns ax of matplotlib', '    ax_ = sns.barplot(x=cols[i][""col""], y=""Survived"", hue=""Sex"", ax=ax[i], data=train_data) # returns ax of matplotlib', '# ax = sns.FacetGrid(train_data, col=""Sex"", row=""Pclass"", hue=""Survived"", legend_out=True)', '# ax.map(sns.kdeplot, ""Age"")', '    label_x = patch.get_x() + patch.get_width()/2  # find midpoint of rectangle', '    label_x = patch.get_x() + patch.get_width()/2  # find midpoint of rectangle', '# Check for survival rate against each category', '# Correlation Coefficents', '# _ = df.groupby([""Sex"", ""Pclass""]).describe()', '# When grouped by sex and by pclass mean and median are most consistent and they values are pretty much closer.', '# So replacing the values based on these stats should be easy.', '# Comparing Age distributions before and after imputing the values.', '# Data distribution remains almost same before and after imputing values.', '# deck_x_pclass.plot.bar(stacked=True)', '# now stack and reset', '# plot grouped bar chart', '# Plot percentage of passengers', '# Replace Deck T with A as it has only one passenger', ""# plt.title('Survival Percentage in Decks')"", ""# plt.title('Random Forest Classifier Mean Feature Importance Between Folds')""]",33
andreoliveira1992_titanic-andre.html,"['# This Python 2.7.14 environment on ANACONDA 4.5.0 with pip 9.0.3; running in ubuntu LTS with a API_key to kaggle', '# Input data files are available in the /home/ghillaz/.kaggle/competitions/titanic', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# pandas.DataFrame ; pandas.DataFrame.info ; pandas.DataFrame.describe', '# Indexing and Selecting Data', '# pandas.isnull ; pandas.DataFrame.sum ;pandas.DataFrame.mode ; pandas.DataFrame.copy ;', '# pandas.DataFrame.fillna ; pandas.DataFrame.drop ; pandas.Series.value_counts ; pandas.DataFrame.loc', ""    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1"", '    # Age Buckets (6 buckets): 0-13.33;13.33-26.67...', 'cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%']",10
anguscc_a-journey-through-titanic-2268c7.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79
anshita2794_titanic-survival-problem-sample.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn']",3
anthonyivan_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
anupchandratre_titanic-basic-solution-with-logistic-regression.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
arindamkumar_solution-2-0-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# save the passenger id for the final submission', '# merge train and test', '# normalize the titles', '# view value counts for the normalized titles', '# view value counts for the normalized titles', '# ## map the first letter of the cabin to the cabin.', '# titanic.Cabin=titanic.Cabin.map(lambda x:x[0])', '# ## view the normalized count', '# titanic.Cabin.value_counts(normalize=True)', '# create X and y for data and target values', '# The parameters that we are going to optimise', '# Perform grid search using the parameters and f1_scorer as the scoring method', '# here cv is used for the cross-validation strategy.', 'clf1=grid_search.best_estimator_        # get the best estimator(classifier)', '# Print the tuned parameters and score', '# create param grid object', '# build and fit model', '# random forrest prediction on test set']",26
arushiburson_titanic-machine-learning-85-accuracy.html,[],0
ashithjake_titanic-survival-prediction-improved-model.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# Import Data', ' #   Column       Non-Null Count  Dtype  ', '# Explore Training Data', '# Data Cleaning', '# Feature Engineering', '# Only training data should used. Test data should not be introduced', '# Function to replace missing age values based of the mean age', '# Fill the missing age values based of the mean age in the training data', '# Convert categorical features into dummy variables', '# Function to map name title to Mr,Mrs,Miss or Others', '# Model Selection']",20
ashvsmay00_titanic-data-science-solutions-83f146.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
astrocyte_cungbuk-univ-2020-comphy2-titanic-tutorial.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.']",8
asyl21795_asyl-titanic.html,[],0
atripathi3675_titanic-basic-solution-with-logistic-regression.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
atul0204_titanic-using-neural-network.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
ayang98_first-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Create regularization hyperparameter space', '# Create hyperparameter options']",10
ayumka_titanic-my-first-shakyo.html,"['# matplotlibã¨ã¯', '# import data from file', '# 3.24 Da-Double Check Cleaned Data', '# 3.25 Split Training and Testing Data']",4
backtosept_titanic-data-science-solutions-488442.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
bhavikjain_bhavik-titanic.html,[],0
bhavinmoriya_titanic-starts-off.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
bilalyussef_tba-titanic.html,"['# General Libraries', '# Cleaning and preprocessing', '# visiualization Libraries', ""plt.style.use('bmh')                    # Use bmh's style for plotting"", ""sns.set_style({'axes.grid':False})      # Remove gridlines"", '# modeling ', '# Classification libraries and evaluation', '# Regression Libraries', '# Other models', '# To ignore unwanted warnings', '# train data ', '# test data', ""def impute_age(age_pclass): # passing age_pclass as ['Age', 'Pclass']"", ""    # Passing age_pclass[0] which is 'Age' to variable 'Age'"", ""    # Passing age_pclass[2] which is 'Pclass' to variable 'Pclass'"", '# (for train) grab age and apply the impute_age, our custom function', '# (for test) grab age and apply the impute_age, our custom function ', '# train data ', '# test data', ""et = ExtraTreesClassifier(n_estimators=100) # bootstrap=False by default #max_features='auto',""]",20
billlwu_titanic-exploratory.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# load train and test data', '# see which field, if any, contains missing values', '# create features', '# correlation matrix', '# try a few transformations to make Fare look more normalized', '# try a few transformations on Age', '# fit logistic regression model via CV, compute in-sample prediction accuracy', '# found a nice tutorial on gridsearching hyperparameters at https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74', '# Number of trees in random forest', '# Number of features to consider at every split', '# Maximum number of levels in tree', '# Minimum number of samples required to split a node', '# Minimum number of samples required at each leaf node', '# Method of selecting samples for training each tree', '# Create the random grid', '# search across 100 different combinations, and use all available cores', '# Fit the random search model', '# Create the parameter grid based on the results of random search ', '# Create a based model', '# Instantiate the grid search model']",29
bittelc_bittelc-titanic.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
bkonovalov_titanic-bkonovalov-submit.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# test_df = test_df[x_cols]', ""#   test_df['Sex'] = le.transform(col_vals)"", '# mae = mean_absolute_error(yp, val_y)', '# gbtree: tree-based models', '# gblinear: linear models', '# from sklearn.metrics import accuracy_score', '# yp = np.rint(yp)', '# acc = accuracy_score(yp, y)', '# print(yp[:10])', '# print(y[:10])', '# print(acc)']",19
brightller_solutions-of-titanic-data-science.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
brudi16_eda-titanic.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '    439     # Create the parser.']",3
builderdavid_titanic-model-baseline2.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '#tr_data.info() # have some missing value at Age and Embarked and many at Cabin', '#tst_data.info() # Also have one missing value at Fare', ""tst_data[tst_data['Fare'].isna()] # this is person we don't know his Fare Value. I guess he didn't survive"", ""title_list = ['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev','Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess','Don', 'Jonkheer'] # We have all the titles for Passengers"", '    return None # for cases no title in name', '# overwrite Fare because only one missing record', 'Prev_Family_Name=set() # A set store the family names in previous Companion group', ""overall_data['Companion_Group_Num'] = 0 # Initial all Companion group as 0 which means they all travel independently"", '    # step 1 check the group', '        Current_Family_Name |= Prev_Family_Name # put these family together for next iteration', '        Group_Num -=1 # use the previous group_num', ""overall_data[idx].groupby(['Survived']).size()#.unstack() #"", ""# columns to code Let's try sklearn ordinal encode for Sex and onehot encode for other"", 'enc2 = OneHotEncoder(sparse=True) # exercise a little with sparse matrix', ""        if featurename == 'ImputedEmbarked': featurename='Embarked' # shorten the column name for convenience"", ""#                        categorical_feature=['Pclass','Male?','EmbarkedCode','CabinCode','Missing_Age','Ticket_Type'],"", '#                        free_raw_data=False', '#                       )', ""#    'num_leaves':sp_randint(10,30), "", ""#    'early_stopping_round':50,"", ""#    'first_metric_only':True,    "", '#    print(k,v[-1])    ', ""tst_data['Survived'] = pd.Series(np.nan,index = tst_data.index) # use the value in test data,did not modify value in overall_data"", ""    elif survived_count>dead_count: tst_data.loc[na_index,'Survived']=1 # similar to the above""]",26
buraky1_titanic-eda-example.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '    # get feature', '    # count number of categorical variable(value/sample)', '    # visualize', '# Pclass vs Survived', '# Sex vs Survived', '# SibSp vs Survived', '# Parch vs Survived', '        # 1 st quartile', '        # 3 rd quartile', '        # IQR', '        # Outlier Step', '        # detect outlier and their indeces', '        # store indeces', '# drop outliers']",22
carlesorf_titanic-deep-in-the-sea-compare-models-and-keras.html,"['# pandas to open data files & processing it.', '# numpy for numeric data processing', '# sklearn to do preprocessing & ML models', '# Matplotlob & seaborn to plot graphs & visulisation', '# to fix random seeds', '# ignore warnings', '# Merge both datasets to work with all data at once', 'SibSp : # of siblings / spouses aboard the Titanic of passenger', 'Parch : # of parents / children aboard the Titanic of passenger', '    # Add some random ""jitter"" to the x-axis', '# if total family size is 1, person is alone.', '# Explore Fare distribution ', '# Apply log to Fare to reduce skewness distribution', ""# Let's do another column with fare limited to 100, because it can be a problem with mean NaN values."", ""# let's see again the correlations to select features"", '# Train Random Forest', '# Predict labels on Validation data which model have never seen before.', '# Various hyper-parameters to tune', '# Root Mean Squared Error', '# evaluate predictions', '# Train Random Forest', '# Predict labels on Validation data which model have never seen before.', 'from keras.models import Sequential # intitialize the ANN', 'from keras.layers import Dense      # create layers', '# Initialising the NN', '# layers', '# Compiling the ANN', '# Train the ANN']",28
catherineb_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', '# view information about training and test dataframes', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",76
clumsyeater_the-titanic-project.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
continue7777_titanic-data-science-solutions.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest']",24
crystaliu_titanic-survival-analytics.html,"['sibsp    # of siblings / spouses aboard the Titanic  ', 'parch    # of parents / children aboard the Titanic  ', ""age = td['Age'].dropna() # do it since seaborn can't create plot with NaN in the variable"", ""nuvar[0] = 'Age_all'   # replace age with age_all because seaborn.pairplot() doesn't work well with NaN"", ""td['Pclass'] = td['Pclass'].astype('category') # Convert Pclass from int type to category"", ""td['Embarked'] = td['Embarked'].fillna('S')  # Use the mode to fill the missing values"", ""# 'PassengerId',"", ""# 'Survived',"", ""# 'Name',"", ""# 'Age',"", ""# 'Ticket',"", ""# 'Cabin',"", 'td[feature_list].info()   # no missing values', ""# 'Pclass_3',"", ""# 'Sex_male',"", '# plot TPR against FPR', '# plot 45 degree line', '# import some data to play with', '# Split the data into a training set and a test set', '# Run classifier', '# Compute confusion matrix', '# Show confusion matrix in a separate window', ""# 'Name',"", ""# 'Ticket',"", ""# 'Cabin',"", ""# 'PassengerId',"", ""# 'Pclass_3',"", ""# 'Sex_male',"", ""# 'Embarked_S'""]",29
cuijamm_titanic-starter-code-score-0-82296.html,[],0
currypurin_titanic-lightgbm-ex-ver2.html,"['# Sexã®å¤‰æ›', '# Embarkedã®å¤‰æ› ä»Šå›žã¯onehot encodingã—ãªã„', '# ä¸è¦ãªåˆ—ã®å‰Šé™¤', '# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’trainã¨validã«åˆ†å‰²', '# lab.Datasetã‚’ä½¿ã£ã¦ã€trainã¨validã‚’ä½œã£ã¦ãŠã', '# lgb.trainã§å\xad¦ç¿’', '# valid_xã«ã¤ã„ã¦æŽ¨è«–', ""print('score', round(accuracy_score(valid_y, (oof > 0.5).astype(int))*100,2))  # validã®score"", ""sample_submission.to_csv('train_test_split.csv', index=False)  # score:75.119"", '# importanceã¯training dataã®åˆ—é\xa0†ã«è¡¨ç¤ºã•ã‚Œã‚‹', '# è¦‹ã‚„ã™ãã™ã‚‹', '# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’trainã¨validã«åˆ†å‰²', '# LightGBMã®åˆ†é¡žå™¨ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–', ""gbm = lgb.LGBMClassifier(objective='binary')  # , importance_type='gain'"", '# trainã¨validã‚’æŒ‡å®šã—å\xad¦ç¿’', '# valid_xã«ã¤ã„ã¦æŽ¨è«–', ""print('score', round(accuracy_score(valid_y, oof)*100,2));  # validã®score"", '# æŒ‡å®šã—ã¦ã„ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¡¨ç¤ºã•ã‚Œã‚‹', '# GridSearchCVã‚’import', '# è©¦è¡Œã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¾…åˆ—ã™ã‚‹', '                           gbm,  # åˆ†é¡žå™¨ã‚’æ¸¡ã™', '                           param_grid=params,  # è©¦è¡Œã—ã¦ã»ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™', '                           cv=3,  # 3åˆ†å‰²äº¤å·®æ¤œè¨¼ã§ã‚¹ã‚³ã‚¢ã‚’ç¢ºèª', 'grid_search.fit(X_train, y_train)  # ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™', 'print(grid_search.best_score_)  # ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º', 'print(grid_search.best_params_)  # ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º', '# ã‚¹ã‚³ã‚¢ã¨ãƒ¢ãƒ‡ãƒ«ã‚’æ\xa0¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ', '                             reg_lambda=10)  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®š']",28
daniel83fr_titanic-how-to-start-a-beginners-path.html,"['# Supress unnecessary warnings so that presentation looks clean', '# Print all rows and columns', '# Required imports']",3
danielhung_titanic-prediction.html,"['# Plotly', '    # Get passenger title from name.', ""#     df = df.join(pd.get_dummies(df['cabin_'], prefix='cabin_', drop_first=True))"", ""#     df = df.join(pd.get_dummies(df['Pclass'], prefix='Pclass', drop_first=True))"", '# feature_extract(df_test).head()']",5
dannydwkim_titanic-thx-to-ahmed.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Outlier detection ', '    # iterate over features(columns)', '        # 1st quartile (25%)', '        # 3rd quartile (75%)', '        # Interquartile range (IQR)', '        # outlier step', '        # Determine a list of indices of outliers for feature col', '        # append the found outlier indices for col to the list of outlier indices ', '    # select observations containing more than 2 outliers', '# detect outliers from Age, SibSp , Parch and Fare', '    # reading train data', '    # reading test data', '    # extracting and then removing the targets from the training data ', '    # merging train data and test data for future feature engineering', ""    # we'll also remove the PassengerID since this is not an informative feature"", '    # we extract the title from each name', '    # a map of more aggregated title', '    # we map each title', '    # a function that fills the missing values of the Age variable', '    # we clean the Name variable', '    # encoding in dummy variable', '    # removing the title variable', ""    # there's one missing fare value - replacing it with the mean."", '    # two missing embarked values - filling them with the most frequent one in the train  set(S)', '    # dummy encoding ', '    # replacing missing cabins with U (for Uknown)', '    # mapping each Cabin value with the cabin letter', '    # dummy encoding ...', '    # mapping string values to numerical one ', '    # encoding into 3 categories:', '    # adding dummy variable', '    # removing ""Pclass""', ""    # a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)"", '    # Extracting dummy variables from tickets:', '    # introducing a new feature : the size of families (including the passenger)', '    # introducing other features based on the family size', '                         subsample=.632, # Standard RF bagging fraction', '                         reg_alpha=10, # Hard L1 regularization', '# turn run_gs to True if you want to run the gridsearch again.']",47
davidcoxon_deeply-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', '# for handling data', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# for visualisation', '# for machine learning', '# Any results you write to the current directory are saved as output.', '# import data', 'df_data = df_train.append(df_test) # The entire data: train + test.', '# exporting the submission', '# Get a statistical overview of the training data', '# Get a statistical overview of the training data', '# Get title', '# Age in df_train and df_test:', '# convert Title categories to Columns', '# convert Embarked categories to Columns', '# Fill the na values in Fare based on average fare', '# convert FareBand categories to Columns', '# Age in df_train and df_test:', '# convert Pclass categories to Columns', '# sort Age into band categories', '# convert AgeGroup categories to Columns', '# convert categories to Columns', '# People with parents or siblings', 'df_data[""Alone""] = np.where(df_data[\'SibSp\'] + df_data[\'Parch\'] + 1 == 1, 1,0) # People travelling alone', '# Age in df_train and df_test:', '# get last name', '# Set survival value', '# Find Family groups by Fare', '        # A Family group is found.', '# Find Family groups by Ticket', '# Family_Survival in df_train and df_test:', '# check if cabin inf exists', '# split Embanked into df_train and df_test:', '# convert categories to Columns', '# convert SibSp categories to Columns', '# convert SibSp categories to Columns', '# define columns to be used', '# create test and training data', '# write data frame to csv file', '# DecisionTree with RandomizedSearch', '# Import necessary modules', '# Setup the parameters and distributions to sample from: param_dist', '# Instantiate a Decision Tree classifier: tree', '# Instantiate the RandomizedSearchCV object: tree_cv', '# Fit it to the data', '# Print the tuned parameters and score', '# Select columns', '# select classifier', '# train model', '# make predictions', '# create test and training data', '# Import necessary modules', '# create model', '# choose loss function and optimizing method', '# define 10-fold cross validation test harness', '    # create model', '    # Compile model', '    # Fit the model', '    # evaluate the model']",61
dbabs8_titanic-data-science-solutions-db.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
dcdanko_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', 'X_train = ica_df.transpose()[:n_train, :] # ', 'X_test  = ica_df.transpose()[n_train:, :] # test_df.drop(""PassengerId"",axis=1).copy()', '----> 4 X_train = ica_df.transpose()[:n_train, :] #', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview', '      6 # preview']",83
dex314_auto-feateng-for-titanic-data-forked.html,"['# Input data files are available in the ""../input/"" directory.', '# train_df.Cabin.value_counts()', '# Threshold for removing correlated variables', '# Absolute value correlation matrix', '# Select columns with correlations above threshold', '# features_positive = features_filtered.loc[:, features_filtered.ge(0).all()]', '# train_X = features_positive[:train_df.shape[0]]', ""# train_y = train_df['Survived']"", '# test_X = features_positive[train_df.shape[0]:]', '# lsvc = LinearSVC(C=0.01, penalty=""l1"", dual=False).fit(train_X, train_y)', '# model = SelectFromModel(lsvc, prefit=True)', '# X_new = model.transform(train_X)', '# X_selected_df = pd.DataFrame(X_new, columns=[train_X.columns[i] for i in range(len(train_X.columns)) if model.get_support()[i]])', '# X_selected_df.shape', '# X_selected_df.columns', '# features.head()', '# random_forest = RandomForestClassifier(n_estimators=2000,oob_score=True)', '# random_forest.fit(X_selected_df, train_y)', '# X_selected_df.shape', '# Y_pred = random_forest.predict(test_X[X_selected_df.columns])', '# print(Y_pred)']",21
dnfinal_titanic-data-science-solutions.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest']",24
dreamwave_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79
dvitsios_titanic-keras-dnn-classifier-one-hot-encoding.html,"['    # Make dummy variables for annotation', '    # Drop the previous rank column', '    # imputing missing values', ""    # label_mapping = {'YES': 1, 'NO': 0}"", ""    # data = data.replace({'Survived': label_mapping})"", '    # imputing missing values', ""    # data = data.interpolate(method='spline', order=2) # interpolate missing values"", '    # normalise data', ""# test = pd.read_csv('test.csv', sep=',')"", '# test_data = prepare_input_df(test, False)', '# test_data[:10]', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# from subprocess import check_output', '# print(check_output([""ls"", ""../input""]).decode(""utf8""))', '# Any results you write to the current directory are saved as output.', '    # Building the model', '    # Compiling the model', '    # sgd_optim = keras.optimizers.SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)', '    # adam=keras.optimizers.Adam(lr=0.00001)', '# model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)', '# # # Split X into training and test sets', '# Creating the model', '# Fit the model', ""# early_stopping = [EarlyStopping(monitor='acc', patience=2)] # do not apply without checking"", '# y_pred = y.values.argmax']",26
echarl2_titanic-with-hyperopt.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# One hot encoded', '# of columns: 12', ""#                            gamma = space['gamma'],"", ""#                            min_child_weight = space['min_child_weight'],"", ""#                            subsample = space['subsample'],"", ""#                            colsample_bytree = space['colsample_bytree'],"", '#                            ', ""#                            base_score=space['base_score'], "", ""# #                           max_delta_step=space['max_delta_step'], "", ""#                            scale_pos_weight=space['scale_pos_weight'],"", '#                            ', '#                            random_state=0,                             ', '#                            seed=None, ', '#                            silent=True, ', '#                            missing=None,', '#                            nthread=-1', '#                            #n_jobs=1,', '    # Applying Cross Validation', ""#    'base_score' : hp.quniform('base_score', .25, .75, .5),"", ""#    'scale_pos_weight' :  hp.quniform('scale_pos_weight', .25, .75, .5),"", ""#    'gamma' : hp.quniform('gamma', 0, 0.95, 0.01),"", ""#    'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),"", ""#    'subsample' : hp.quniform('subsample', 0.1, 1, 0.01),"", ""#    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.01)"", '# Fitting XGBoost to the Training set', ""#                            gamma = best['gamma'],"", ""#                            min_child_weight = best['min_child_weight'],"", ""#                            subsample = best['subsample'],"", ""#                            colsample_bytree = best['colsample_bytree'],"", '#                            ', ""#                            base_score=best['base_score'], "", ""# #                           max_delta_step=best['max_delta_step'], "", ""#                            scale_pos_weight=best['scale_pos_weight'],"", '#                            ', '#                            random_state=0,                             ', '#                            seed=None, ', '#                            silent=True, ', ""#                            booster='gbtree'"", ""#                            objective='binary:logistic',"", '# Applying k-Fold Cross Validation']",48
econundrums_titanic-notebook.html,"['# Exploring mean values', '# Dropping Embarked and Cabin']",2
eeshwarib_titanic-data-analysis-1.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# shape of our train data(no of rows,no of columns)', '# descriptive anlalysis of numerical variables', '# check data for null', '# unique values in each feature of dataset', '# grouping pclass and survived features of the dataset and plotting them using seaborn heatmap which gives us the survival rate based on passenger class', ""# emb=train_data['Embarked'].mode()"", '# librabries needed for our model building and predicting', '# since we need numerical data to process', ""# Converting 'object'-dtype to 'int' dtype"", '# Using Decision Tree Classifier to predict']",16
effydog_titanic-survival-prediction-end-to-end-ml-pipeline.html,"['# We can use the pandas library in python to read in the csv file.', '# This creates a pandas dataframe and assigns it to the titanic variable.', '# Print the first 5 rows of the dataframe.', ' # plots an axis lable', '# sets our legend for our graph.', 'titanic[""Deck""].unique() # 0 is for null values', 'titanic[""Deck""].unique() # Z is for null values', '# Create a family size variable including the passenger themselves', '# Discretize family size', '# The .apply method generates a new series', '    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.', '# Titles with very low cell counts to be combined to ""rare"" level', '# Also reassign mlle, ms, and mme accordingly', '#                \'Dr\', \'Major\', \'Rev\', \'Sir\', \'Jonkheer\']), ""Title""] = \'Rare Title\'', '# Titles with very low cell counts to be combined to ""rare"" level', '# Also reassign mlle, ms, and mme accordingly', '    # Split sets into train and test', '    # All age values are stored in a target array', '    # All the other values are stored in the feature array', '    # Create and fit a model', '    # Use the fitted model to predict the missing values', '    # Assign those predictions to the full data set', '# Import the linear regression class', '# Sklearn also has a helper that makes it easy to do cross validation', ""# The columns we'll use to predict the target"", '# Initialize our algorithm class', '# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.', '# We set random_state to ensure we get the same splits every time we run this.', ""    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds."", ""    # The target we're using to train the algorithm."", '    # Training the algorithm using the predictors and target.', '    # We can now make predictions on the test fold', '# Map predictions to outcomes (only possible outcomes are 1 and 0)', '# Initialize our algorithm', '# Compute the accuracy score for all the cross validation folds.', '# Take the mean of the scores (because we have one for each fold)', '# Initialize our algorithm with the default paramters', '# n_estimators is the number of trees we want to make', '# min_samples_split is the minimum number of rows we need to make a split', '# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)', '# Take the mean of the scores (because we have one for each fold)', '# Take the mean of the scores (because we have one for each fold)', ' #             ""FsizeD"", ""Embarked"", ""NlengthD"",""Deck"",""TicketNumber""]', '# Perform feature selection', '# Get the raw p-values for each feature, and transform from p-values into scores', '# Initialize our algorithm', '# Compute the accuracy score for all the cross validation folds.  ']",47
elshera_titanic-dataset-predicting-survivals.html,"['# Load the neccessary Libraries', '# Explore the dataset to see values, datatypes, nan values', '# data dimensions', '# show some initial rows ', '# For the Cathegorical variables we should see the set of their values over all samples.', '# Let see if the 209 cabines in the dataset host each one person or more', '# Are the Ticket unique', '# It makes sense to extract those columns which contain booleant values like 0-1 or Trus - False. ', '# Embarked have 2 missing values. With low error we can fill them based on the most likely value.', '# The most recurrent value is ""S"" therefore we can fill the missing values in Embarked with S and replace labels with ', '# numbers', '# Could replace SibSp and Parch with one column, called Family, indicating the overall family number. ', '# I do not see much difference if one is a brother or a sister from beeing a mother or a father.', '# Consider now replacing the Sex type from a label to a number.', '# Consider the distribution of the age', '# generate random numbers age_mean', '# fill in the randome values to the missing age values', '# convert from float to int', '# There are some columns which can be dropped already', '# finally look at the dataset', '# To better consider correlation between the various fields, we might use a correlation matrix.', '# collect all the survival', '# create groups based on them', '# Prepare the data. One of the first things to do is to split between the target to be predicted Y, and the variable ', '# before creating the feature matriy we need to drop the target ""Survived""', '# create features matrix', '# show the second row', '# load and prepare the test data.', '# We already know the data to be dropped', '# Filling missing data from the Age', '# Consider now replacing the Sex type from a label to a number.', '# Could replace SibSp and Parch with one column, called Family', '# We can drop now SibSp and Parch', '# create features matrix', '# Linear Regression', '# Random Forests', '# Support Vector Machine', '# If 3 or more out of 5 predictors vote for 1 than we have Survived = 1 otherwise we have Survived = 0']",38
elwlslek_titanic-eda-to-predict.html,"['# ë°ì´í„° ë¡œë“œ', '# í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³‘í•© ( ì\xa0„ì²˜ë¦¬ ìœ„í•´ )', '# Age, Cabin, Embarked ê²°ì¸¡ì¹˜ í™•ì¸', '# Age, Fare, Cabin ê²°ì¸¡ì¹˜ í™•ì¸', '# ì—¬ì„±ì´ ë‚¨ì„±ë³´ë‹¤ ë§Žì´ ì‚´ì•„ë‚¨ìŒ', '# ìƒì¡´ìžì—ì„œëŠ” ì°¨ì´ê°€ ì—†ì§€ë§Œ, ì‚¬ë§ìžëŠ” ëŒ€ë¶€ë¶„ 3ë“±ì¹¸, 2ë“±ì¹¸, 1ë“±ì¹¸ ìˆœìœ¼ë¡œ ë§Žë‹¤.', '# ì„\xa0ì°©ìž¥ì€ ì˜í–¥ì´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ë³´ìž„', '# Name ì»¬ëŸ¼ìœ¼ë¡œë¶€í„° ê²°í˜¼ì—¬ë¶€, ì„±ë³„, í™•ì¸ ê°€ëŠ¥', '# title_re ê°€ ì—†ëŠ” ê²½ìš°ì—ëŒ€í•œ ì²˜ë¦¬ê°€ ê°€ëŠ¥', '# for data in total_data:', ""#     data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.',expand=True)"", '# Mr : man', '# Miss : unmarried woman', '# Mrs : married woman', '# master : boy', '# Ms : not indicate her marital status.', '# Mr : 1 // Miss : 2 // Mrs : 3 // Master : 4 // Ohter : 5', '# ë¹„ìŠ·í•œ ì˜ë¯¸ ë§µí•‘ -> Mlle, Ms, Mme', '    # map í•¨ìˆ˜ì— ë§¤í•‘ë˜ì§€ ì•ŠëŠ” rowëŠ” nan ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì—', ""    data['Sex'] = data['Sex'].map({'female':0,'male':1}).astype(int) # map í•¨ìˆ˜ì— ë§¤í•‘ë˜ì§€ ì•ŠëŠ” rowëŠ” nan ì²˜ë¦¬"", '# ê²°ì¸¡ì¹˜ ì²˜ë¦¬', '    # í‘œì¤€íŽ¸ì°¨ëŠ” í‰ê·\xa0ìœ¼ë¡œ ë¶€í„° ì›ëž˜ ë°ì´íƒ€ì— ëŒ€í•œ ì˜¤ì°¨ë²”ìœ„ì˜ ê·¼ì‚¬ê°’ ( ë¶„ì‚°ì˜ ì\xa0œê³±ê·¼ ìœ¼ë¡œ ê·¼ì‚¬í•œë‹¤. )', '# Age : continous', '# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ ì\xa0œê±°', '# kNN Score']",25
eqchee_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", '# Data analysis and handling', 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Visualisation', '# Machine Learning', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.']",11
esraa966_titanic-competition.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# Make copy to avoid changing original data ', '# Apply label encoder to each column with categorical data', '# Imputation', '# Imputation removed column names; put them back', 'from sklearn.linear_model import LogisticRegression # Logistic Regression', '# Select numerical columns', '# Keep selected columns only', '# Preprocessing for numerical data', '# Preprocessing for categorical data', '# Bundle preprocessing for numerical and categorical data', '# Preprocessing of training data, fit model ', '# Preprocessing of validation data, get predictions', '# Evaluate the model', '# predictions = my_model.predict(imputed_X_valid)', '# print(accuracy_score(predictions, y_valid))', ""# test_data_sub = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')"", ""# output = pd.DataFrame({'Id': test_data_sub.index, 'Survived': prediction_rf})"", ""# output.to_csv('submission.csv', index=False)"", ""# print('done!')""]",27
evan8899_titanic-data-science-solutions.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
fernandot44_starter-titanic-0ff4440a-2.html,"['import matplotlib.pyplot as plt # plotting', 'import numpy as np # linear algebra', 'import os # accessing directory structure', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Distribution graphs (histogram/bar graph) of column data', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values', '# Correlation matrix', ""    df = df.dropna('columns') # drop columns with NaN"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '# Scatter and density plots', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns', '    # Remove rows and columns that would lead to df being singular', '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots', ""nRowsRead = 1000 # specify 'None' if want to read whole file"", '# train_and_test2.csv has 1309 rows in reality, but we are only loading/previewing the first 1000 rows']",16
flygare_titanic.html,"['   3052         # else, only a single dtype is given', '    503                 # _astype_nansafe works fine with 1-d only']",2
francoisolivier_titanic4.html,"['# This script shows you how to make a submission using a few', '# useful Python libraries.', '# It gets a public leaderboard score of 0.76077.', '# Maybe you can tweak it and do better...?', '# Load the data', ""# We'll impute missing values using the median for numeric columns and the most"", '# common value for string columns.', ""# This is based on some nice code by 'sveitser' at http://stackoverflow.com/a/25562948"", '# Join the features from train and test together before imputing missing values,', '# in case their distribution is slightly different', ""# XGBoost doesn't (yet) handle categorical features automatically, so we need to change"", '# them to columns of integer values.', '# See http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing for more', '# details and options', '# Prepare the inputs for the model', '# You can experiment with many other options here, using the same .fit() and .predict()', '# methods; see http://scikit-learn.org', '# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost', '# Kaggle needs the submission to have a certain format;', '# see https://www.kaggle.com/c/titanic-gettingStarted/download/gendermodel.csv', ""# for an example of what it's supposed to look like.""]",21
g9jiggy_titanic-solution-an-approach-from-a-beginner-s.html,"['# Data Dictionary', '# Variable\tDefinition\tKey', '# survival\tSurvival\t0 = No, 1 = Yes', '# pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd', '# sex\tSex\t', '# Age\tAge in years\t', '# sibsp\t# of siblings / spouses aboard the Titanic\t', '# parch\t# of parents / children aboard the Titanic\t', '# ticket\tTicket number\t', '# fare\tPassenger fare\t', '# cabin\tCabin number\t', '# embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton', '# Variable Notes', '# pclass: A proxy for socio-economic status (SES)', '# 1st = Upper', '# 2nd = Middle', '# 3rd = Lower', '# age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5', '# sibsp: The dataset defines family relations in this way...', '# Sibling = brother, sister, stepbrother, stepsister', '# Spouse = husband, wife (mistresses and fiancÃ©s were ignored)', '# parch: The dataset defines family relations in this way...', '# Parent = mother, father', '# Child = daughter, son, stepdaughter, stepson', '# Some children travelled only with a nanny, therefore parch=0 for them.', '# Importing the usual libraries and filter warnings', ' #   Column       Non-Null Count  Dtype  ', '# To see the survival count', '# To see the survival count among male and female', '# Since most of the population onboard was embarked for Southampton and ', '# One value of fare is missing', ""# Since almost a thousand people are missing Cabin its best we drop this column as we couldn't see any corelation between the "", '# value of Cabin and Survived at this stage', '# Looks like we have dealt with all the missing values quickly ', '# Later we could explore multiple other pro techniqies to deal with missing values but at this stage', '# we are looking for a quick model solution', ' #   Column    Non-Null Count  Dtype  ', '# Categorical values need to be converted into numbers and one such simple approach is one hot encoding', '# Loading ', '# Logistic Regression', '# Lets try some hyperparameter tuning with Grid search']",41
germanocorrea_titanic-first-draft.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# pegando mÃ©dia, desvio padrÃ£o e NaN no dataset de treino', '# mesma coisa com dataset de teste', '# Gerando nÃºmeros aleatÃ³rios entre mÃ©dia e desvio padrÃ£o.', '# desenhando plot original', '# Completando valores nulo com os valores aleatÃ³rios', '# logistic Regression', '# correlation']",15
gihanr_machine-learning-and-titanic.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', ""# survived_female_ages_age_NaN = data[(data['Survived'] == 1) & (data['Sex'] == 'female') & (~survived_female_ages['Age'].notna())]"", '# len(survived_female_ages_age_NaN)', ""# survived_female_ages = data[(data['Survived'] == 1) & (data['Sex'] == 'female')]"", '# replace NaN with survived females means', '# Analysis of based on Pclass', '# survived females with pclass', '# Analysis of based on Pclass', '# check ages of survived and not survived females', '# check ages of survived and not survived females', '# Analysis on Cabins', ""# data_replaced_ages.loc[data_replaced_ages['Embarked'].isnull()] = 'S'"", '# Change categorical values to numberical values using sklearn', '# pd.DataFrame({""PassengerId"": passengerId, ""Survived"" : results }, ignore_index=True)']",15
gmayock_gs-titanic-5-trying-to-get-in-the-top-25.html,"['# Step 1 is to import both data sets', '# Step two is to create columns which I will add to the respective datasets, in order to know which row came from which dataset when I combine the datasets', '# Now we append them by creating new columns in the original data. We use the same column name', '# Now we can merge the datasets while retaining the key to split them later', '# Encode gender (if == female, True)', '# Split out Title', '# Replace the title values with an aliased dictionary', '# Fill NaN of Age - first create groups to find better medians than just the overall median and fill NaN with the grouped medians', '# Fill NaN of Embarked', '# Fill NaN of Fare, adding flag for boarded free, binning other fares', '# Fill NaN of Cabin with a U for unknown. Not sure cabin will help.', '# Counting how many people are riding on a ticket', '# Finding cabin group', '# Adding a family_size feature as it may have an inverse relationship to either of its parts', '# Mapping ports to passenger pickup order', '# Encode childhood', '# One-Hot Encoding the titles', '# One-Hot Encoding the Pclass', '# One-Hot Encoding the  cabin group', '# One-Hot Encoding the ports', '# here is the expanded model set and metric tools', '# Here are the features', '# Define the classifiers I will use', '# Fit and use cross_val_score and k_fold to score accuracy', ""# So we're just going to take the features with a chi_squared over 1. "", '# rf_ stands for reduced features', '# Fit and use cross_val_score and k_fold to score accuracy', '# Fit and use cross_val_score and k_fold to score accuracy', ""# d = [['clf_name', 'feature_name', 'n_features', 'support', 'ranking']]"", '# for clf in classifiers2:', '#     for i in range (1,3):', '#         name = clf.__class__.__name__', '#         print(name, i)', '#         rfe = RFE(clf, i)', '#         fitted_clf = rfe.fit(nf3_cvs_train_data, nf3_cvs_target)', '#         n_feat = fitted_clf.n_features_', '#         n_supp = fitted_clf.support_', '#         rank = fitted_clf.ranking_', '#         d_new = [name, new_features_3, n_feat, n_supp, rank]', '#         d.append(d_new)', '# d_f = pd.DataFrame(d)', '# d_f', '# from yellowbrick.features import RFECV', '# import warnings', '# warnings.filterwarnings(""ignore"")', '# # Create RFECV visualizer with linear SVM classifier', '# for clf in classifiers2:', '#     viz = RFECV(clf)', '#     viz.fit(nf3_cvs_train_data, nf3_cvs_target);', '#     viz.poof();', '# Fit and use cross_val_score and k_fold to score accuracy']",51
goldob_titanic.html,[],0
gomesbruna_titanic-survival-prediction-1.html,"[' # plots an axis lable', '# sets our legend for our graph.', 'titanic[""Deck""].unique() # 0 is for null values', 'titanic[""Deck""].unique() # Z is for null values', '# Create a family size variable including the passenger themselves', '# Discretize family size', '# The .apply method generates a new series', '    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.', '# Titles with very low cell counts to be combined to ""rare"" level', '# Also reassign mlle, ms, and mme accordingly', '#                \'Dr\', \'Major\', \'Rev\', \'Sir\', \'Jonkheer\']), ""Title""] = \'Rare Title\'', '# Titles with very low cell counts to be combined to ""rare"" level', '# Also reassign mlle, ms, and mme accordingly', '    # Split sets into train and test', '    # All age values are stored in a target array', '    # All the other values are stored in the feature array', '    # Create and fit a model', '    # Use the fitted model to predict the missing values', '    # Assign those predictions to the full data set', '# Import the linear regression class', '# Sklearn also has a helper that makes it easy to do cross validation', ""# The columns we'll use to predict the target"", '# Initialize our algorithm class', '# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.', '# We set random_state to ensure we get the same splits every time we run this.', ""    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds."", ""    # The target we're using to train the algorithm."", '    # Training the algorithm using the predictors and target.', '    # We can now make predictions on the test fold', '# Map predictions to outcomes (only possible outcomes are 1 and 0)', '# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.', '# We set random_state to ensure we get the same splits every time we run this.', ""    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds."", ""    # The target we're using to train the algorithm."", '    # Training the algorithm using the predictors and target.', '    # We can now make predictions on the test fold', '# Map predictions to outcomes (only possible outcomes are 1 and 0)']",37
gracyf_titanic-survival-predictor-my-first-in-kaggle.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# absolute numbers', '# percentages', '# Find potential outliers in values array', '# and visualize them on a plot', '# We will have to scale it', '# Can replace Lady, Mlle , Mme , the Countess - Lady', '# Capt, Col, Jonkheer, Rev  - Ranked Personal', '# Miss and Ms to - Miss', '# Make train/test split', '# As usual in machine learning task we have X_train, y_train, and X_test', '#    test_size = 0.2, random_state = 0)', '# Caution! All models and parameter values are just ', ""# demonstrational and shouldn't be considered as recommended."", '# Initialize 1st level models.', '# Compute stacking features', '# Initialize 2nd level model', '# Fit 2nd level model', '# Predict', '# Final prediction score', 'SEED = 0 # for reproducibility', 'NFOLDS = 5 # set folds for out-of-fold prediction', '# Put in our parameters for said classifiers', '# Random Forest parameters', '# Extra Trees Parameters', '# AdaBoost parameters', '# Gradient Boosting parameters', '# Support Vector Classifier parameters ', '# Create 5 objects that represent our 4 models', 'train_pred_val_rf, test_pred_val_rf = get_oof(rf,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # Random Forest', 'train_pred_val_ada,test_pred_val_ada = get_oof(ada, X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # AdaBoost ', 'train_pred_val_gb, test_pred_val_gb = get_oof(gb,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # Gradient Boost', 'train_pred_val_svc, test_pred_val_svc = get_oof(svc,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # Support Vector Classifier', ""    # 'ExtraTrees' : train_pred_val_et.ravel(),"", ""    # 'AdaBoost': train_pred_val_ada.ravel(),"", ""     # 'SupportVector' : train_pred_val_svc.ravel(),"", ""     # 'XGBoost' :   train_pred_val_xgb.ravel() , "", ""     # 'BernouliNB' : train_pred_val_bernb.ravel(),"", ""     # 'SupportVector' : test_pred_val_svc.ravel(),"", ""     # 'XGBoost' :   test_pred_val_xgb.ravel(),"", ""     # 'BernouliNB' : test_pred_val_bernb.ravel(),"", '# create the sub models', '# create the ensemble model']",50
greenarrow2018_titanic-prediction.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# male = 1, female = 0;C = 0, Q = 1, S = 2']",10
greeshmagirish_titanic-regression.html,[],0
hamdi1_titanic-or-dietanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# data analysis and wrangling', '# visualization', '# machine learning']",12
harityadav_titanic-competition.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
harrygem_titanic-pipeline-and-save-load-model.html,"['# import necessary libraries', '        # check below link to understand the data cleansing process.', '        # https://www.kaggle.com/startupsci/titanic-data-science-solutions', '        # make sure to return a narray instead of DataFrame', '# create the pipeline: preprocessing first, then RF model', '# load data', '# fit the pipeline with Training features data and label data', '# Use pickle to save model for next usage.', '# Open saved model, and directly make the prediction with new data']",9
hnike25_titanic-deep-learning-keras-79-acc.html,"['    # Fare feature cleaning', 'y_axis = ax.axes.get_yaxis().set_visible(False) # turn off the y axis label', '    # Convert to categorical values Title ', '    # get_x pulls left or right; get_height pushes up or down', '# Age feature cleaning ...Again this is the work of Yassine Ghouzam....', 'y_axis = ax.axes.get_yaxis().set_visible(False) # turn off the y axis label', '# Family size feature cleaning and transform ', '    # get_x pulls left or right; get_height pushes up or down', '# Standardizing the dataset to normalize them', '    # Cross validate model with Kfold stratified cross val', '# ', '# XGB classifier fitting', '    # get_width pulls left or right; get_y pushes up or down', '# This function will return 1 (Survive) or 0 (Not survive)..This will help during the evalution on the test data during submission']",14
hudanivy_ai-camp-logistic-regression-titanic-homework.html,"['import numpy as np # æ•°ç»„å¸¸ç”¨åº“', 'import pandas as pd # è¯»å…¥csvå¸¸ç”¨åº“', 'from patsy import dmatrices,dmatrix # å¯æ\xa0¹æ®ç¦»æ•£å˜é‡è‡ªåŠ¨ç”Ÿæˆå“‘å˜é‡', 'from sklearn.linear_model import LogisticRegression # sk-learnåº“Logistic Regressionæ¨¡åž‹', 'from sklearn.model_selection import train_test_split, cross_val_score # sk-learnåº“è®\xadç»ƒä¸Žæµ‹è¯•', 'from sklearn import metrics # ç”Ÿæˆå„é¡¹æµ‹è¯•æŒ‡æ\xa0‡åº“', 'import matplotlib.pyplot as plt # ç”»å›¾å¸¸ç”¨åº“']",7
hyzyzs_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79
iamaniket_titanicsurvivorprediction.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing']",2
imsanjoykb_titanic-data-science-competition.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Save to csv file']",17
inshalkhan_titanic-analysis.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', ' #   Column       Non-Null Count  Dtype  ', ' #   Column       Non-Null Count  Dtype  ', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# import the classification module ', '# setup the environment ', '# compare performance of different classification models', '# evaluate model', '# Random Forest']",13
ioanaandreeaanton_titanic-competition.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.']",8
ishida66_titanic-random-forest-82-78.html,[],0
itachiuchiha10_survival-titanic.html,"['# in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# drop Parch & SibSp', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column', '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# Logistic Regression', '# get Correlation Coefficient for each feature using Logistic Regression', '# Random Forests']",11
jandersonff_titanic-xgboost-jj.html,"['# Correct the age colums', '# plots a bar graph of those who surived vs those who did not.               ', '# convert str features to Number ', '#    print(joined_df[f], le.fit_transform(joined_df[f]))', '#    joined_df[f] = le.fit_transform(joined_df[f])', '    381             # Python 3 and no explicit encoding', '    384             # Python 3 and binary mode']",7
jatinmittal0001_titanic-challenge-ensemble.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# missing value treatment', '# cabin column treatment', ""# now treating 'age' variable"", '# now checking #missing values', '# filling embark missing values with values which occured most', '#', ""# also ticket number won't provide aby valueable information so we can drop that"", '# ONE HOT ENCODING OF CATEGORICAL DATA', '# scaling data', '# fit only to training data i.e. find mean and dev for training data', '# apply those transformtions to x_train and x_test data set', '# it is almost 60%, therefore right now we are not upsampling or downsampling it', '# applying model: XGBOOST', ""# since the size of data is small, we'll now train on final_train_data, and directly predict test_data"", '# trainging data may give more accuracy', '# apply those transformtions to x_train and x_test data set', '    # apply those transformtions to x_train and x_test data set', '\'\\nfrom sklearn.model_selection import KFold\\nkf = KFold(n_splits=5)\\nfor train_index, test_index in kf.split(final_train_data):\\n    #print(""TRAIN:"", train_index, ""TEST:"", test_index)\\n    x_train, x_test = final_train_data.iloc[train_index], final_train_data.iloc[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n    scale = StandardScaler()\\n    scale.fit(x_train)\\n\\n    # apply those transformtions to x_train and x_test data set\\n    x_train = scale.transform(x_train)\\n    x_test = scale.transform(x_test)\\n    model = VotingClassifier(estimators=[(\\\'xgb\\\', xgb_model),(\\\'svc\\\',svc)], voting=\\\'soft\\\')\\n    model = model.fit(x_train,y_train)\\n    y_pred = model.predict(x_test)\\n    print(find_accuracy(y_pred, y_test))\\n\'']",28
jayashreebiswal_titanic-disaster-prediction-logistic-regression.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", ' #   Column       Non-Null Count  Dtype  ', ' #   Column       Non-Null Count  Dtype  ']",11
jimchen65_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
jingnanyang_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory']",7
jmda527_titanic-predict-by-randomforest-jmda.html,"['    456     # Create the parser.', '# final model for predicting survived', '      1 # final model for predicting survived', '    456     # Create the parser.']",4
junyeong00_titanic-surviver-with-cross-validation.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# 1. by using plot', '# 2. by using pivot', '# delete three outlier data', '# submission.to_csv(""./decision_tree.csv"")']",12
karthikgunasekaran_titanic-data-science-solutions.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
kasonli_learning-ml-1-titanic-data.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
kaushikmishra_titanicpredictor.html,"['# of siblings / spouses aboard the Titanic', '# of parents / children aboard the Titanic', '# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', ""# Import the libraries we'll use below."", 'import seaborn as sns  # for nicer plots', 'sns.set(style=""darkgrid"")  # default style', '    #  The baseline is that Nobody survived the tragedy Titanic.', '    #  1502 out of 2224 people died in the tragic event.', '    #  We choose 68% of the people died', '# Change NaN in Ages column to the median values', '#  Change NaN in the Embarked column `Southampton` or `S` which is the heighest in frequency.', ""#  Drop Name table as it is not co-related with one's survival."", '#  Drop the Cabin column beacause it is very sparse.', '#  Drop the ticket number table, since its numeric value does not really mean anything.', '#  Separiting X_train and Y_train', '    # This is not strictly necessary, but each time you build a model, TF adds', '    # new nodes (rather than overwriting), so the colab session can end up', '    # storing lots of copies of the graph when you only care about the most', '    # recent. Also, as there is some randomness built into training with SGD,', '    # setting a random seed ensures that results are the same on each identical', '    # training run.', '    # Build a model using keras.Sequential. While this is intended for neural', '    # networks (which may have multiple layers), we want just a single layer for', '    # logistic regression.', '    # Keras layers can do pre-processing.', '      units=512,                   # number of units/neurons', '      use_bias=True,               # use a bias param', '      activation=""relu"",          # apply the relu function ', '      units=256,                   # number of units/neurons', '      use_bias=True,               # use a bias param', '      activation=""relu"",            # apply the relu function', '      units=128,                   # number of units/neurons', '      use_bias=True,               # use a bias param', '      activation=""relu"",            # apply the relu function', '    # This layer constructs the linear set of parameters for each input feature', '    # (as well as a bias), and applies a sigmoid to the result. The result is', '    # binary logistic regression.', '      units=1,                     # output dim (for binary classification)', '      use_bias=True,               # use a bias param', '      activation=""sigmoid""         # apply the sigmoid function!', '    # Finally, we compile the model. This finalizes the graph for training.', '    # We specify the binary_crossentropy loss (equivalent to log loss).', ""    # Notice that we are including 'binary accuracy' as one of the metrics that we"", '    # ask Tensorflow to report when evaluating the model.', '# Fit the model.', '  x = X_train,   # our binary training examples', '  y = Y_train,   # corresponding binary labels', '  epochs=5,             # number of passes through the training data', '  batch_size=64,        # mini-batch size for SGD', '  validation_split=0.1, # use a fraction of the examples for validation', '  verbose=1             # display some progress output during training', '#  Trying to find outliers', ""#  Drop Name table as it is not co-related with one's survival."", '#  Drop the Cabin column beacause it is very sparse.', '#  Drop the ticket number table, since its numeric value does not really mean anything.', '# Change NaN in Ages column to the median values', '#  Change NaN in the Embarked column `Southampton` or `S` which is the heighest in frequency.', '      units=64,                                   # number of units/neurons', '      use_bias=True,                              # use a bias param', '      activation=""relu"",                          # apply the relu function', '      kernel_regularizer=regularizers.l2(0.01)    # addred L2 regularisaiton', '#  Apply one-hot encoding for `Sex` and `Embarked` feature']",69
keenank_titanic-data-science.html,[],0
keilorgilbert_titanic-initial-exploration-of-data.html,[' #   Column       Non-Null Count  Dtype  '],1
kerneler_starter-survivedtitanic-29986389-6.html,"['import matplotlib.pyplot as plt # plotting', 'import numpy as np # linear algebra', 'import os # accessing directory structure', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Distribution graphs (histogram/bar graph) of column data', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values', '# Correlation matrix', ""    df = df.dropna('columns') # drop columns with NaN"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '# Scatter and density plots', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns', '    # Remove rows and columns that would lead to df being singular', '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots', ""nRowsRead = 1000 # specify 'None' if want to read whole file"", '# survivedTitanic.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows']",16
kerneler_starter-titanic-07c21d6b-d.html,"['import matplotlib.pyplot as plt # plotting', 'import numpy as np # linear algebra', 'import os # accessing directory structure', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Distribution graphs (histogram/bar graph) of column data', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values', '# Correlation matrix', ""    df = df.dropna('columns') # drop columns with NaN"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '# Scatter and density plots', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns', '    # Remove rows and columns that would lead to df being singular', '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots', ""nRowsRead = 1000 # specify 'None' if want to read whole file"", '# titanic.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows']",16
kerneler_starter-titanic-all-zeros-csv-file-eb06f941-e.html,"['import matplotlib.pyplot as plt # plotting', 'import numpy as np # linear algebra', 'import os # accessing directory structure', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Distribution graphs (histogram/bar graph) of column data', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values', '# Correlation matrix', ""    df = df.dropna('columns') # drop columns with NaN"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '# Scatter and density plots', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns', '    # Remove rows and columns that would lead to df being singular', '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots', ""nRowsRead = 1000 # specify 'None' if want to read whole file"", '# all_0s.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows']",16
keweishang_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
kitushan_titanic-data-science-solutions-dc3035.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",25
kleytontorikai_titanic-survivors.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# Read the data', '# Select categorical columns with relatively low cardinality (arbitrary)', '# Select numeric columns', '# Keep selected columns only', '# One-hot encode the data', ""# Can't just drop NaN rows because the test set might contain NaN values"", '# Feature Scaling', '# The Euclidean distance is the minkowski metric with p=2.', '# Applying k-Fold Cross Validation', '# Read test set for submission', '# Apply the same transformations on X_sub as in X_train', '# One-hot encode', '# Select the same columns except Survived', '# Replace NaN for -1', '# Feature Scaling']",23
kmisiunas_titanic-classification-using-decision-treed.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Baseline models', '# load data', '    # to do', '# simple decision tree', '# results on test set: 0.70813', '# results on test set: ', '# Random forrests ']",15
kobezorro_titanic-my-try.html,"['# ml', '# visualization', '# Load the data', '# feature processing methods', '    # cut into categories', '    # bins = (0, 8, 15, 31, 1000)', ""    # group_names = ['1_quartile', '2_quartile', '3_quartile', '4_quartile']"", ""    # df['Fare'] = pd.cut(df.Fare, bins, labels=group_names)"", ""    # fare_map = {'1_quartile': 1, '2_quartile': 2, '3_quartile': 3, '4_quartile': 4}"", ""    # df['Fare'] = df['Fare'].map(fare_map)"", '            # Convert random age float to nearest .5 age', '    # cut into categories', '    # bins = (0, 5, 12, 18, 25, 35, 60, 100)', ""    # group_names = ['Baby', 'Child', 'Teenager', 'Student', 'Young', 'Adult', 'Senior']"", ""    # df['Age'] = pd.cut(df.Age, bins, labels=group_names)"", ""    # age_map = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young': 5, 'Adult': 6, 'Senior': 7}"", ""    # df['Age'] = df['Age'].map(age_map)"", '# train & predict', '# train and test data', '# feature selection', '# xgboost', '# semi-supervised with xgboost', 'semi_train_df = semi_train_df.sample(frac=1).reset_index(drop=True)  # shuffle', '# random forest', '# adaboost', '# stacking']",26
kvinaya_titanic-survived-count.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
leapleap_titanic-analysis-heavy-on-the-data-cleaning.html,"['# make pandas window less tiny', '# for my own sanity temporarily:', ""pd.options.mode.chained_assignment = None # None|'warn'|'raise'"", '# and the test data', ""# have a look at counts, in this case we're just using 'HasCabin'"", ""# on y axis as it's a nice column full of 1's."", '# do the same thing to test before i forget...']",7
lemoinef_titanic-data-exploration-and-visualization.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '    # convert value into number or strip excess quotes if string', '    # get booleans for filtering', '    else: # catch invalid operation codes', '    # filter data and outcomes', '    # Check that the key exists', ""    # Return the function before visualizing if 'Cabin' or 'Ticket'"", '    # is selected: too many unique categories to display', '    # Merge data and outcomes into single dataframe', '    # Apply filters to data', '    # Create outcomes DataFrame', '    # Create plotting figure', ""    # 'Numerical' features"", '        # Remove NaN values from Age data', '        # Divide the range of data into bins and count survival rates', ""        # 'Fares' has larger range of values than 'Age' so create more bins"", ""        # Overlay each bin's survival rates"", '        # Add legend to plot', ""    # 'Categorical' features"", '        # Set the various categories', '        # Create DataFrame containing categories and count of each', '        # Set the width of each bar', ""        # Display each category's survival rates"", '    # Common attributes for plot formatting', '    # Report number of passengers with missing values', '# Women between the age of 40 and 50, in third class, all died. ', '# All women in second class survived, whatever there age.', '# Most women in first class survived', '# Most men die, unless they are 10 years old or less', '# We might say that men yougner than 10 years old survived.', '# Men in first class that are less than 40 years old are more likely to survive. ', '# But not always true, we see some red between 10 and 20 years old. 1 person died. ']",39
lhavanya_titanic-analysis.html,[],0
liyanageskydisc_starter-titanic-0cbe8c70-9.html,"['import matplotlib.pyplot as plt # plotting', 'import numpy as np # linear algebra', 'import os # accessing directory structure', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Distribution graphs (histogram/bar graph) of column data', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values', '# Correlation matrix', ""    df = df.dropna('columns') # drop columns with NaN"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '# Scatter and density plots', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns', '    # Remove rows and columns that would lead to df being singular', '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots', ""nRowsRead = 1000 # specify 'None' if want to read whole file"", '# train_and_test2.csv has 1309 rows in reality, but we are only loading/previewing the first 1000 rows']",16
liyaoo_titanic-decisiontree.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.']",8
lobodemonte_titanic-who-d-survive.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", ""# Let's check for missing values, we will have to address these missing values later "", ""# Let's first check the easier label columns "", '# We can see that Pclass, Sex, and Embarking Port were very important', '# for determining survival ', ""# Now let's look at numeric columns, but first we must cut them"", ""# print(train_raw[['Survived','age_range','Pclass']].groupby(['age_range','Pclass']).mean())"", ""del train_raw['age_range'] # let's clean that up "", '# Looks like being young is an advantage to survive the titanic ', '# What about your family size?', '# Small families had higher survival rates, followed by solo travelers, and finally large families ', '# What about Fare paid?', '# Of course the more you paid, the richer you were, the more likely you survived', ""# For cabin, let's separate the number and letter"", '# We see that D, E, B cabins had higher survival rates, and those with missing cabins had the lowest', '# But what about cabin numbers?', '# Looks like certain cabin numbers also had higher survival rates', ""# Let's do something similar to ticket"", '# Does the ticket length tell us anything?', ""# Let's examine the info we can extract from Name"", '# We can get the title from Name', '# We can see certain nobility titles seem to have way better odds of survival', '# What about how long the name is?', '# We see that long names have higher survival rates, maybe people with longer names were also richer? ', '# These functions transform our columns into the final form we need them in ', 'cols = list(train.columns[2:]) # features relevant to our ML model', '# We will use these functions to select the best features from our data', '        lr = ensemble.RandomForestClassifier() # NOTE: using different classifier yields diff results ', ""# We'll use this function to save our results to CSV"", '# We use this function to tune our model', '# save_results(gbc_tuned, test[cols]) # Uncomment whichever model you want to use', '#     ""min_samples_split"" : [2, 4, 10, 12, 16],', '#     ""n_estimators"": n_estimators,', '# Best score:  0.8293955181721173', ""# Best params:  {'criterion': 'gini', 'max_leaf_nodes': 14, 'min_samples_leaf': 1}"", '# IS Score:  0.856341189674523', '# save_results(log_tuned, test[cols])']",45
loremipsuma_titanic-eda.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", ' #   Column       Non-Null Count  Dtype  ', '    # get feature', '    # count number of categorical varible(value/sample)', '# Pclass - Survived', '# Sex - Survived', '# Sibsp - Survived', '# Parch - Survived', '        # 1st quartile', '        # 3rd quartile', '        # IQR', '        # Outlier step', '        # detect outlier and their indeces', '        # store indeces', '# find outliers', '# drop outliers', '# concat datasets', '# which columns ', '# ', '# convert to categorical']",28
luke3d_titanic-random-forest-82-78.html,[],0
lukego_titanic-data-preprocess.html,"['# æŸ¥çœ‹æ•°æ®ä¸\xadç¼ºå¤±å€¼çš„æ¯”ä¾‹', '# æ•°å€¼ç±»åž‹ç‰¹å¾ç®±çº¿å›¾', '# æ•°å€¼ç±»åž‹ç‰¹å¾æ¡å½¢å›¾', '# æ˜Žæ˜¾æœ‰ååˆ†å¸ƒï¼Œè¿›è¡Œå–logå¯¹æ•°å˜æ¢', '# ç¦»æ•£ç‰¹å¾åˆ†å¸ƒ', '# å¯¹idç±»åž‹æ—\xa0æ³•ç›´æŽ¥å¯è§†åŒ–ï¼Œå…ˆæš‚æ—¶è·³è¿‡', '# è¿›ä¸€æ\xad¥çœ‹çœ‹ticketå–å€¼ä¸Žå¯¹åº”é¢‘æ•°', '# å¯¹idç±»åž‹å¦‚å–å€¼å¤ªå¤šæ—\xa0æ³•ç›´æŽ¥å¯è§†åŒ–ï¼Œé€šå¸¸æŒ‰é¢‘çŽ‡ä»Žé«˜åˆ°ä½ŽæŽ’åºåŽå¯è§†åŒ–', '# æŸ¥çœ‹ç‰¹å¾åœ¨æ\xad£è´Ÿæ\xa0·æœ¬ä¸Šçš„åˆ†å¸ƒ', '# æ•°å€¼ç±»åž‹ç‰¹å¾æŸ¥çœ‹åˆ†å¸ƒ', '# æ‰“å°pearsonç³»æ•°', '# åˆ†ç±»å˜é‡éœ€è¦å…ˆencodeæˆæ•°å€¼åž‹æ‰èƒ½æ±‚ç›¸å…³æ€§', '# æ•´ä½“æ¥çœ‹ç‰¹å¾ä¸Žlabelä¹‹é—´çš„ç›¸å…³æ€§', '# å…ˆçœ‹æ–‡æœ¬ç±»çš„ç‰¹å¾çš„ç»Ÿè®¡è¯é¢‘æƒ…å†µï¼Œæœ‰ä¸ªåˆæ\xad¥äº†è§£', '# å¯ä»¥è€ƒè™‘æŠŠå§“åä¸\xadçš„ç§°è°“æå–åšç‰¹å¾æ•°æ®', '# cabinæ•°æ®å¯çŸ¥å…¶é¦–å\xad—æ¯ä»£è¡¨å®¢èˆ±ç±»åˆ«ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶é¦–å\xad—æ¯æå‡º']",16
lystahi_titanic-data-lightgbm.html,"[""sample_submission.to_csv('submission.csv', index=False)  # scoreã¯0.77511""]",1
maheshsk_titanic-survival-prediction-end-to-end-ml-pipeline.html,"['# We can use the pandas library in python to read in the csv file.', '# This creates a pandas dataframe and assigns it to the titanic variable.', '# Print the first 5 rows of the dataframe.', ' # plots an axis lable', '# sets our legend for our graph.', 'titanic[""Deck""].unique() # 0 is for null values', 'titanic[""Deck""].unique() # Z is for null values', '# Create a family size variable including the passenger themselves', '# Discretize family size', '# The .apply method generates a new series', '    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.', '# Titles with very low cell counts to be combined to ""rare"" level', '# Also reassign mlle, ms, and mme accordingly', '#                \'Dr\', \'Major\', \'Rev\', \'Sir\', \'Jonkheer\']), ""Title""] = \'Rare Title\'', '# Titles with very low cell counts to be combined to ""rare"" level', '# Also reassign mlle, ms, and mme accordingly', '    # Split sets into train and test', '    # All age values are stored in a target array', '    # All the other values are stored in the feature array', '    # Create and fit a model', '    # Use the fitted model to predict the missing values', '    # Assign those predictions to the full data set', '# Import the linear regression class', '# Sklearn also has a helper that makes it easy to do cross validation', ""# The columns we'll use to predict the target"", '# Initialize our algorithm class', '# Generate cross validation folds for the titanic dataset.  It return the row indices corresponding to train and test.', '# We set random_state to ensure we get the same splits every time we run this.', ""    # The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds."", ""    # The target we're using to train the algorithm."", '    # Training the algorithm using the predictors and target.', '    # We can now make predictions on the test fold', '# Map predictions to outcomes (only possible outcomes are 1 and 0)', '# Initialize our algorithm', '# Compute the accuracy score for all the cross validation folds.', '# Take the mean of the scores (because we have one for each fold)', '# Initialize our algorithm with the default paramters', '# n_estimators is the number of trees we want to make', '# min_samples_split is the minimum number of rows we need to make a split', '# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)', '# Take the mean of the scores (because we have one for each fold)', '# Take the mean of the scores (because we have one for each fold)', ' #             ""FsizeD"", ""Embarked"", ""NlengthD"",""Deck"",""TicketNumber""]', '# Perform feature selection', '# Get the raw p-values for each feature, and transform from p-values into scores', '# Initialize our algorithm', '# Compute the accuracy score for all the cross validation folds.  ']",47
mahsahassankashi_predict-titanic-survival-accuracy-0-83.html,"['# -*- coding: utf-8 -*-', 'from sklearn.cross_validation import StratifiedKFold # Add important libs', ""with open(path3, 'w',  newline='') as f3, open(path2, 'r') as f4: # write output and other column from test""]",3
maimahdi_titanicrf.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
mamunalbd4_mamun-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.']",8
manishshah120_titanic-kaggle-competition.html,"['# # Pclass Column', ""# train_dataset = train_dataset.astype({'Pclass': 'object'})"", ""# train_dataset['Pclass']= label_encoder.fit_transform(train_dataset['Pclass'])"", ""# checktestDataset = test_dataset[test_dataset['Age'] >= 60]"", '# checktestDataset.head(20)', ""# checktestDataset = test_dataset[test_dataset['Fare'].isnull()==True]"", '# checktestDataset', '# # Pclass Column', ""# train_dataset = train_dataset.astype({'Pclass': 'object'})"", ""# train_dataset['Pclass']= label_encoder.fit_transform(train_dataset['Pclass'])"", ""# test_df = pd.read_csv('/content/test.csv')"", '# submission = pd.DataFrame({', ""#                             'PassengerId': test_df['PassengerId'],"", ""#                             'Survived': y_pred_SVC"", '#                           })', ""# submission.to_csv('prediction_without_Ensemble2.csv', index = False)"", ""# print('Done Saving')"", '# from statistics import mode', '# final_pred_Max_Voting = np.array([])', '# for i in range(0, len(test_dataset)):', '#     final_pred_Max_Voting = np.append(final_pred_Max_Voting, mode([y_pred_Logistic[i], y_pred_KNN[i], y_pred_RF[i], y_pred_SVC[i], y_pred_SVC[i]]))', ""# test_df = pd.read_csv('/content/test.csv')"", '# submission = pd.DataFrame({', ""#                             'PassengerId': test_df['PassengerId'],"", ""#                             'Survived': final_pred_Max_Voting"", '#                           })', ""# submission.to_csv('prediction_with_Ensemble_Max_Voting3.csv', index = False)"", ""# print('Done Saving')""]",28
manjureddygl_titanic-prediction.html,"['# Importing Classifier Modules', 'import matplotlib.pyplot as plt # Plot the graphes', 'sns.set() # setting seaborn default for plots', '# delete unnecessary feature from dataset', '# fill missing Fare with median fare for each Pclass', '# fill missing Fare with median fare for each Pclass', '# machine learning', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Decision Tree', '# Random Forest']",12
mantej_titanic-waley-bhaiya.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# Filling the missing values in Embarked with S bcoz 78.3 is very close to 80']",9
mattleedev_titanic-lunch-and-learn.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# inline plotting', ""# machine learning (we'll talk more about these later!)"", '# get titanic & test csv files as a DataFrame', '# preview the data', ""# Let's look at the columns (nomenclature: features, classes)"", ""# Let's get some aggregate data quickly"", '# checking if there is any missing values in the data sets', '# Same thing for the test data', '# Visual tools can help us get a better sense of the data', ""# First let's look at where people boarded from"", ""# Next let's look at where survivors boarded from"", ""# Finally let's look at your chances of surviving depending on where you boarded"", '# What to do about those pesky', '# null values we found in our aggregate exploration?', '# How do we look now?', ""# Let's take a look at the total numbers"", ""# Let's look at those that survived"", ""# Finally let's look at your chances of surviving depending on where your age"", ""# Let's fill in the null values here aswell.  This can be tricky.  "", '# There is no right way to do this.  ', ""# Get average,std, and count of NaN's in train_df"", ""# Get average,std, and count of NaN's in test_df"", '# generate random numbers between (mean - std) & (mean + std)', '# We can now fill in our NaN values using these randomly generated values', '# Convert from float to int to make things ', '# How do we look now?', ""# Let's now look at what our data looks like after this"", ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", ""# First let's look at where people boarded from"", ""# Next let's look at where survivors boarded from"", ""# Finally let's look at your chances of surviving depending on where you boarded"", ""# First let's look at where people boarded from"", ""# We'll first create percentil bins and group all our fares into one of these 20 "", '# percentile bins.', ""# Next let's look at where survivors boarded from"", ""# Finally let's look at your chances of surviving depending on where you boarded"", '# There was one sneaky NaN ""Fare"" in the test data.  Let\'s fill this using the median value', '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# Plots', '# Total number of individuals with family on board', '# Number of survivors based on whether or not they had family on board', '# Average number of survivors based on having family on board', '# As we saw previously, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child in order to seperate the men ', ""# from the boys.  Unfortunately, men don't traditionally fair so well in maritime disasters"", '# Here we can create our own custom function', '# Can you figure out what this function does?', '# Let\'s create a new feature called ""Person""', '# No need to use Sex column since we created Person column', '# Create dummy variables for our ""Person"" feature. The goal is to create 2 new', '# features ""Child"" and ""Female"" that we will ultimately use to train our model.', '# These features will only output a 1 if that person falls into this category, and', '# a 0 otherwise.  We don\'t need the ""Male"" feature since 2 zeros in both ""Child"" and', '# ""Female"" will be enough information to indicate maleness.  ', '# We can now add these features using the .join() method', ""# Let's plot these now"", '# Total number of individuals with family on board', '# Number of survivors based on whether or not they had family on board', '# Average number of survivors based on having family on board', ""# Let's see what our data looks like now."", '# We can now drop the features that we don\'t find very useful like ""Name"", ""Ticket"", and', '# ""Person"" (since this information is captured in ""Child"" and ""Female"")', ""# Let's see where we are at"", '# It might be nice to ""one-hot-encode"" our ""Pclass"" feature.  ', '# Question: Why might we want to do this?', ""# We'll make sure of SK-Learns binarizer"", '# Since we don\'t need ""Pclass"" anymore, let\'s drop it', '# Check to see what our data looks like here', '# Up to you to decide what to do with ""Embarked"" feature. ', '# Either keep it and ""one-hot"" it, or drop it.  We drop it out of laziness.', '# One last thing that we might want to do is scale our data.  ', '# Why might this be desirable?  What can go wrong otherwise?', '# What can go wrong doing this?', '# Ok time to exchange our old un-scaled values out for our newly scaled data', ""# Finally, as a last sanity check, let's take a look at our data a final time."", '# As a first step we split the data into our training features (variables we wish to ', '# use to train our model), and training labels (survived or not).', ""# We do the same thing with the test data, but of course are missing our labels (that's"", '# what we wish to predict!)', '# Question: why use .copy()?', '# How can we quickly assess the quality of our models without having to submit our ', ""# results every time to Kaggle and wait for their response?  Let's further split"", '# our X_train and Y_train into a 70%-30% split.  This way we can train on 70%', '# chunk and then test it on the 30% chunk.  This 30% chunk is known as the ""validation set"".', '# Your code here:', '# Finally we can submit our results', '# submission = pd.DataFrame({', '#         ""PassengerId"": test_df[""PassengerId""],', '#         ""Survived"": Y_predict', '#     })', ""# submission.to_csv('titanic.csv', index=False)""]",97
maxsantos_titanic-notebook.html,"['# Impute missing values for age in training set', ""# Create Child column in training set ('Feature Engineering')"", '# Create Family Size column for training set', '# Simplify Cabin column, by slicing off numbers', ""# NaN Cabin values labelled as 'N'"", '# Visualise new features', '# Child plot', '# Famliy_Size plot', '# Cabin plot', '# Convert sex to integer values in training set', '# Convert embarked to integer values, and impute missing values', '# Create feature and target arrays', '# Split into training and test set', '# Create random forest classifier', '# Choose some parameter combinations to try (these values were borrowed from another user)', '# Type of scoring used to compare parameter combinations', '# Run the grid search with 10-fold cross-validation', '# Set the classifier to the best combination of parameters', '# Fit the best algorithm to the data, and print feature importances & prediction score', '# Print confusion matrix, showing actual numbers of correct and incorrect predictions', '# Accuracy on test set (diagonal divided by total in confusion matrix)', '# Print classification report, showing precision and recall calculated from confusion matrix', '# high precision = a low rate of incorrect survival predictions', '# high recall = predicted a large number of survivals correctly', '# Fit logreg to Kaggle test data', '# Create submission file', '# Fit knn to kaggle test data', '# Create submission file']",28
maxwellgreene_python-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
mayurrathod1124_titanic-project.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
michaelhayden_xgboosttitanic92.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', 'train_test_data = [train, test] # combining train and test dataset', '# make it numeric', '# notice there is not any more nan values']",11
mihoku91_ensemble-learning-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '  # This is added back by InteractiveShellApp.init_path()']",9
mikeaalv_titanic-competition.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'from sklearn.model_selection import KFold, GridSearchCV # cv and parameter search', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# train_data.head()', '# test_data.head()', '# check nan value', '# type(allx)', '# allx.isnull().sum()', ""# np.flatnonzero(allx['Fare'].isnull()==True)"", '# use median in train and test data set to replace nan', '# deal with the only nan in Fare', '# allx.head()', '# # parameter searching? 6 100', '# gbm=xgb.XGBClassifier()', ""# clf = GridSearchCV(gbm,{'max_depth': [2,4,6],'n_estimators': [50,100,200,500]},verbose=1)"", '# clf.fit(trainvalid_x,trainvalid_y[:,0])', '# print(clf.best_score_)', '# print(clf.best_params_)', '#     print(confusion_matrix(actuals,predictions))']",25
milanlamichhane_titanic-ml.html,[],0
mirandora_titanic-tutorial-code.html,[],0
monthepp_titanic-machine-learning-from-disaster.html,"['# import python standard library', '# import data manipulation library', '# import data visualization library', '# import sklearn model class', '# import sklearn model selection', '# import sklearn model evaluation classification metrics', '# acquiring training and testing data', '# visualize head of the training data', '# visualize tail of the testing data', '# combine training and testing dataframe', '# describe training and testing data', '# convert dtypes numeric to object', '# list all features type number', '# list all features type object', '# feature extraction: surname', '# feature extraction: title', '# feature exploration: sex', '# feature exploration: age', '# feature extraction: age', '# feature extraction: family size', '# feature extraction: ticket string', '# feature extraction: has ticket string', '# feature exploration: fare', '# feature extraction: fare', '# feature extraction: cabin', '# feature extraction: cabin string', '# feature extraction: has cabin', '# feature exploration: embarked', '# feature extraction: embarked', '# list all features type number', '# list all features type object', '# feature exploration: survived', '# feature exploration: survived where family size equal to 1', '# feature exploration: survived where family size more than 1', '# feature extraction: ticket dataframe', '# describe ticket dataframe', '# convert dtypes numeric to object', '# convert dtypes object to numeric', '# feature extraction: together', '# feature exploration: survived', '# feature extraction: with sex and title', '# feature extraction: ticket_self dataframe', '# feature extraction: survived peer', '# feature extraction: ticket_title dataframe', '# feature extraction: survived peer title', '# feature exploration: survived peer and with sex and title', '# feature extraction: survived', '# convert category codes for data dataframe', '# convert dtypes object to numeric for data dataframe', '# describe data dataframe', '# verify dtypes object', '# compute pairwise correlation of columns, excluding NA/null values and present through heat map', '# select all features to evaluate the feature importances', '# set up random forest classifier to find the feature importances', '# plot the feature importances', '# list feature importances', '# select the important features', '# perform train-test (validate) split', '# logistic regression model setup', '# logistic regression model fit', '# logistic regression model prediction', '# logistic regression model metrics', '# decision tree classifier model setup', '# decision tree classifier model fit', '# decision tree classifier model prediction', '# decision tree classifier model metrics', '# random forest classifier model setup', '# random forest classifier model fit', '# random forest classifier model prediction', '# random forest classifier model metrics', '# specify the hyperparameter space', '# random forest classifier grid search model setup', '# random forest classifier grid search model fit', '# random forest classifier grid search model prediction', '# random forest classifier grid search model metrics', '# model selection', '# prepare testing data and compute the observed value', '# submit the results']",78
moosecat_titanic-visualisations-plotly-and-d3.html,"['# data analysis and wrangling', '# visualization', '# plotly', '# machine learning', 'df = train_df # as we alter train_df later', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', '# import graph objects as ""go""', ""#                            filename='distplot with pandas')"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", '# colours', ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# rules defined in the tree object clf', '    if clf.tree_.children_left[node_index] == -1:  # indicates leaf', '        #                          for count, label in count_labels))', '# Random Forest', ""# submission.to_csv('../output/submission.csv', index=False)""]",33
mosleylm_titanic-data-set-exploration.html,"[""training_data['Family'] = training_data['SibSp'] + training_data['Parch'] + 1 # add 1 for 'self'"", ""test_data['Family'] = test_data['SibSp'] + test_data['Parch'] + 1 # add 1 for 'self'   "", ""#                                                        'Don', 'Jonkheer', 'Lady', 'Sir', 'Dr', 'Rev',"", ""#                                                        'Major'], 'Other')"", ""#                                                        'Don', 'Jonkheer', 'Lady', 'Sir', 'Dr', 'Rev',"", ""#                                                        'Major'], 'Other')"", '# drop unused data columns', '# Normalize the data', ""# these are the features we'll use to classify"", '# intialize f1 and accuracy values', '        # add splits f1, acc', '    f1s[cls] = f1s[cls] / 10.0 # average over ten runs', '    acc[cls] = acc[cls] / 10.0 # average over ten runs']",13
mrhippo_titanic-prediction-and-analysis-with-dsh.html,"['!pip install datasciencehelper # pip install Data Science Helper', ' #   Column    Non-Null Count  Dtype  ', ' #   Column  Non-Null Count  Dtype  ', ' #   Column    Non-Null Count  Dtype  ', '# converting features to categorical', ""test_ID = test['PassengerId'] # save PassengerId (for submission)"", ' #   Column    Non-Null Count  Dtype  ', ' #   Column  Non-Null Count  Dtype  ', ' #   Column  Non-Null Count  Dtype  ', ' #   Column    Non-Null Count  Dtype  ', 'all_data[""Sex""] = [1 if each == ""male"" else 0 for each in all_data[""Sex""]] # male -> 1, female -> 0']",11
mukultiwari_titanic-2.html,"['# Imports', '# Importing Dataset', '# Saving the passengerId of test data for later use.', '# Since passengerId does not have significant contribution to survival directly therefore we will Drop it.', '# Preprocessing and Feature Egineering', '    # Title Feature', '    # Name Leangth', '    # Dropping the name feature ', '    # Categorizing the name length by simply dividing it with 10.', '    # Taking care of null values in Age ', '    # We will create a new feature of family size = SibSp + Parch + 1', '    # wheather or not the passenger was alone ?', '    # Replacing Null values of Fare with Mean', '    # Categorizing the fare value by dividing it with 20 simply', '    # Making a new feature hasCabin which is 1 if cabin is available else 0', '    # Since ""S"" is the most frequent class constituting 72% of the total therefore we will replace null values with ""S""', '# Cleaning Data for Classification', '# Resolving the categorical data for training set', '# Splitting the dataset into training and test set', '# For Submission Use whole Data for training', '# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)', '# Feature Scaling', '# Non_scaled_x_train = X_train', '# Non_scaled_x_test = X_test', '# scaler_x = MinMaxScaler((-1,1))', '# X_train = scaler_x.fit_transform(X_train)', '# X_test = scaler_x.transform(X_test)', '# For Submission consider whole data', '# Feature Scaling', '# Hyper Parameter tuned', '# XGBoost', '# Hyper Parameter tuned', '# Logistic Regression', '# Hyper Parameter tuned', '# Logistic Regression', '# Hyper Parameter tuned', '# KNN', '# Hyper Parameter tuned', '# Random Forest', '# Ensemble of Models Hard Voting', '#     yTest = np.array([])', '#     for i in range(0,len(df)):', '#         yTest = np.append(yTest, mode([xgb_pred[i], lr_pred[i], K_svm_pred[i], knn_pred[i], rf_pred[i]]))', '#     return yTest.astype(int)', '# from sklearn.metrics import classification_report', '# # classification report for precision, recall f1-score and accuracy', '# matrix = classification_report(yTest,y_test,labels=[1,0])', ""# print('Classification report : \\n',matrix)"", '# Making Submission', '# Preparing test data ', '# Taking care of categorical data', '# Feature Scaling', '# y_pred = rf.predict(titanic_test)']",53
mukultiwari_titanic-top-14-with-random-forest.html,"['# Viewing the data', '# Information on the dataset', '# We have 7 Numeric (PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare)', '# and 5 categorical Features (Name, Sex, Ticket, Cabin, Embarked)', '# Training Data contains 866 null values ( Age-177, Cabin-687, Emarked-2)', '# Test Data contains 414 null values ( Age-86, Fare-1, Cabin-327)', '# Saving the passengerId of test data for later use.', '# Since passengerId does not have significant contribution to survival directly therefore we will Drop it.', '# As seen the population of passengers as per Pclass is ( 3 > 1 > 2) ', '# Survival percentage as per classes is ( 1 > 2 > 3)', '# Inference: 1st class passengers have higher survival rate ', '# Making a new feature Title having only the title extracted from the first name', '# Making a new feature nameLength telling the length of the name', '# Title Feature', '# Name Leangth', '# Dropping the name feature ', '# Categorizing the name length by simply dividing it with 10.', '# Observations', '# Name length as seen the longer the name the higher is the survival.', '# Titles like Mrs. Ms. the lady or any royalty have high survival rates.', '# As can be seen that (number of males > number of females) but Survival ratio is inverse', '# More females survived as compared to males', '# Creating a list of age values without null values', '# Age contains 177 null values in training set and 86 in test set', '# Creating a new list of survived and dead', '# Taking care of null values in Age ', '# Null Ages in Training set (177 null values)', '# Null Ages in Test set (86 null values)', '# Observations:', '# Maximum passengers have age between 20-40 years', '# Survival rate is maximum for childrens and elderly', '# We will create a new feature of family size = SibSp + Parch + 1', '# As observed maximum passengers are alone but the survival is maximum for the family of 4', '# wheather or not the passenger was alone ?', '# Observations:', '# Making a new feature ticket length', '# Having ticket length may or may not increase acuracy as its not significant, in my case it did increase accuracy.', '# Fare has 0 null values in training data but 1 null values in test data', '# mean of fare in test data is 35 we will replace nul value with mean', '# Creating a new list of survived and dead', '# Categorizing the fare value by dividing it with 20 simply', '# Observations:', '# The most frequent fare is between 0-100', '# The survival rate is directly praportional to rate i.e. higher the rate higher the survival chances.', '# Null values in test data', '# Null values in training data', '# Making a new feature hasCabin which is 1 if cabin is available else 0', '# As observed maximum population on titanic dataset does not have cabin but survival for having cabin is more.', '# Embarked has 2 null values in the training data', '# Since ""S"" is the most frequent class constituting 72% of the total therefore we will replace null values with ""S""', '# Observations:', '# The maximum passengers are from Southampton', '# The maximum survival rate is of the passengers who boarded from Cherbourg', '# If we observe the fare as grouped by boarding ststion ', '# we observe that the most premium customers boarded from Cherbourg therefore maximum survival rate .', '# Splitting the dataset into dependent and independent features', '# Resolving the categorical data for training set', '# Splitting the dataset into training and test set', '# Feature Scaling', '# Making a list of accuracies', '# As observed Xgboost performs best.', '# We will be making three submissions', '# Random Forest', '# K-Svm', '# Xgboost', '# Since Random Forest scores best after submission we will apply Grid Search CV on RF', '# Preparing test data ', '# Taking care of categorical data', '# Feature Scaling']",69
multiw_exploring-the-titanic-dataset-in-progress.html,"['import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Import csv data as DataFrame objects.', '# Display the first 5 samples (passengers) of our dataset.', '# of siblings / spouses aboard the Titanic', '# of parents / children aboard the Titanic', '# Display the last 5 samples of our dataset.']",6
musonda2day_titanic-prediction.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# Number of trees in random forest', '# Number of features to consider at every split', '# Maximum number of levels in tree', '# Minimum number of samples required to split a node', '# Minimum number of samples required at each leaf node', '# Method of selecting samples for training each tree', '# Create the random grid', '# Use the random grid to search for best hyperparameters', '# First create the base model to tune', '# Random search of parameters, using 3 fold cross validation, ', '# search across 100 different combinations, and use all available cores']",19
myndel_titanic-machine-learning-from-disaster.html,"['#    Survived    Survived                   0 = No, 1 = Yes', '#    Pclass      Ticket class               1 = 1st, 2 = 2nd, 3 = 3rd', '#    Name        Name', '#    Sex         Sex', '#    Age         Age in years', '#    SibSp       # of siblings / spouses aboard the Titanic', '#    Parch       # of parents / children aboard the Titanic', '#    Ticket      Ticket number', '#    Fare        Passenger fare', '#    Cabin       Cabin number', '#    Embarked    Port of Embarkation        C = Cherbourg, Q = Queenstown, S = Southampton', '# Columns with missing values', ""    # 72% of Embarked values are 'S' which stands for Southampton"", '    # so in my opinion we can fill missing data with this value.', '    # Age we will fill with mean values for age', '    # Now we have to transform alphabetic values to numberic', ""# Let's see how our data looks like"", '# Train the best model', '# Check for empty values', '# I will fill NaN values with mean']",20
mzr2017_titanic-best-working-classifier.html,"['6. Age¶we have plenty of missing values in this feature. # generate random numbers between (mean - std) and (mean + std).', '    # Mapping Sex', '    # Mapping titles', '    # Mapping Embarked', '    # Mapping Fare', '    # Mapping Age', '# Feature Selection']",7
naimur978_titanic-who-s-gonna-survive-public-score-81.html,"['# machine learning models', '# preprocessing functions and evaluation models', ""# Let's be rebels and ignore warnings for now"", '# console ascii color code, not necessarry to use', ' #   Column       Non-Null Count  Dtype  ', ' #   Column       Non-Null Count  Dtype  ', ' #   Column       Non-Null Count  Dtype  ', '# loop through all columns to see if there are any outliers', '# Create dummy classifer', '# train the model', '# Get accuracy score', '# create scaler', '# apply normalization to training set and transform training set', '# transform validation set', '# function to train a given model, generate predictions, and return accuracy score', '# create model apply fit_evaluate_model', '# create model apply fit_evaluate_model', '# create model apply fit_evaluate_model', '# create model apply fit_evaluate_model', '# create model apply fit_evaluate_model', '# create model apply fit_evaluate_model', '# create dataframe of accuracy and model and sort values', '# The number of trees in the forest algorithm, default value is 10.', '# The minimum number of samples required to split an internal node, default value is 2.', '# The minimum number of samples required to be at a leaf node, default value is 1.', '# The number of features to consider when looking for the best split, default value is auto.', '# Define the grid of hyperparameters to search', '# create model', '# create Randomized search object', '# Fit on the all training data using random search object', '# Create a range of trees to evaluate', '# 100, 200, 300, 400, 500, 600, 700', '# 50, 100, 150, 200, 250, 300, 350', '# define all parameters except n_estimators', '# Grid Search Object using the trees range, the model and 5-fold cross validation', '# fit the dataset to grid search object', '# Get the results into a dataframe', '# Plot the training and testing error vs number of trees', '# set title, labels and legend', '# Feature Importance', '# Plot the feature importance scores']",41
nazimonderorhan_getting-started-with-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
neilslab_titanic-voting-ensemble.html,"['    # Mapping Sex', '    # Mapping titles', '    # Mapping Embarked', '    # Mapping Fare', '    # Mapping Age', '# Feature Selection']",6
nico33_titanic-data-science-solutions.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest']",24
nikanth_titanic-data-science-solutions.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', '# submission.head()']",25
niranjana55_titanic-prb.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Varies with sklearn version', '# Load the data set', ""# Remove the fields from the data set that we don't want to include in our model"", '# Replace categorical data with one-hot encoded data', '# Remove the from the feature data', '# Create the X and y arrays', '# Split the data set in a training set (70%) and a test set (30%)', '    439     # Create the parser.']",16
nitinkaushik1978_titanic-basic-solution-with-logistic-regression.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
nlprunner_titanic-practice-with-scikit-learn.html,[],0
noogakl81_titanic-script.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc.fit(X_train, Y_train)', '# Random Forests', '# Gaussian Naive Bayes', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview', '# put ensemble together']",69
nptumis103035_titanic-data-science-solutions-8954f2.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '            # age_mean = guess_df.mean()', '            # age_std = guess_df.std()', '            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '            # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Gaussian Naive Bayes', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Random Forest', '    384             # Python 3 and no explicit encoding', '    387             # Python 3 and binary mode']",26
ojaswiawasthi_titanic-datatset-submission-logistic-regression.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", 'features=[""Pclass"",""Sex"",""Age"",""SibSp"",""Parch"",""Fare"",""Embarked""] # collecting all features ', 'X.info()    # Getting the information so as to analyse which columns in the dataset have object type values which have to be fixed by LabelEncoding in the preprocessing step', ' #   Column    Non-Null Count  Dtype  ', '# Fitting a Logistic regression model on the training data', '# The Cleaning Phase for test data', ' #   Column    Non-Null Count  Dtype  ']",15
okanerayger_titanic-survive-prediction.html,"['Passenger ID(Unique identifing # for each passenger)', '# There is no Royal Category at test_df,', '# new field family size']",3
omkarsabnis_svm-knn-rf-and-dt-implementation-titanic.html,[],0
omrihar_getting-to-know-kaggle-with-the-titanic-data-set.html,"['# Some important imports', '# Load the data', '# Prepare the data for the logistic regression', '# Handle NaNs', '# There are 177 NaN values in the age columns (out of 890 rows)', '# As a first attempt, we simply drop those rows', ""# Split this data to train and test datasets so I can estimate how well I'm doing"", ""# First attempt, feed the data to scikit-learn's LogisticRegression algorithm"", '# without tweaking any of the parameters', '# To see which factors are most important here, we plot coefficients with the data columns', '# The result of this is around 0.8, which if we compare with the leaderboard is not very good.', '# The best people get is 100% on the test data set with many above 0.88', '# To proceed now, I will plot some ""learning curves"" to see where I should spend my', '# time in improving the learning algorithm', ""# Since scikit-learn doesn't give you access to the cost function directly, we have to use a circumspect route"", '# (or program it myself)', '      4 # (or program it myself)']",17
orhunyildiz_titanic-notebook.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", ' #   Column       Non-Null Count  Dtype  ', '    # get feature', '    # count number of categorical variable(value/sample)', '    # visualize', '# Pclass vs Survived', '# Sex vs Survived', '# SibSp vs Survived', '# Parch vs Survived', '# Parch vs Survived', '        # 1st quartile', '        # 3rd quartile', '        # IQR', '        # Outlier step', '        # Detect outliers and their indices', '        # Store indices', '# drop outliers', '# convert to categorical']",26
ospohngellert_titanic-kernel-79-4-accuracy.html,[],0
ozankaans_beginner-titanic-disaster-prediction.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Matplotlib and seaborn for plotting graphs for EDA', '# Load DataSet', '# Data Info', '# Count the nulls present in columns', '# Drop columns not used or convert string into categorical data', '# Almost all female survived whoose Fare>30', '# Pair Plot applicable for low no. of dimensions', '# Facetgrid + Distance plot', '# Check for balanced data', '# Count Not Null', '# Classification Report', '# Confusion Matrix', '# Accuracy=TP+TN/Total=0.8022', '# Error_rate=FP+FN/Total=0.1977']",22
panditdandgule_complete-beginner-your-first-titanic-submission.html,[],0
pankajb64_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",53
pankeshpatel_titanic-survival-prediction.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# download .csv file from Kaggle Kernel', '# function that takes in a dataframe and creates a text link to  ', '# download it (will only work for files < 2MB or so)', '# create a random sample dataframe', '# create a link to download the dataframe', '# Copying the IDs to a variable ', '# We will use this, when making submission to the competition  ', '# Early observation', '# those passengers who has less # of siblings / spouses aboard the Titanic, they survived more', '# There is a strong linear relationship among these two variables', '# Early observation ', '# those passengers who has less #  of parents/children aboard the Titanic, they survived more', '# There is a strong linear relationship among these two variables', '# The relationship is not so clear to me, Need more work', '# There is not linear relationship between Pclass and Survived ', '# The relationship is not so clear to me, need more work', '# There is not linear relationship between Pclass and Survived ', '# The relationship is not so clear to me, need more work', '# There is not linear relationship between Pclass and Survived', '# It has been obvious that PassengerId and Name has no correlation with our target variable. ', '# Therefore, we will delete this column for simplicity reasons.', '# get information about missing values ', '# Data inmputation', '# It seems that NA values in ""Cabin""  be replaced with None', '# Data imputation for Age and Fare -  Mean Imputation', '# They are not categorical variables', '# Data Imputation for Embarked -- Mode imputation', '# Simplify ""Age"" attribute ', '# Kernel Reference: https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish/output', '# Simplify Fare attribute', '# Kernel Reference: https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish/output', '# reduce the feature', '# drop the column now', '# Data is now ready for encoder', '# all_data = pd.get_dummies(all_data)', '# all_data.head()', ""# Since, we have finished data imputation. Let's separate train_data and test_data"", '# X', ""# We won't touch test data now for a moment."", '# We will use the test data, once our model is ready.', '# Our target variable is ', ""# Let's first have train-test split"", '# Choose the type of classifier. ', '# Choose some parameter combinations to try', '# Type of scoring used to compare parameter combinations', '# Run the grid search', '# Set the clf to the best combination of parameters', '# Fit the best algorithm to the data. ', '# get the accuracy score of a model', '# Testing Logistic Regression as some  Kernel is getting high score than Random Forest Classifier', '# Kernel -- https://www.kaggle.com/patilneha09/pneha-titanic-dataset-solution', '# Results for the submission', '# choose the algorithm ', '# make predictions using test_data', '# Prepare dataset', '# you could use any filename. We choose submission here']",64
parakhsrivastava_titanic-survival-beginners.html,"['# Transform Pclass and Sex variable', '# Fill NAs in Age and normalizing values', '# Fill NA in Fare and Normalizing values', '# Fill NA in Embarked and transform values', ""print(test_D['Embarked'][test_D['Embarked'] == 'S'].count()) # 270"", ""print(test_D['Embarked'][test_D['Embarked'] == 'C'].count()) # 102"", ""print(test_D['Embarked'][test_D['Embarked'] == 'Q'].count()) # 46""]",7
parlads_gettingstartedwithtitanicstastices.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', ' #   for filename in filenames:', '  #      print(os.path.join(dirname, filename))', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",11
patrikdurdevic_the-titanic-tensorflow-deep-learning-crisp-dm.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'import tensorflow as tf # Deep learning', 'import matplotlib.pyplot as plt # Plots', 'import math # Basic math', 'import seaborn as sns # Plots']",6
paultimothymooney_titanic-quick-analysis-w-minimal-dataset.html,"[""# os.chdir('/Users/ptm/desktop/Current_working_directory')"", ""# trainingData = pd.read_csv('train.csv')"", ""# testingData = pd.read_csv('test.csv')"", '# Next we will need to split up our training data, setting aside 20% of the training data for cross-validation testing, such that we can avoid potentially overfitting the data.', '    # boxplot algorithm comparison', '#    predictions = model.predict(c)', '#    print(accuracy_score(d, predictions))', ""#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))"", '#    predictions = model.predict(c)', '#    print(accuracy_score(d, predictions))', ""#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))"", '#    predictions = model.predict(c)', '#    print(accuracy_score(d, predictions))', ""#    print('Logistic Regression - Training set accuracy: %s' % accuracy_score(d, predictions))"", '# It looks like our model can predict with about 70-75% accuracty whether or not a given', '# passenger survived the sinking of the Titanic despite using only a minimal dataset.  That is pretty good!', '#    ""PassengerId"": testingData2[""PassengerId""],', '#    ""Survived"": prediction})', ""# to finish the submission process, upload the file 'new_submission.csv' to Kaggle""]",19
pcjukaria_titanic-survival.html,['# you could use any filename. We choose submission here'],1
penguincwarrior_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79
penguinwang96825_xgboost-newbie-for-titanic-dataset.html,[],0
peterabdou_titanic-learn-from-others.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction""]",6
plasticgrammer_titanic-training.html,[],0
profversaggi_new-titanic-script.html,[],0
psangam_titanic-survival-analysis-and-prediction-using-ml.html,"['# Data Analysis', '# Data Visualization', '# allowing multiple/scrollable outputs', '# Function inspired by Helge Bjorland: An Interactive Data Science Tutorial', '# Load data', '# preliminary analysis', '# preliminary analysis', '# Grouping Family Size to Ordinals', '# to get an idea of all titles in both datasets (to make cleaning easier)', '# Variables needed for building prediction model', '# Support Vector Machines', '# Decision Tree', '# Gaussian Naive Bayes', '# Perceptron', '# Stochastic Gradient Descent', '# Random Forest', '# Model Evaluation', '# #competition submission: Random Forest Trees', '# submission = pd.DataFrame({', '#         ""PassengerId"": df_test[""PassengerId""],', '#         ""Survived"": Y_pred', '#     })', ""# submission.to_csv('submission.csv', index=False)""]",23
qwordy_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Train, predict, write result', '# formal_process()']",10
ragulshan_titanic-comprehensive-eda-to-ml.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'test = df.iloc[891:,:] # to submit our predictions', '# Setup random seed', ""gender_ratio =train_full.groupby(['male'])['Survived'].mean().to_frame() *100 # 0 means female and 1 means male."", '# from sklearn.metrics import plot_roc_curve', '# #Plotting Roc curve for diff models', '# fig,ax = plt.subplots(1,1,figsize=(10,5))', '# plot_roc_curve(log_model,x_val,y_val,ax=ax)', '# plot_roc_curve(clf,x_val,y_val,ax=ax);', '# #Plotting Roc curve for diff curves', '# fig,ax = plt.subplots(1,1,figsize=(10,5))', '# #Model with deafult settings', '# plot_roc_curve(tree,x_val,y_val,ax=ax)', '# #Best model', '# plot_roc_curve(tr_grid,x_val,y_val,ax=ax);', '# Number of trees in random forest', '# Number of features to consider at every split', '# Maximum number of levels in tree', '# Minimum number of samples required to split a node', '# Minimum number of samples required at each leaf node', '# Create the grid', '# #Plotting Roc curve for diff curves', '# fig,ax = plt.subplots(1,1,figsize=(10,5))', '# #Model with deafult settings', '# plot_roc_curve(rf,x_val,y_val,ax=ax)', '# #Best model', '# plot_roc_curve(rf_grid,x_val,y_val,ax=ax);', '# group data by Survived vs Non-Survived', '# and find nulls for cabin', '# from sklearn.ensemble import StackingClassifier', '# #Base level estimators', ""# estimators =[('rf',rf_grid),('tr',tr_grid),('gbm',gbm), ('xgb',xgb),('log',log_model)]"", '# #create a stacking model', '# stacking1 = StackingClassifier(estimators=estimators,', ""#                               final_estimator =SVC(kernel = 'rbf')"", '#                               )', '# #Training the stackers', '# stacking1.fit(x_train,y_train)', '# #Stacking classifier', '# from sklearn.ensemble import StackingClassifier', '# #Base level estimators', ""# estimators =[('rf',rf_grid),('tr',tr_grid),('gbm',gbm), ('xgb',xgb),('log',log_model)]"", '# #create a stacking model', '# stacking2 = StackingClassifier(estimators=estimators,', '#                               final_estimator =LogisticRegression(C=100))', '# #Training the stackers', '# stacking2.fit(x_train,y_train)', '# #Stacking model assessment(train-set)', ""# print('Stacking(svm) train-set score  ',stacking1.score(x_train,y_train)*100)"", ""# print('Stacking(logit) train-set score',stacking2.score(x_train,y_train)*100)"", '# #Stacking model assessment(Test-set)', ""# print('\\nStacking(svm) test-set score   ',stacking1.score(x_val,y_val)*100)"", ""# print('Stacking(logit) test-set score ',stacking2.score(x_val,y_val)*100)"", ""# #Kaggle doesn't support latest verison of sklearn modules which makes it hard to implement latest features"", '# #Lets plot confuison matrix (https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)', '# from sklearn.metrics import plot_confusion_matrix', '# np.set_printoptions(precision=3)', '# # Plot non-normalized confusion matrix', '# titles_options = [(""Confusion matrix, without normalization"", None),', '#                   (""Normalized confusion matrix"", \'true\')]', '# #svm with default settings', '# for title, normalize in titles_options:', '#     disp = plot_confusion_matrix(svm, x_val,y_val,', ""#                                  display_labels=['Not Survived','Survived'],"", '#                                  cmap=plt.cm.Blues,', '#                                  normalize=normalize)', '#     disp.ax_.set_title(title)', '#     print(title)', '#     print(disp.confusion_matrix)', '# ## Plotting CM', '# np.set_printoptions(precision=3)', '# # Plot non-normalized confusion matrix', '# titles_options = [(""Confusion matrix, without normalization"", None),', '#                   (""Normalized confusion matrix"", \'true\')]', '# #svm with default settings', '# for title, normalize in titles_options:', '#     disp = plot_confusion_matrix(grid_svm, x_val,y_val,', ""#                                  display_labels=['Not Survived','Survived'],"", '#                                  cmap=plt.cm.Blues,', '#                                  normalize=normalize)', '#     disp.ax_.set_title(title)', '#     print(title)', '#     print(disp.confusion_matrix)', '# #compare precision-Recall score', '# from sklearn.metrics import plot_precision_recall_curve', '# #compare both models', '# fig,ax = plt.subplots(1,1,figsize=(12,5))', '# plot_precision_recall_curve(svm, x_val,y_val,ax=ax)', ""# ax.set_title('PR Curve')"", '# plot_precision_recall_curve(grid_svm, x_val,y_val,ax=ax);']",91
rajshree07_random-forest-titanic-prediction.html,['# Name length column'],1
raviagheda_titanic-survival-using-logistic-regression.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# replacing male with 0 and femlae with 1', '# Dealing with null value in AGE column']",11
ren666_kaggle-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', 'plt.ylabel(u""age"")                         # è®¾å®šçºµåæ\xa0‡åç§°', ""plt.legend((u'top', u'2',u'3'),loc='best') # sets our legend for our graph."", 'fig.set(alpha=0.2)  # è®¾å®šå›¾è¡¨é¢œè‰²alphaå‚æ•°', '#', '    # æŠŠå·²æœ‰çš„æ•°å€¼åž‹ç‰¹å¾å–å‡ºæ¥ä¸¢è¿›Random Forest Regressorä¸\xad', '    # ä¹˜å®¢åˆ†æˆå·²çŸ¥å¹´é¾„å’ŒæœªçŸ¥å¹´é¾„ä¸¤éƒ¨åˆ†', '#     unknown_age = age_df[age_df.Age.isnull()].values', '    # yå³ç›®æ\xa0‡å¹´é¾„', '    # Xå³ç‰¹å¾å±žæ€§å€¼', '    # fitåˆ°RandomForestRegressorä¹‹ä¸\xad', '    # ç”¨å¾—åˆ°çš„é¢„æµ‹ç»“æžœå¡«è¡¥åŽŸç¼ºå¤±æ•°æ®', '# def set_Cabin_type(df):', '#     df.loc[ (df.Cabin.notnull()), \'Cabin\' ] = ""Yes""', '#     df.loc[ (df.Cabin.isnull()), \'Cabin\' ] = ""No""', '#     return df', '# data_train = set_Cabin_type(data_train)', '# for dataset in combine:', ""#     dataset['isChild']=0"", ""#     dataset.loc[dataset['Age']<=14,'isChild']=1"", '# data_train.shape', '# #ç‰¹å¾å›\xa0å\xadåŒ– data_train', ""# dummies_Embarked  = pd.get_dummies(data_train['Embarked'],prefix='Embarked')"", ""# dummies_Sex = pd.get_dummies(data_train['Sex'],prefix='Sex')"", ""# dummies_Pclass = pd.get_dummies(data_train['Pclass'],prefix = 'Pclass')"", ""# dummies_isChild = pd.get_dummies(data_train['isChild'],prefix = 'isChild')"", ""# dummies_FamliySize =pd.get_dummies(data_train['FamliySize'],prefix = 'FamliySize')"", ""# dummies_isAlone =pd.get_dummies(data_train['isAlone'],prefix = 'isAlone')"", ""# dummies_AgeBand =pd.get_dummies(data_train['AgeBand'],prefix = 'AgeBand')"", '# data_train = pd.concat([data_train,dummies_Embarked,dummies_Sex,dummies_Pclass,dummies_isChild,dummies_FamliySize,dummies_isAlone,dummies_AgeBand],axis=1)', '# data_train.head()', '# #ç‰¹å¾å›\xa0å\xadåŒ– data_test', ""# dummies_Embarked  = pd.get_dummies(data_test['Embarked'],prefix='Embarked')"", ""# dummies_Sex = pd.get_dummies(data_test['Sex'],prefix='Sex')"", ""# dummies_Pclass = pd.get_dummies(data_test['Pclass'],prefix = 'Pclass')"", ""# dummies_isChild = pd.get_dummies(data_test['isChild'],prefix = 'isChild')"", ""# dummies_FamliySize =pd.get_dummies(data_test['FamliySize'],prefix = 'FamliySize')"", ""# dummies_isAlone =pd.get_dummies(data_test['isAlone'],prefix = 'isAlone')"", ""# dummies_AgeBand =pd.get_dummies(data_test['AgeBand'],prefix = 'AgeBand')"", '# data_test = pd.concat([data_test,dummies_Embarked,dummies_Sex,dummies_Pclass,dummies_isChild,dummies_FamliySize,dummies_isAlone,dummies_AgeBand],axis=1)', '# data_test.info()', '# #å½’ä¸€åŒ–', '# import sklearn.preprocessing as preprocessing', '# scaler = preprocessing.StandardScaler()', ""# age_scale_param = scaler.fit(train_df['Age'].values.reshape(-1,1))"", ""# train_df['Age_scaled']= scaler.fit_transform(train_df['Age'].values.reshape(-1,1),age_scale_param)"", ""# fare_scale_param = scaler.fit(train_df['Fare'].values.reshape(-1,1))"", ""# train_df['Fare_scaled'] = scaler.fit_transform(train_df['Fare'].values.reshape(-1,1),fare_scale_param)"", '# train_df', ""# test_df['Age_scaled'] = scaler.fit_transform(test_df['Age'].values.reshape(-1,1), age_scale_param)"", ""# test_df['Fare_scaled'] = scaler.fit_transform(test_df['Fare'].values.reshape(-1,1), fare_scale_param)"", '# test_df', ""# train_df = train_df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Embarked_.*|Sex_.*|Pclass_.*')"", '# train_np = train_df.values', ""# test = test_df.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Embarked_.*|Sex_.*|Pclass_.*')"", ""# test = test_df.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Embarked_.*|Sex_.*|Pclass_.*')"", '# predictions = clf.predict(test)', ""# result = pd.DataFrame({'PassengerId':data_test['PassengerId'].values,'Survived':predictions.astype(np.int32)})"", '# result', ""# result.to_csv('Submission.csv',index=False)"", '# #æŸ¥çœ‹æ‰“åˆ†æƒ…å†µ', '# from sklearn import cross_validation', ""# clf =linear_model.LogisticRegression(C=1.0,penalty='l1',tol=1e-6)"", ""# all_data = train_df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Embarked_.*|Sex_.*|Pclass_.*')"", '# X = all_data.values[:,1:]', '# y = all_data.values[:,0]', '# print (cross_validation.cross_val_score(clf,X,y,cv=5))', '# #åˆ†å‰²æ•°æ® è®\xadç»ƒæ•°æ®:cvæ•°æ®=7:3', '# split_train,split_cv = cross_validation.train_test_split(train_df,test_size=0.3,random_state=0)', ""# split_train_df =split_train.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Embarked_,*|Sex_.*|Pclass_.*')"", '# #ç”Ÿæˆ,æ¨¡åž‹', ""# clf = linear_model.LogisticRegression(C=1.0,penalty='l1',tol=1e-6)"", '# X =split_train_df.values[:,1:]', '# y = split_train_df.values[:,0]', '# clf.fit(X,y)', '# #å¯¹cross validationæ•°æ®è¿›è¡Œé¢„æµ‹', ""# split_cv_df = split_cv.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Embarked_.*|Sex_.*|Pclass_.*')"", '# predictions = clf.predict(split_cv_df.values[:,1:])', '# origin_data_train = pd.read_csv(""../input/train.csv"")', ""# bad_cases = origin_data_train.loc[origin_data_train['PassengerId'].isin(split_cv[predictions!=split_cv_df.values[:,0]]['PassengerId'].values)]"", '# bad_cases', '# # æ\xa0¹æ®å\xad¦ä¹\xa0æ›²çº¿åˆ¤æ–\xadæ¨¡åž‹çŠ¶æ€ è¿‡æ‹Ÿåˆ æ¬\xa0æ‹Ÿåˆ', '# import numpy as np ', '# import matplotlib.pyplot as plt', '# from sklearn.learning_curve import learning_curve', '# #ç”¨sklearnçš„learning_curveå¾—åˆ°training_score, ä½¿ç”¨matplotç”»å‡ºlearning curve', '# def plot_learning_curve(estimator,title,X,y,ylim=None,cv=None,n_jobs=1,train_sizes=np.linspace(.05,1.,20),verbose=0,plot=True):', '#     """"""', '#     ç”»å‡ºdataåœ¨æŸæ¨¡åž‹ä¸Šçš„learning curve', '#     å‚æ•°è¯´æ˜Ž:', '#     ----------------------------------', '#     estimateor:ä½\xa0ç”¨çš„åˆ†ç±»å™¨', '#     title: æ\xa0‡é¢˜', '#     X : è¾“å…¥çš„feature,numpyç±»åž‹', '#     y : è¾“å…¥çš„target ', '#     ylim  : tupleæ\xa0¼å¼çš„(ymin,ymax),è®¾å®šå›¾åƒä¸\xadåæ\xa0‡çš„æœ€é«˜ç‚¹å’Œæœ€ä½Žç‚¹', '#     cv :åšcross validationçš„æ—¶å€™,æ•°æ®åˆ†æˆçš„ä»½æ•° ,é»˜è®¤æ˜¯3ä»½', '#     n_jobs:å¹¶è¡Œçš„ä»»åŠ¡æ•°(é»˜è®¤1)', '#     """"""', '#     train_sizes,train_scores,test_scores = learning_curve(estimator,X,y,cv=cv,n_jobs=n_jobs,train_sizes=train_sizes,verbose=verbose)', '#     train_scores_mean  = np.mean(train_scores,axis=1)', '#     train_scores_std = np.std(train_scores,axis=1)', '#     test_scores_mean = np.mean(test_scores,axis=1)', '#     test_scores_std = np.std(test_scores,axis=1)', '#     if plot:', '#         plt.figure()', '#         plt.title(title)', '#         if ylim is not None:', '#             plt.ylim(*ylim)', ""#         plt.xlabel('train samples')"", ""#         plt.ylabel('score')"", '# #         plt.gca().invert_yaxis()', '#         plt.grid()', '#         plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, ', '#                          alpha=0.1, color=""b"")', '#         plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, ', '#                          alpha=0.1, color=""r"")', '#         plt.plot(train_sizes, train_scores_mean, \'o-\', color=""b"", label=u""scores on train dataset"")', '#         plt.plot(train_sizes, test_scores_mean, \'o-\', color=""r"", label=u""scores on cv dataset"")', '#         plt.legend(loc=""best"")', '#         plt.draw()', '#         plt.show()', '# #         plt.gca().invert_yaxis()', '#     midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2', '#     diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])', '#     return midpoint, diff', '# plot_learning_curve(clf, u""learning curve"", X, y)']",134
reubensinha_competition-kernel-titanic-survival-prediction.html,[],0
ristola_titanic.html,"['    width = 0.4 # the width of the bars ', '    ind = np.arange(len(y)) # the x locations for the groups', '    # Our priors Beta distribution parameters', '        # Update our parameters', '        # Run CV', '        # Update best result', '        # Update our parameters', '        # Run CV', '        # Update best result', '# The real work begins with reading the data', '# Combine the train and test data into one file for some preprocessing', 'SibSp    # of siblings / spouses aboard the Titanic  - Interval', 'Parch    # of parents / children aboard the Titanic  - Intervall', '# Filling the missing value in Fare with the median Fare of 3rd class alone passenger', '    # Parameters that we are going to tune.', '    # Initial xgboost parameters to be tuned']",16
rmelick_titanic-notebook-complete.html,"['    # gender is categorical male, female, or unknown', '    # age is continuous, but there is some missing data', '    # we replace all the missing data with 0 (which means it will be)']",3
roarki_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79
rohitmahor_titanic-problem.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# data preprocessing', '# 1. completeing dataset', ""# print(data_train.info(), '\\n', data_test.info())"", '# 2. creating new features familySize, isAlone', ""# print(data_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"", ""# print(pd.crosstab(data_train['Title'], data_train['Sex']))"", ""# print(data_train[['isAlone', 'Survived']].groupby(['isAlone'], as_index=False).mean())"", '# 3. Creating dummy variable dataset', '# 4. categorical variables fare, Age', '# print(data_train)', '# get_dummies of title', '# for d in data:', '#     print(d.head(10))', '# data[0] = data[0].append(cat_title)', '# from sklearn.preprocessing import OneHotEncoder', '# onehotencoder = OneHotEncoder(categorical_features=[4, 6])', '# data_train = onehotencoder.fit_transform(data_train)', '# print(data_train)', '# split dataset in training and test dataset', '# print(X_train, y_train)', '# classifier models', '#     print(score, name)', '# predict data using KNN']",31
rohitvijay_titanic-prediction-using-logistic-regression.html,[],0
runzedong_titanic-1st-version.html,"[""  X = X[np.logical_and(X > clip[0], X < clip[1])] # won't work for two columns."", ""  X = X[np.logical_and(X > clip[0], X < clip[1])] # won't work for two columns."", ""X_train = Data_train.drop(['Survived'], axis = 1) # Input columns""]",3
rutwikdeshmukh_titanic-survival-predictions.html,[],0
sabarostami_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
sabirul_titanic-using-naive-bayes.html,[],0
sadam25_titanic-beginners-model-built-using-log-reg.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.']",8
samliu_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
samuelhei_titanic-logistic-regression.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
sanchitvj_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", ' #   Column       Non-Null Count  Dtype  ', ""train_df['Title'] = train_df['Title'].astype(int)                     # this is performed beacuse it was giving float values of title"", '# We will use GridSearchCV to find best parameters']",12
sandratierno_predictive-analytics-for-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', 'from sklearn import ensemble # random forest ', 'from sklearn import tree # tree ', '# Any results you write to the current directory are saved as output.']",10
sanjaynandakumar_titanic-survival-analysis-using-logistic-reg.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '    #    inputData = inputData.drop(uniqueId, axis=1)']",10
satyamkr1729_titanic-dataset.html,[],0
sauravprasad_eda-titanic.html,[],0
sebastiengclaro_kaggle-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", '# EDA on train data set', '# Check types of each columns and non-Null values', ' #   Column       Non-Null Count  Dtype  ', '# Remove column Cabin and missing value', '# Reorder columns to get our label at the end', '# Remove unwanted features and check Parch values', '# Count the number of Parch values', '# Map categories', '# Embarked categorical value', '# SibSp categorical value', '# Binarize gender', '# Pclass categories', '# Drop Pclass and replace by dummies', '# Checkpoint categories cols', '# Import matplotlib and seaborn', '# Histogram distributions Age', '# Histo Fare', '# Boxplot Fare', '# Remove outliers and keep 95% of data', '# Boxplot Fare', '# Histo Fare', '# Last check of preprocess', '# We need to make a function to preprocess the test dataset', '    # Remove columns and NA', '    df = df.fillna(0) # NA values in test but we can not drop them', '    # Map values', '    # Dummies Pclas', '    # Reorder cols', '# Import libraries', '# Init data', ' #   Column       Non-Null Count  Dtype  ', '# Preprocess test data', '# Fit scaler on train data', ' #   Column    Non-Null Count  Dtype  ', '# Train the model', '# Save result']",45
sethzgold_titanic.html,[],0
shari1_titanic.html,[],0
sharma98_titanic-top-4.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.']",8
shep312_applying-lightgbm-to-titanic-dataset.html,"['# Not sure passenger ID is useful as a feature, but need to save it from the test set for the submission', ""# 'Embarked' is stored as letters, so fit a label encoder to the train set to use in the loop"", '# Dataframes to work on', '    # Record anyone travelling alone', ""    # Transform 'Embarked'"", ""    # Transform 'Sex'"", '    # Drop features that seem unusable. Save passenger ids if test', '# Separate the label', '# Take a hold out set randomly', '# Create an LGBM dataset for training', '# Create an LGBM dataset from the test', '# Finally, create a dataset for the FULL training data to give us maximum amount of data to train on after ', '# performance has been calibrate', ""    'boosting': 'dart',          # dart (drop out trees) often performs better"", ""    'learning_rate': 0.05,       # Learning rate, controls size of a gradient descent step"", ""    'min_data_in_leaf': 20,      # Data set is quite small so reduce this a bit"", ""    'feature_fraction': 0.7,     # Proportion of features in each boost, controls overfitting"", ""    'num_leaves': 41,            # Controls size of tree since LGBM uses leaf wise splits"", ""    'metric': 'binary_logloss',  # Area under ROC curve as the evaulation metric"", '# Plot the log loss during training', '# Plot feature importance']",21
shotaku_titanic-prediction-using-neural-nework.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
shuanggao_titanic-tiro.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# loc[è¡Œæ\xa0‡ç\xad¾ï¼Œåˆ—æ\xa0‡ç\xad¾] æŒ‰ç…§è¡Œåˆ—æ\xa0‡ç\xad¾è¿›è¡Œåˆ‡ç‰‡ï¼Œ', '        # æ•°æ®é›†çš„åå\xad—å¦‚Todoroff, Mr. Lalio æˆ‘ä»¬è¦æ‰¾å‡ºå…¶ä¸\xadçš„Mr åˆ™è¿”å›žgroup(1)', '# print(titanic[features])', '# print(predictions)', '# print(predictions)', '# n_estimators=10 éšæœºæ£®æž—é‡Œè¦æž„å»ºæ\xa0‘çš„ä¸ªæ•°  min_samples_split=2 æ•°æ®æœ€å°åˆ‡åˆ†ä¸ªæ•°  min_samples_leaf=1 å¶å\xadç»“ç‚¹çš„æœ€å°ä¸ªæ•°', ""# features = ['Pclass','Age','Sex','Fare','Embarked','FamilySize','titles']"", '#         test_predictions = model_i.predict(titanic[features].iloc[test,:])', ""# print('æµ‹è¯•æ•°æ®é›†ï¼š\\n',titanic_test)"", '# print(titanic_test.describe())', '# print(titanic_test.describe())', ""# print(titanic['Sex'].unique())"", ""# print(titanic_test['Embarked'].unique())"", '# print(titanic_test.head())', '        # æ•°æ®é›†çš„åå\xad—å¦‚Todoroff, Mr. Lalio æˆ‘ä»¬è¦æ‰¾å‡ºå…¶ä¸\xadçš„Mr åˆ™è¿”å›žgroup(1)', '# print(titles)', '# print(pd.value_counts(titles))', '# print(pd.value_counts(titles))', '    # ç”¨æ•´ä¸ªè®\xadç»ƒé›†å¯¹æ¨¡åž‹è¿›è¡Œè®\xadç»ƒ.', '    # ä½¿ç”¨æµ‹è¯•æ•°æ®é›†è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬å¿…é¡»å°†æ‰€æœ‰åˆ—éƒ½è½¬æ¢ä¸ºæµ®ç‚¹æ•°ä»¥é¿å…é”™è¯¯.']",28
simo2712_titanic-disaster-ml-first-project.html,"['Sibsp:      # of siblings / spouses aboard the Titanic', 'Parch:      # of parents / children aboard the Titanic', ""  X = X[np.logical_and(X>clip[0], X<clip[1])] # won't work for two columns."", ""  X = X[np.logical_and(X>clip[0], X<clip[1])] # won't work for two columns.""]",4
simo333_titanic-3-0-xgboost.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
simulacra_titanic-with-xgboost.html,"[""X_full['Cabin_mapped'] = X_full['Cabin'].astype(str).str[0] # this captures the letter"", '# this transforms the letters into numbers', '# Create the parameter grid: gbm_param_grid ', '# Instantiate the regressor: gbm', '# Perform random search: grid_mse', '# Fit randomized_mse to the data', '# Print the best parameters and lowest RMSE']",7
sjessies_ml-titanic-1-0.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# load of test and training data', '# Exploring and understand the data set', '# Scan training data, null data count', '# Mapping unique values from Embarked', '# Clean data process ', '    # Fill de Nan Valuen in Fare an Age ', '    # Fill the Nan values in Age Column', '# Create classifier object', '# Train the classifier (fit the estimator) using the training data', '# Estimate the accuracy of the classifier on future data, using the test data', '# Clean data process ', '    # Fill de Nan Valuen in Fare an Age ', '    # Fill the Nan values in Age Column', ""# Add the column ['predicted_survived'] to data set""]",22
smivvla_titanic-kernel.html,"['# load dataframes', '# print some statistics', '    # if the title exists, extract and return it.', '# ', '    # Create new feature Last Name in order to group families', ""#     dataset['LastName'] = dataset['LastName'].astype('category')"", '    # Create a new feature Title, containing the titles of passenger names', '    # Group all non-common titles into one single grouping ""Rare""', '    # Mapping titles', ""#     dataset['Title'] = dataset['Title'].astype('category')"", '    # Feature that tells whether a passenger had a cabin on the Titanic', '    # Create new feature FamilySize as a combination of SibSp and Parch', '    # Create new feature IsAlone from FamilySize', '    # Mapping Embarked', '    # Remove all NULLS in the Fare column and create a new feature CategoricalFare', '    # Fill null age rows with median value', '    # Create new feature', ""#     dataset['Age*Class'] = dataset['Age'] * dataset['Pclass']"", '    # Mapping Sex', ""#     drop_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']"", '# get all column names', '# numeric columns', '# categorical columns', ""#     'knn': {"", ""#         'model': KNeighborsClassifier,"", ""#         'params': {"", ""#             'n_neighbors' : range(1, 10),"", ""#             'weights' : ['uniform', 'distance'],"", ""#             'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],"", ""#             'leaf_size' : range(10, 100, 10),"", '#         }', '#     },', ""#     'lrcv': {"", ""#         'model': LogisticRegressionCV,"", ""#         'params': {"", ""#             'Cs': [1, 2, 4, 8, 16, 32],"", ""#             'fit_intercept': [True, False],"", ""#             'refit': [True, False],"", ""#             'multi_class': ['ovr'],"", ""#             'penalty': ['l2'],"", ""#             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],"", ""#             'tol': [0.01, 0.05, 0.1, 0.5, 1, 5],"", ""#             'cv': [cv]"", '#         },', ""#         'best_params': {'tol': 0.05, 'solver': 'newton-cg', 'refit': True, 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': False, 'cv': 4, 'Cs': 2},"", ""#         'best_score': 0.8428731762065096"", '#     },', ""            'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node"", ""            'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node"", ""            'min_samples_split': range(2, 10), # Minimum number of samples required to split a node"", ""            'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node"", ""            'bootstrap': [True, False], # Method of selecting samples for training each tree,"", ""            'min_samples_split': range(2, 10), # Minimum number of samples required to split a node"", ""            'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node"", ""            'bootstrap': [True, False], # Method of selecting samples for training each tree,"", ""            'min_samples_split': range(2, 10), # Minimum number of samples required to split a node"", ""            'min_samples_leaf': range(1, 10), # Minimum number of samples required at each leaf node"", ""#     'xgb': {"", ""#         'model': XGBClassifier,"", ""#         'params': {"", ""#             'n_estimators': range(8, 20),"", ""#             'max_depth': range(5, 20),"", ""#             'learning_rate': [.01, .05, .1, .2, .5, 1, 2],"", ""#             'colsample_bytree': [.6, .7, .8, .9, 1]"", '#         }', '#     }', '        # Initialize with best parameters & fit to data', '        # Perform random search', '        # Fit to data', '        # Print the best parameters and best accuracy', '# """""" ----------------------------------- Fitting XGBoost classifier ------------------------------------- """"""', '# xgb_params = {', ""#     'n_estimators': range(20, 501, 20),"", ""#     'max_depth': range(4, 21, 4),"", ""#     'learning_rate': [.01, .05, .1, .2, .5, 1, 2],"", ""#     'colsample_bytree': [.6, .7, .8, .9, 1]"", '# }', ""# # xgb = XGBClassifier(**{'n_estimators': 20, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': 0.8})"", '# # Perform random search', '# xgb = RandomizedSearchCV(param_distributions=xgb_params,', '#                               estimator=XGBClassifier(), scoring=""accuracy"",', '#                               verbose=1, n_iter=N_ITER, cv=N_FOLDS)', '# # Fit to data', ""# print(f'Fitting xgb...')    "", '# xgb.fit(X_train, y_train)', ""# print(f'Best parameters found for {name}: {xgb.best_params_}')"", ""# print(f'Best accuracy found {name}: {xgb.best_score_}')"", '# pred = xgb.predict(X_test)', ""# pred = MODELS[max(MODELS, key=lambda k: MODELS[k]['best_score'])]['best_estimator'].predict(x_test)""]",89
someshugar_titanic-data-eda-and-prediction-model.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# import datasets', '# view first five lines of training data', '# Data Munging', '# Drop low corrlations and high cardinality', '# label', '# Creade dummy variables for Sex and drop original, as well as an unnecessary column (male or female)', '# Split training data into training and validation set', '# Initialize model', '# Fit data', '# Calc accuracy', '  # This is added back by InteractiveShellApp.init_path()']",19
souravdas4_prediction-of-survival-in-titanic-disaster.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '#', '#', '#', '#', '#', '#', '# Logistic Regression', ""#          intercept_scaling=1, max_iter=1000000, multi_class='ovr', n_jobs=1,"", ""#          penalty='l2', random_state=None, solver='saga', tol=0.0000001,"", '#          verbose=0, warm_start=False)']",13
srksiva20_titanic-competetion.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Reading data ']",9
srolka_titanic-survival-logistic-regression.html,[],0
ssandorov_titanic-competition.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# Because Sex has no missing values I can transform it directly', '# I will change the missing values form Emarked with the most frequent value', '# Now I transform it to a numerical feature', '# Imputation', '# Imputation removed column names; put them back']",13
sudeeshvarier_titanic-logistic-regression-analysis-ml.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
sumeetpisal_titanic-prediction.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]",9
sunnynevarekar_titanic-survival-prediction.html,[],0
surenj_titanic.html,[],0
suthirs_titanic-dataset-null-missing-values-treatment.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', 'train.shape # Rows -891, Columns - 12', '# To check which are all the columns having numerical values', '# to check which are all the columns having categorical values (except numerical colums)', '# length of ""categorical"" & ""numerical"" should match with lenght of dataset columns', '# To check null values columns preset or not in the given dataset if yes what is the count of each columns', '# to check percentage of the null values present in the each columns inthe dataset', '# Based on client input you can drop the null values(if client says consider the null values if greate than 50%)', '# Below are the command to drop the null values based on client input', '# train.fillna(20,inplace=True) - beginer can use this code to treat Null value', '      2 # Below are the command to drop the null values based on client input', '      3 # train.fillna(20,inplace=True) - beginer can use this code to treat Null value', '# lengh of drop columns and length of retain columns should match', '      1 # lengh of drop columns and length of retain columns should match', '# Replace the null values in numerical columns with ""median""', '# Replace the null values in categorical cloumns with ""average counts""', '# Finally to check if any null values present or not ', '# Null values treatment done and you can see all the columns having zero null values ']",25
tarunchoubisa_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79
tetsuyazama_titanic-example-for-nst.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã‚‚åŒã˜ã“ã¨ã‚’è¡Œã†ï¼ˆãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ï¼‰', 'from sklearn.svm import SVC # ã‚«ãƒ¼ãƒãƒ«ã‚’é™å®šã—ãªã„SVMãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª', '# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€™è£œ', '# GridSearchCVãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–', '# ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã§äº¤å·®æ¤œè¨¼ã‚’è¡Œã„æœ€ã‚‚å„ªã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŽ¢ã—å‡ºã™ï¼ˆ=Grid Searchï¼‰', '# æœ€ã‚‚ã‚¹ã‚³ã‚¢ãŒé«˜ã‹ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›', '# æœ€ã‚‚é«˜ã‹ã£ãŸã‚¹ã‚³ã‚¢']",15
thelazydatascientist_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# import data visualization library', '# import sklearn model class', '# import sklearn model selection', '# import sklearn model evaluation classification metrics', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",12
tkayyang_titanic-practice.html,[],0
tonypythoneer_titanic-try-to-make-complex.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# data of Embarked C is too few', '# data of Embarked Q is too few']",5
trinkwasserpsprudler_titanic.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'import seaborn as sns # data visualisation', '# show columns and dtypes', '# Column Description', '# PassengerId -> id', '# Survived -> if the passenger survived or not (0=not survived / 1=survived)', '# Pclass -> The class which the passenger travelled [3,2,1]', '# Name -> The name of the passenger', ""# Sex -> gender ['male', 'female']"", '# Age -> the age of the passenger', '# SibSp -> Amount of Siblings/Spouses', '# Parch -> Amount of family members on board (mother, father, daughter, son, stepdaughter, stepson)', '# Ticket -> Ticket Number', '# Fare -> Passenger Fare', '# Cabine -> Cabine Number', '# Embarked -> Port of Embarkation (Port where the passengers went on board) C = Cherbourg, Q = Queenstown, S = Southampton', '# Save the report in output', '# Show numerical description of the columns (-> object columsn are ignored)', '# Show columsn with nan values', '# Print correlation Matrix', '# drop embarked as it is not neccessary for prediction', '# only for test_data, since there is a missing ""Fare"" values', '# Man kÃ¶nnte die Werte hier auch normalisieren...', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '      5 # get average, std, and number of NaN values in titanic_df', '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '      9 # average survived passengers by age', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# combine the values in train_data', '# combine the values in test_data', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '     19 # plot', ""     22 # sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', ""     29 # sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# from sklearn.model_selection import train_test_split', '# # if we set the random state to a fix value we alwas get the same result in splitting', '# train_set, test_set = train_test_split(train_data_selected, test_size=0.2, random_state=42)', '# Random Forests', '    248         # Validate or convert input data']",70
umang5916_titanic-data-analysis-logistic-regression.html,[],0
umarhabib_analyzing-titanic-data-and-ml-model.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# transform the categorical columns']",9
valentindb_homework-1-titanic.html,"['# Ignore warnings', '# Display some statistics', '# Display missing data', ""# Let's plot some histograms to have a previzualisation of some of the data ..."", '    # Introducing a new feature : the size of families (including the passenger)', '    # Introducing other features based on the family size', '    # Deal with missing values', '    # Replacing missing cabins with U (for Uknown)', ""   # processed_df.Cabin.fillna('U', inplace=True)"", '    # Mapping each Cabin value with the cabin letter', ""   # processed_df['Cabin'] = processed_df['Cabin'].map(lambda c: c[0])"", '    # Dummy encoding ...', ""   # cabin_dummies = pd.get_dummies(processed_df['Cabin'], prefix='Cabin')    "", '   # processed_df = pd.concat([processed_df, cabin_dummies], axis=1)', '    # Get Title and Status from Name', '    # fills the missing values of the Age variable', ""# Let's divide the train dataset in two datasets to evaluate perfomance of machine learning models used"", '# Split dataset for prediction', '# Create and train model on train data sample', '# Predict for test data sample', '# Compute error between predicted data and true response and display it in confusion matrix', '# Predicting the Test set results', 'm_prediction = (m_prediction > 0.5) # convert probabilities to binary output', '# Compute error between predicted data and true response and display it in confusion matrix', '    # Initializing our ANN', '    # Adding the input layer and the first hidden layer of our ANN with dropout', '    # Dropout will disable some neurons (here 50% of all neurons) to avoid overfitting', '    # Add other layers, it is not necessary to pass the shape because there is a layer before', '    # Adding the output layer', '    # Compilling the ANN', '# Training the ANN', '# Predicting the Test set results', 'ann_prediction = (ann_prediction > 0.5) # convert probabilities to binary output', '# Compute error between predicted data and true response and display it in confusion matrix', 'm_prediction = (m_prediction > 0.5) # convert probabilities to binary output', '# Number of trees in random forest', '# Number of features to consider at every split', '# Maximum number of levels in tree', '# Minimum number of samples required to split a node', '# Minimum number of samples required at each leaf node', '# Method of selecting samples for training each tree', '# Create the random grid', '# Use the random grid to search for best hyperparameters', '# First create the base model to tune', '# Random search of parameters, using 3 fold cross validation, ', '# search across 100 different combinations, and use all available cores', '# Fit the random search model', '# Create the parameter grid based on the results of random search ', '# Instantiate the grid search model', '# Fit the grid search to the data', '# Create and train model on train data sample', '# Predict for test data sample']",52
vardos_leaning-titanic.html,"['{""cells"":[{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""c9c0744f-9e2d-223c-acce-38c9ef8235d2""},""source"":[""# Test 1""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""5767a33c-8f18-4034-e52d-bf7a8f7d8ab8""},""outputs"":[],""source"":[""# data analysis and wrangling\\n"",""import pandas as pd\\n"",""import numpy as np\\n"",""import random as rnd\\n"",""\\n"",""# visualization\\n"",""import seaborn as sns\\n"",""import matplotlib.pyplot as plt\\n"",""%matplotlib inline\\n"",""\\n"",""# machine learning\\n"",""from sklearn.linear_model import LogisticRegression\\n"",""from sklearn.svm import SVC, LinearSVC\\n"",""from sklearn.ensemble import RandomForestClassifier\\n"",""from sklearn.neighbors import KNeighborsClassifier\\n"",""from sklearn.naive_bayes import GaussianNB\\n"",""from sklearn.linear_model import Perceptron\\n"",""from sklearn.linear_model import SGDClassifier\\n"",""from sklearn.tree import DecisionTreeClassifier""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""6b5dc743-15b1-aac6-405e-081def6ecca1""},""source"":[""## Acquire data\\n"",""\\n"",""The Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""e7319668-86fe-8adc-438d-0eef3fd0a982""},""outputs"":[],""source"":[""train_df = pd.read_csv(\'../input/train.csv\')\\n"",""test_df = pd.read_csv(\'../input/test.csv\')\\n"",""combine = [train_df, test_df]""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""3d6188f3-dc82-8ae6-dabd-83e28fcbf10d""},""source"":[""## Analyze by describing data\\n"",""\\n"",""Pandas also helps describe the datasets answering following questions early in our project.\\n"",""\\n"",""**Which features are available in the dataset?**\\n"",""\\n"",""Noting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""ce473d29-8d19-76b8-24a4-48c217286e42""},""outputs"":[],""source"":[""print(train_df.columns.values)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""cd19a6f6-347f-be19-607b-dca950590b37""},""source"":[""**Which features are categorical?**\\n"",""\\n"",""These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n"",""\\n"",""- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n"",""\\n"",""**Which features are numerical?**\\n"",""\\n"",""Which features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n"",""\\n"",""- Continous: Age, Fare. Discrete: SibSp, Parch.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f""},""outputs"":[],""source"":[""# preview the data\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""97f4e6f8-2fea-46c4-e4e8-b69062ee3d46""},""source"":[""**Which features are mixed data types?**\\n"",""\\n"",""Numerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n"",""\\n"",""- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\\n"",""\\n"",""**Which features may contain errors or typos?**\\n"",""\\n"",""This is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\\n"",""\\n"",""- Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""f6e761c2-e2ff-d300-164c-af257083bb46""},""outputs"":[],""source"":[""train_df.tail()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""8bfe9610-689a-29b2-26ee-f67cd4719079""},""source"":[""**Which features contain blank, null or empty values?**\\n"",""\\n"",""These will require correcting.\\n"",""\\n"",""- Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\\n"",""- Cabin > Age are incomplete in case of test dataset.\\n"",""\\n"",""**What are the data types for various features?**\\n"",""\\n"",""Helping us during converting goal.\\n"",""\\n"",""- Seven features are integer or floats. Six in case of test dataset.\\n"",""- Five features are strings (object).""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""9b805f69-665a-2b2e-f31d-50d87d52865d""},""outputs"":[],""source"":[""train_df.info()\\n"",""print(\'_\'*40)\\n"",""test_df.info()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""859102e1-10df-d451-2649-2d4571e5f082""},""source"":[""**What is the distribution of numerical feature values across the samples?**\\n"",""\\n"",""This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\\n"",""\\n"",""- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\\n"",""- Survived is a categorical feature with 0 or 1 values.\\n"",""- Around 38% samples survived representative of the actual survival rate at 32%.\\n"",""- Most passengers (> 75%) did not travel with parents or children.\\n"",""- Nearly 30% of the passengers had siblings and/or spouse aboard.\\n"",""- Fares varied significantly with few passengers (<1%) paying as high as $512.\\n"",""- Few elderly passengers (<1%) within age range 65-80.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""58e387fe-86e4-e068-8307-70e37fe3f37b""},""outputs"":[],""source"":[""train_df.describe()\\n"",""# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\\n"",""# Review Parch distribution using `percentiles=[.75, .8]`\\n"",""# SibSp distribution `[.68, .69]`\\n"",""# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""5462bc60-258c-76bf-0a73-9adc00a2f493""},""source"":[""**What is the distribution of categorical features?**\\n"",""\\n"",""- Names are unique across the dataset (count=unique=891)\\n"",""- Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\\n"",""- Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\\n"",""- Embarked takes three possible values. S port used by most passengers (top=S)\\n"",""- Ticket feature has high ratio (22%) of duplicate values (unique=681).""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""8066b378-1964-92e8-1352-dcac934c6af3""},""outputs"":[],""source"":[""train_df.describe(include=[\'O\'])""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""2cb22b88-937d-6f14-8b06-ea3361357889""},""source"":[""### Assumtions based on data analysis\\n"",""\\n"",""We arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\\n"",""\\n"",""**Correlating.**\\n"",""\\n"",""We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\\n"",""\\n"",""**Completing.**\\n"",""\\n"",""1. We may want to complete Age feature as it is definitely correlated to survival.\\n"",""2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\\n"",""\\n"",""**Correcting.**\\n"",""\\n"",""1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\\n"",""2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\\n"",""3. PassengerId may be dropped from training dataset as it does not contribute to survival.\\n"",""4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\\n"",""\\n"",""**Creating.**\\n"",""\\n"",""1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\\n"",""2. We may want to engineer the Name feature to extract Title as a new feature.\\n"",""3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\\n"",""4. We may also want to create a Fare range feature if it helps our analysis.\\n"",""\\n"",""**Classifying.**\\n"",""\\n"",""We may also add to our assumptions based on the problem description noted earlier.\\n"",""\\n"",""1. Women (Sex=female) were more likely to have survived.\\n"",""2. Children (Age<?) were more likely to have survived. \\n"",""3. The upper-class passengers (Pclass=1) were more likely to have survived.""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""6db63a30-1d86-266e-2799-dded03c45816""},""source"":[""## Analyze by pivoting features\\n"",""\\n"",""To confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\\n"",""\\n"",""- **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\\n"",""- **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\\n"",""- **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""0964832a-a4be-2d6f-a89e-63526389cee9""},""outputs"":[],""source"":[""train_df[[\'Pclass\', \'Survived\']].groupby([\'Pclass\'], as_index=False).mean().sort_values(by=\'Survived\', ascending=False)""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""68908ba6-bfe9-5b31-cfde-6987fc0fbe9a""},""outputs"":[],""source"":[""train_df[[\\""Sex\\"", \\""Survived\\""]].groupby([\'Sex\'], as_index=False).mean().sort_values(by=\'Survived\', ascending=False)""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""01c06927-c5a6-342a-5aa8-2e486ec3fd7c""},""outputs"":[],""source"":[""train_df[[\\""SibSp\\"", \\""Survived\\""]].groupby([\'SibSp\'], as_index=False).mean().sort_values(by=\'Survived\', ascending=False)""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""e686f98b-a8c9-68f8-36a4-d4598638bbd5""},""outputs"":[],""source"":[""train_df[[\\""Parch\\"", \\""Survived\\""]].groupby([\'Parch\'], as_index=False).mean().sort_values(by=\'Survived\', ascending=False)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""0d43550e-9eff-3859-3568-8856570eff76""},""source"":[""## Analyze by visualizing data\\n"",""\\n"",""Now we can continue confirming some of our assumptions using visualizations for analyzing the data.\\n"",""\\n"",""### Correlating numerical features\\n"",""\\n"",""Let us start by understanding correlations between numerical features and our solution goal (Survived).\\n"",""\\n"",""A histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\\n"",""\\n"",""Note that x-axis in historgram visualizations represents the count of samples or passengers.\\n"",""\\n"",""**Observations.**\\n"",""\\n"",""- Infants (Age <=4) had high survival rate.\\n"",""- Oldest passengers (Age = 80) survived.\\n"",""- Large number of 15-25 year olds did not survive.\\n"",""- Most passengers are in 15-35 age range.\\n"",""\\n"",""**Decisions.**\\n"",""\\n"",""This simple analysis confirms our assumptions as decisions for subsequent workflow stages.\\n"",""\\n"",""- We should consider Age (our assumption classifying #2) in our model training.\\n"",""- Complete the Age feature for null values (completing #1).\\n"",""- We should band age groups (creating #3).""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""50294eac-263a-af78-cb7e-3778eb9ad41f""},""outputs"":[],""source"":[""g = sns.FacetGrid(train_df, col=\'Survived\')\\n"",""g.map(plt.hist, \'Age\', bins=20)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""87096158-4017-9213-7225-a19aea67a800""},""source"":[""### Correlating numerical and ordinal features\\n"",""\\n"",""We can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\\n"",""\\n"",""**Observations.**\\n"",""\\n"",""- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\\n"",""- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\\n"",""- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\\n"",""- Pclass varies in terms of Age distribution of passengers.\\n"",""\\n"",""**Decisions.**\\n"",""\\n"",""- Consider Pclass for model training.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""916fdc6b-0190-9267-1ea9-907a3d87330d""},""outputs"":[],""source"":[""# grid = sns.FacetGrid(train_df, col=\'Pclass\', hue=\'Survived\')\\n"",""grid = sns.FacetGrid(train_df, col=\'Survived\', row=\'Pclass\', size=2.2, aspect=1.6)\\n"",""grid.map(plt.hist, \'Age\', alpha=.5, bins=20)\\n"",""grid.add_legend();""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""36f5a7c0-c55c-f76f-fdf8-945a32a68cb0""},""source"":[""### Correlating categorical features\\n"",""\\n"",""Now we can correlate categorical features with our solution goal.\\n"",""\\n"",""**Observations.**\\n"",""\\n"",""- Female passengers had much better survival rate than males. Confirms classifying (#1).\\n"",""- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\\n"",""- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\\n"",""- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\\n"",""\\n"",""**Decisions.**\\n"",""\\n"",""- Add Sex feature to model training.\\n"",""- Complete and add Embarked feature to model training.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""db57aabd-0e26-9ff9-9ebd-56d401cdf6e8""},""outputs"":[],""source"":[""# grid = sns.FacetGrid(train_df, col=\'Embarked\')\\n"",""grid = sns.FacetGrid(train_df, row=\'Embarked\', size=2.2, aspect=1.6)\\n"",""grid.map(sns.pointplot, \'Pclass\', \'Survived\', \'Sex\', palette=\'deep\')\\n"",""grid.add_legend()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""6b3f73f4-4600-c1ce-34e0-bd7d9eeb074a""},""source"":[""### Correlating categorical and numerical features\\n"",""\\n"",""We may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\\n"",""\\n"",""**Observations.**\\n"",""\\n"",""- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\\n"",""- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\\n"",""\\n"",""**Decisions.**\\n"",""\\n"",""- Consider banding Fare feature.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""a21f66ac-c30d-f429-cc64-1da5460d16a9""},""outputs"":[],""source"":[""# grid = sns.FacetGrid(train_df, col=\'Embarked\', hue=\'Survived\', palette={0: \'k\', 1: \'w\'})\\n"",""grid = sns.FacetGrid(train_df, row=\'Embarked\', col=\'Survived\', size=2.2, aspect=1.6)\\n"",""grid.map(sns.barplot, \'Sex\', \'Fare\', alpha=.5, ci=None)\\n"",""grid.add_legend()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""cfac6291-33cc-506e-e548-6cad9408623d""},""source"":[""## Wrangle data\\n"",""\\n"",""We have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\\n"",""\\n"",""### Correcting by dropping features\\n"",""\\n"",""This is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\\n"",""\\n"",""Based on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\\n"",""\\n"",""Note that where applicable we perform operations on both training and testing datasets together to stay consistent.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""da057efe-88f0-bf49-917b-bb2fec418ed9""},""outputs"":[],""source"":[""print(\\""Before\\"", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\\n"",""\\n"",""train_df = train_df.drop([\'Ticket\', \'Cabin\'], axis=1)\\n"",""test_df = test_df.drop([\'Ticket\', \'Cabin\'], axis=1)\\n"",""combine = [train_df, test_df]\\n"",""\\n"",""\\""After\\"", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""6b3a1216-64b6-7fe2-50bc-e89cc964a41c""},""source"":[""### Creating new feature extracting from existing\\n"",""\\n"",""We want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\\n"",""\\n"",""In the following code we extract Title feature using regular expressions. The RegEx pattern `(\\\\w+\\\\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.\\n"",""\\n"",""**Observations.**\\n"",""\\n"",""When we plot Title, Age, and Survived, we note the following observations.\\n"",""\\n"",""- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\\n"",""- Survival among Title Age bands varies slightly.\\n"",""- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\\n"",""\\n"",""**Decision.**\\n"",""\\n"",""- We decide to retain the new Title feature for model training.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""df7f0cd4-992c-4a79-fb19-bf6f0c024d4b""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'Title\'] = dataset.Name.str.extract(\' ([A-Za-z]+)\\\\.\', expand=False)\\n"",""\\n"",""pd.crosstab(train_df[\'Title\'], train_df[\'Sex\'])""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""908c08a6-3395-19a5-0cd7-13341054012a""},""source"":[""We can replace many titles with a more common name or classify them as `Rare`.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""553f56d7-002a-ee63-21a4-c0efad10cfe9""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'Title\'] = dataset[\'Title\'].replace([\'Lady\', \'Countess\',\'Capt\', \'Col\',\\\\\\n"","" \\t\'Don\', \'Dr\', \'Major\', \'Rev\', \'Sir\', \'Jonkheer\', \'Dona\'], \'Rare\')\\n"",""\\n"",""    dataset[\'Title\'] = dataset[\'Title\'].replace(\'Mlle\', \'Miss\')\\n"",""    dataset[\'Title\'] = dataset[\'Title\'].replace(\'Ms\', \'Miss\')\\n"",""    dataset[\'Title\'] = dataset[\'Title\'].replace(\'Mme\', \'Mrs\')\\n"",""    \\n"",""train_df[[\'Title\', \'Survived\']].groupby([\'Title\'], as_index=False).mean()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""6d46be9a-812a-f334-73b9-56ed912c9eca""},""source"":[""We can convert the categorical titles to ordinal.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""67444ebc-4d11-bac1-74a6-059133b6e2e8""},""outputs"":[],""source"":[""title_mapping = {\\""Mr\\"": 1, \\""Miss\\"": 2, \\""Mrs\\"": 3, \\""Master\\"": 4, \\""Rare\\"": 5}\\n"",""for dataset in combine:\\n"",""    dataset[\'Title\'] = dataset[\'Title\'].map(title_mapping)\\n"",""    dataset[\'Title\'] = dataset[\'Title\'].fillna(0)\\n"",""\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""f27bb974-a3d7-07a1-f7e4-876f6da87e62""},""source"":[""Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""9d61dded-5ff0-5018-7580-aecb4ea17506""},""outputs"":[],""source"":[""train_df = train_df.drop([\'Name\', \'PassengerId\'], axis=1)\\n"",""test_df = test_df.drop([\'Name\'], axis=1)\\n"",""combine = [train_df, test_df]\\n"",""train_df.shape, test_df.shape""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""2c8e84bb-196d-bd4a-4df9-f5213561b5d3""},""source"":[""### Converting a categorical feature\\n"",""\\n"",""Now we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\\n"",""\\n"",""Let us start by converting Sex feature to a new feature called Gender where female=1 and male=0.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""c20c1df2-157c-e5a0-3e24-15a828095c96""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'Sex\'] = dataset[\'Sex\'].map( {\'female\': 1, \'male\': 0} ).astype(int)\\n"",""\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""d72cb29e-5034-1597-b459-83a9640d3d3a""},""source"":[""### Completing a numerical continuous feature\\n"",""\\n"",""Now we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\\n"",""\\n"",""We can consider three methods to complete a numerical continuous feature.\\n"",""\\n"",""1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\\n"",""\\n"",""2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\\n"",""\\n"",""3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\\n"",""\\n"",""Method 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""c311c43d-6554-3b52-8ef8-533ca08b2f68""},""outputs"":[],""source"":[""# grid = sns.FacetGrid(train_df, col=\'Pclass\', hue=\'Gender\')\\n"",""grid = sns.FacetGrid(train_df, row=\'Pclass\', col=\'Sex\', size=2.2, aspect=1.6)\\n"",""grid.map(plt.hist, \'Age\', alpha=.5, bins=20)\\n"",""grid.add_legend()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""a4f166f9-f5f9-1819-66c3-d89dd5b0d8ff""},""source"":[""Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""9299523c-dcf1-fb00-e52f-e2fb860a3920""},""outputs"":[],""source"":[""guess_ages = np.zeros((2,3))\\n"",""guess_ages""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""ec9fed37-16b1-5518-4fa8-0a7f579dbc82""},""source"":[""Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""a4015dfa-a0ab-65bc-0cbe-efecf1eb2569""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    for i in range(0, 2):\\n"",""        for j in range(0, 3):\\n"",""            guess_df = dataset[(dataset[\'Sex\'] == i) & \\\\\\n"",""                                  (dataset[\'Pclass\'] == j+1)][\'Age\'].dropna()\\n"",""\\n"",""            # age_mean = guess_df.mean()\\n"",""            # age_std = guess_df.std()\\n"",""            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\\n"",""\\n"",""            age_guess = guess_df.median()\\n"",""\\n"",""            # Convert random age float to nearest .5 age\\n"",""            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\\n"",""            \\n"",""    for i in range(0, 2):\\n"",""        for j in range(0, 3):\\n"",""            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\\\\n"",""                    \'Age\'] = guess_ages[i,j]\\n"",""\\n"",""    dataset[\'Age\'] = dataset[\'Age\'].astype(int)\\n"",""\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""dbe0a8bf-40bc-c581-e10e-76f07b3b71d4""},""source"":[""Let us create Age bands and determine correlations with Survived.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""725d1c84-6323-9d70-5812-baf9994d3aa1""},""outputs"":[],""source"":[""train_df[\'AgeBand\'] = pd.cut(train_df[\'Age\'], 5)\\n"",""train_df[[\'AgeBand\', \'Survived\']].groupby([\'AgeBand\'], as_index=False).mean().sort_values(by=\'AgeBand\', ascending=True)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""ba4be3a0-e524-9c57-fbec-c8ecc5cde5c6""},""source"":[""Let us replace Age with ordinals based on these bands.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""797b986d-2c45-a9ee-e5b5-088de817c8b2""},""outputs"":[],""source"":[""for dataset in combine:    \\n"",""    dataset.loc[ dataset[\'Age\'] <= 16, \'Age\'] = 0\\n"",""    dataset.loc[(dataset[\'Age\'] > 16) & (dataset[\'Age\'] <= 32), \'Age\'] = 1\\n"",""    dataset.loc[(dataset[\'Age\'] > 32) & (dataset[\'Age\'] <= 48), \'Age\'] = 2\\n"",""    dataset.loc[(dataset[\'Age\'] > 48) & (dataset[\'Age\'] <= 64), \'Age\'] = 3\\n"",""    dataset.loc[ dataset[\'Age\'] > 64, \'Age\']\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""004568b6-dd9a-ff89-43d5-13d4e9370b1d""},""source"":[""We can not remove the AgeBand feature.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""875e55d4-51b0-5061-b72c-8a23946133a3""},""outputs"":[],""source"":[""train_df = train_df.drop([\'AgeBand\'], axis=1)\\n"",""combine = [train_df, test_df]\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""1c237b76-d7ac-098f-0156-480a838a64a9""},""source"":[""### Create new feature combining existing features\\n"",""\\n"",""We can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""7e6c04ed-cfaa-3139-4378-574fd095d6ba""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'FamilySize\'] = dataset[\'SibSp\'] + dataset[\'Parch\'] + 1\\n"",""\\n"",""train_df[[\'FamilySize\', \'Survived\']].groupby([\'FamilySize\'], as_index=False).mean().sort_values(by=\'Survived\', ascending=False)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""842188e6-acf8-2476-ccec-9e3451e4fa86""},""source"":[""We can create another feature called IsAlone.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""5c778c69-a9ae-1b6b-44fe-a0898d07be7a""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'IsAlone\'] = 0\\n"",""    dataset.loc[dataset[\'FamilySize\'] == 1, \'IsAlone\'] = 1\\n"",""\\n"",""train_df[[\'IsAlone\', \'Survived\']].groupby([\'IsAlone\'], as_index=False).mean()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""e6b87c09-e7b2-f098-5b04-4360080d26bc""},""source"":[""Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""74ee56a6-7357-f3bc-b605-6c41f8aa6566""},""outputs"":[],""source"":[""train_df = train_df.drop([\'Parch\', \'SibSp\', \'FamilySize\'], axis=1)\\n"",""test_df = test_df.drop([\'Parch\', \'SibSp\', \'FamilySize\'], axis=1)\\n"",""combine = [train_df, test_df]\\n"",""\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""f890b730-b1fe-919e-fb07-352fbd7edd44""},""source"":[""We can also create an artificial feature combining Pclass and Age.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""305402aa-1ea1-c245-c367-056eef8fe453""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'Age*Class\'] = dataset.Age * dataset.Pclass\\n"",""\\n"",""train_df.loc[:, [\'Age*Class\', \'Age\', \'Pclass\']].head(10)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""13292c1b-020d-d9aa-525c-941331bb996a""},""source"":[""### Completing a categorical feature\\n"",""\\n"",""Embarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""bf351113-9b7f-ef56-7211-e8dd00665b18""},""outputs"":[],""source"":[""freq_port = train_df.Embarked.dropna().mode()[0]\\n"",""freq_port""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""51c21fcc-f066-cd80-18c8-3d140be6cbae""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'Embarked\'] = dataset[\'Embarked\'].fillna(freq_port)\\n"",""    \\n"",""train_df[[\'Embarked\', \'Survived\']].groupby([\'Embarked\'], as_index=False).mean().sort_values(by=\'Survived\', ascending=False)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""f6acf7b2-0db3-e583-de50-7e14b495de34""},""source"":[""### Converting categorical feature to numeric\\n"",""\\n"",""We can now convert the EmbarkedFill feature by creating a new numeric Port feature.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""89a91d76-2cc0-9bbb-c5c5-3c9ecae33c66""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset[\'Embarked\'] = dataset[\'Embarked\'].map( {\'S\': 0, \'C\': 1, \'Q\': 2} ).astype(int)\\n"",""\\n"",""train_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""e3dfc817-e1c1-a274-a111-62c1c814cecf""},""source"":[""### Quick completing and converting a numeric feature\\n"",""\\n"",""We can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\\n"",""\\n"",""Note that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\\n"",""\\n"",""We may also want round off the fare to two decimals as it represents currency.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""3600cb86-cf5f-d87b-1b33-638dc8db1564""},""outputs"":[],""source"":[""test_df[\'Fare\'].fillna(test_df[\'Fare\'].dropna().median(), inplace=True)\\n"",""test_df.head()""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""4b816bc7-d1fb-c02b-ed1d-ee34b819497d""},""source"":[""We can not create FareBand.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""0e9018b1-ced5-9999-8ce1-258a0952cbf2""},""outputs"":[],""source"":[""train_df[\'FareBand\'] = pd.qcut(train_df[\'Fare\'], 4)\\n"",""train_df[[\'FareBand\', \'Survived\']].groupby([\'FareBand\'], as_index=False).mean().sort_values(by=\'FareBand\', ascending=True)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""d65901a5-3684-6869-e904-5f1a7cce8a6d""},""source"":[""Convert the Fare feature to ordinal values based on the FareBand.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""385f217a-4e00-76dc-1570-1de4eec0c29c""},""outputs"":[],""source"":[""for dataset in combine:\\n"",""    dataset.loc[ dataset[\'Fare\'] <= 7.91, \'Fare\'] = 0\\n"",""    dataset.loc[(dataset[\'Fare\'] > 7.91) & (dataset[\'Fare\'] <= 14.454), \'Fare\'] = 1\\n"",""    dataset.loc[(dataset[\'Fare\'] > 14.454) & (dataset[\'Fare\'] <= 31), \'Fare\']   = 2\\n"",""    dataset.loc[ dataset[\'Fare\'] > 31, \'Fare\'] = 3\\n"",""    dataset[\'Fare\'] = dataset[\'Fare\'].astype(int)\\n"",""\\n"",""train_df = train_df.drop([\'FareBand\'], axis=1)\\n"",""combine = [train_df, test_df]\\n"",""    \\n"",""train_df.head(10)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""27272bb9-3c64-4f9a-4a3b-54f02e1c8289""},""source"":[""And the test dataset.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""d2334d33-4fe5-964d-beac-6aa620066e15""},""outputs"":[],""source"":[""test_df.head(10)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""69783c08-c8cc-a6ca-2a9a-5e75581c6d31""},""source"":[""## Model, predict and solve\\n"",""\\n"",""Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\\n"",""\\n"",""- Logistic Regression\\n"",""- KNN or k-Nearest Neighbors\\n"",""- Support Vector Machines\\n"",""- Naive Bayes classifier\\n"",""- Decision Tree\\n"",""- Random Forrest\\n"",""- Perceptron\\n"",""- Artificial neural network\\n"",""- RVM or Relevance Vector Machine""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""0acf54f9-6cf5-24b5-72d9-29b30052823a""},""outputs"":[],""source"":[""X_train = train_df.drop(\\""Survived\\"", axis=1)\\n"",""Y_train = train_df[\\""Survived\\""]\\n"",""X_test  = test_df.drop(\\""PassengerId\\"", axis=1).copy()\\n"",""X_train.shape, Y_train.shape, X_test.shape""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""579bc004-926a-bcfe-e9bb-c8df83356876""},""source"":[""Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\\n"",""\\n"",""Note the confidence score generated by the model based on our training dataset.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""0edd9322-db0b-9c37-172d-a3a4f8dec229""},""outputs"":[],""source"":[""# Logistic Regression\\n"",""\\n"",""logreg = LogisticRegression()\\n"",""logreg.fit(X_train, Y_train)\\n"",""Y_pred = logreg.predict(X_test)\\n"",""acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\\n"",""acc_log""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""3af439ae-1f04-9236-cdc2-ec8170a0d4ee""},""source"":[""We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\\n"",""\\n"",""Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\\n"",""\\n"",""- Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\\n"",""- Inversely as Pclass increases, probability of Survived=1 decreases the most.\\n"",""- This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\\n"",""- So is Title as second highest positive correlation.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""e545d5aa-4767-7a41-5799-a4c5e529ce72""},""outputs"":[],""source"":[""coeff_df = pd.DataFrame(train_df.columns.delete(0))\\n"",""coeff_df.columns = [\'Feature\']\\n"",""coeff_df[\\""Correlation\\""] = pd.Series(logreg.coef_[0])\\n"",""\\n"",""coeff_df.sort_values(by=\'Correlation\', ascending=False)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""ac041064-1693-8584-156b-66674117e4d0""},""source"":[""Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).\\n"",""\\n"",""Note that the model generates a confidence score which is higher than Logistics Regression model.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""7a63bf04-a410-9c81-5310-bdef7963298f""},""outputs"":[],""source"":[""# Support Vector Machines\\n"",""\\n"",""svc = SVC()\\n"",""svc.fit(X_train, Y_train)\\n"",""Y_pred = svc.predict(X_test)\\n"",""acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\n"",""acc_svc""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""172a6286-d495-5ac4-1a9c-5b77b74ca6d2""},""source"":[""In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n"",""\\n"",""KNN confidence score is better than Logistics Regression but worse than SVM.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""ca14ae53-f05e-eb73-201c-064d7c3ed610""},""outputs"":[],""source"":[""knn = KNeighborsClassifier(n_neighbors = 3)\\n"",""knn.fit(X_train, Y_train)\\n"",""Y_pred = knn.predict(X_test)\\n"",""acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\n"",""acc_knn""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""810f723d-2313-8dfd-e3e2-26673b9caa90""},""source"":[""In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n"",""\\n"",""The model generated confidence score is the lowest among the models evaluated so far.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""50378071-7043-ed8d-a782-70c947520dae""},""outputs"":[],""source"":[""# Gaussian Naive Bayes\\n"",""\\n"",""gaussian = GaussianNB()\\n"",""gaussian.fit(X_train, Y_train)\\n"",""Y_pred = gaussian.predict(X_test)\\n"",""acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\n"",""acc_gaussian""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""1e286e19-b714-385a-fcfa-8cf5ec19956a""},""source"":[""The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d""},""outputs"":[],""source"":[""# Perceptron\\n"",""\\n"",""perceptron = Perceptron()\\n"",""perceptron.fit(X_train, Y_train)\\n"",""Y_pred = perceptron.predict(X_test)\\n"",""acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\n"",""acc_perceptron""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""a4d56857-9432-55bb-14c0-52ebeb64d198""},""outputs"":[],""source"":[""# Linear SVC\\n"",""\\n"",""linear_svc = LinearSVC()\\n"",""linear_svc.fit(X_train, Y_train)\\n"",""Y_pred = linear_svc.predict(X_test)\\n"",""acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\n"",""acc_linear_svc""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""dc98ed72-3aeb-861f-804d-b6e3d178bf4b""},""outputs"":[],""source"":[""# Stochastic Gradient Descent\\n"",""\\n"",""sgd = SGDClassifier()\\n"",""sgd.fit(X_train, Y_train)\\n"",""Y_pred = sgd.predict(X_test)\\n"",""acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\n"",""acc_sgd""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7""},""source"":[""This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n"",""\\n"",""The model confidence score is the highest among models evaluated so far.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""dd85f2b7-ace2-0306-b4ec-79c68cd3fea0""},""outputs"":[],""source"":[""# Decision Tree\\n"",""\\n"",""decision_tree = DecisionTreeClassifier()\\n"",""decision_tree.fit(X_train, Y_train)\\n"",""Y_pred = decision_tree.predict(X_test)\\n"",""acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\n"",""acc_decision_tree""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""85693668-0cd5-4319-7768-eddb62d2b7d0""},""source"":[""The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n"",""\\n"",""The model confidence score is the highest among models evaluated so far. We decide to use this model\'s output (Y_pred) for creating our competition submission of results.""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""f0694a8e-b618-8ed9-6f0d-8c6fba2c4567""},""outputs"":[],""source"":[""# Random Forest\\n"",""\\n"",""random_forest = RandomForestClassifier(n_estimators=100)\\n"",""random_forest.fit(X_train, Y_train)\\n"",""Y_pred = random_forest.predict(X_test)\\n"",""random_forest.score(X_train, Y_train)\\n"",""acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\n"",""acc_random_forest""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""f6c9eef8-83dd-581c-2d8e-ce932fe3a44d""},""source"":[""### Model evaluation\\n"",""\\n"",""We can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees\' habit of overfitting to their training set. ""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6""},""outputs"":[],""source"":[""models = pd.DataFrame({\\n"",""    \'Model\': [\'Support Vector Machines\', \'KNN\', \'Logistic Regression\', \\n"",""              \'Random Forest\', \'Naive Bayes\', \'Perceptron\', \\n"",""              \'Stochastic Gradient Decent\', \'Linear SVC\', \\n"",""              \'Decision Tree\'],\\n"",""    \'Score\': [acc_svc, acc_knn, acc_log, \\n"",""              acc_random_forest, acc_gaussian, acc_perceptron, \\n"",""              acc_sgd, acc_linear_svc, acc_decision_tree]})\\n"",""models.sort_values(by=\'Score\', ascending=False)""]},{""cell_type"":""code"",""execution_count"":null,""metadata"":{""_cell_guid"":""28854d36-051f-3ef0-5535-fa5ba6a9bef7""},""outputs"":[],""source"":[""submission = pd.DataFrame({\\n"",""        \\""PassengerId\\"": test_df[\\""PassengerId\\""],\\n"",""        \\""Survived\\"": Y_pred\\n"",""    })\\n"",""# submission.to_csv(\'../output/submission.csv\', index=False)""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""fcfc8d9f-e955-cf70-5843-1fb764c54699""},""source"":[""Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.""]},{""cell_type"":""markdown"",""metadata"":{""_cell_guid"":""aeec9210-f9d8-cd7c-c4cf-a87376d5f693""},""source"":[""## References\\n"",""\\n"",""This notebook has been created based on great work done solving the Titanic competition and other sources.\\n"",""\\n"",""- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n"",""- [Getting Started with Pandas: Kaggle\'s Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n"",""- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)""]}],""metadata"":{""_change_revision"":0,""_is_fork"":false,""kernelspec"":{""display_name"":""Python 3"",""language"":""python"",""name"":""python3""},""language_info"":{""codemirror_mode"":{""name"":""ipython"",""version"":3},""file_extension"":"".py"",""mimetype"":""text/x-python"",""name"":""python"",""nbconvert_exporter"":""python"",""pygments_lexer"":""ipython3"",""version"":""3.6.0""}},""nbformat"":4,""nbformat_minor"":0}']",1
varun0727_titanic-machine-learning-from-disaster.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.']",8
vincentlino_titanic-data-science-solutions-d3a853.html,"['# data analysis and wrangling', '# visualization', '# machine learning', '# preview the data', '# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.', '# Review Parch distribution using `percentiles=[.75, .8]`', '# SibSp distribution `[.68, .69]`', '# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')"", ""# grid = sns.FacetGrid(train_df, col='Embarked')"", ""# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})"", '# fill age na with median group by title', ""# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')"", '        # age_mean = guess_df.mean()', '        # age_std = guess_df.std()', '        # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)', '        # Convert random age float to nearest .5 age', '# Logistic Regression', '# Support Vector Machines', '# Perceptron', '# Linear SVC', '# Stochastic Gradient Descent', '# Decision Tree', '# Gridsearch and tuning an randomforrest algorithm', '# Choose the type of classifier. ', '# Choose some parameter combinations to try', '# Type of scoring used to compare parameter combinations', '# Run the grid search', '# Set the clf to the best combination of parameters', '# Fit the best algorithm to the data. ', '# Random Forest', '# Xgboost', ""# submission.to_csv('../output/submission.csv', index=False)""]",33
vinicius150987_titanic.html,"['# Replace columns', '# Family', '# Class', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set', '# Predicting Test Set']",13
vitorishi_titanic-a-beginner-approach-with-neural-networks.html,"[' # Mapping Age', '# Mapping titles', '# Mapping Embarked', '# Mapping Fare', '# ']",5
vivekkr970_titanic-survival-prediction.html,"['# below is distribution of embarked according to fare and sex', '# we replace missing age by mean age of pessenger who belong to some group of class/sex/family', '# we replace the missing value of fare for test dataset using interpolation', '# in case of Pclass and FamilySize their order has some meaning so we will not create dummies for them', '# in case of Pclass and FamilySize their order has some meaning so we will not create dummies for them', '# environment settings: ', '# create a list of features column', '# create x and y for training and test data', '# create a NumPy array with the same shape as y_test', '# fill the array with the mean value of y_test', '# compute null RMSE', '# compute RMSE', '# compute error between predicted data and true responce and display it in confusion matrix', '# compute RMSE', '# compute error between predicted data and true responce and display it in confusion matrix', '# compute RMSE', '# compute error between predicted data and true responce and display it in confusion matrix', '# compute RMSE', '# compute error between predicted data and true responce and display it in confusion matrix']",19
vloden_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'import matplotlib.pyplot as plt # plotting', '# Input data files are available in the ""../input/"" directory.', '# Any results you write to the current directory are saved as output.', '# Class affects survival chances ', '# Sex affects survival chances', '# Age slightly affects survival chances', ""# SibSp doesn't affect survival chances"", '# Parch slightly affects survival chances', '# Fare affects survival chances', '# Embarkation slightly affects survival chances']",15
waiyankhinemyo_titanic-logistics-regression-knn-decision-tree.html,"[' #   Column       Non-Null Count  Dtype  ', ' #   Column       Non-Null Count  Dtype  ', ' #   Column       Non-Null Count  Dtype  ']",3
wildwatermelon_titanic-tutorial.html,"['# Tutorial from https://towardsdatascience.com/your-first-kaggle-competition-submission-64da366e48cb', '# print train data', '# test_data.head(10)', '# In this column, there are plenty of NAs. To deal with it, we are going to replace NAs with â€˜Sâ€™ because it is the most occurred value.', '# When you cut with qcut, the bins will be chosen so that you have the same number of records in each bin (equal parts).', '# Age has some missing values. ', '# We will fill it with random numbers between (average age minus average standard deviation) and (average age plus average standard deviation).', '# After that, we will group it in the set of 5', '# print(X_train.head(10))', '# print(test_data.head(10))']",10
winnievorihilala_titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", ' #   Column       Non-Null Count  Dtype  ', '# Visualiation des variables catÃ©gorielles et quantitatives ', '# variables quantitatives', '# Variables categorielles', ' #   Column       Non-Null Count  Dtype  ', ""# Examen d'une des variables catÃ©gorielles"", '# Analyse statistique selon la variable catÃ©gorielle', '# Voir le nombre de personnes par classe', '# Code Tutoriel Kaggle ', ""# ? : pourquoi la somme des pourcentage n'est pas Ã©gale Ã\xa0 100 ?"", '# Proportion de femmes qui ont survÃ©cu', ""# Proportion d'hommes qui ont survÃ©cu"", ""# Examen d'une des variables quantitatives"", '# Analyse statistique selon la variable quantitative', '# Proportion de personnes par classe sociale', '# Proportion de personnes (survivantes et non-survivantes) par classe sociale', ""# Examen d'une des variables quantitatives"", '# Analyse statistique selon la variable quantitative', ""# Examen d'une des variables catÃ©gorielles"", '# Analyse statistique selon la variable catÃ©gorielle', '# Code tutoriel Kaggle: Random Forest', '# from sklearn.ensemble import RandomForestClassifier', 'model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1) # cf RandomForestClassifier dans sklearn', ""# n_estimators : nombre d'arbres dans la foret"", '# max_depth : profondeur maximale des arbres', '# random_state : ContrÃ´le Ã\xa0 la fois le caractÃ¨re alÃ©atoire du bootstrap des Ã©chantillons utilisÃ©s ', ""# lors de la construction d'arbres (si bootstrap=True) et l'Ã©chantillonnage des entitÃ©s Ã\xa0 prendre en compte "", '# lors de la recherche de la meilleure rÃ©partition sur chaque nÅ“ud (si )', '# DÃ©coupage du jeu de donnÃ©es en apprentissage et validation', ""# LDA sur donnÃ©es d'apprentissage et validation et calculs d'erreur de classification"", '# LDA', '# QDA', '# Recherche de la valeur optimale du paramÃ¨tre de rÃ©gularisaton C avec GridSearchCV', '# the grid', '# the classifier', ""clf_reg = linear_model.LogisticRegression(tol=1e-5, multi_class='multinomial', solver='lbfgs') # solveur par defaut"", '# Perf a K-fold validation using the accuracy as the performance measure', 'K = 10 # feel free to adapt the value of $K$', '# we will do it on a grid search using n_jobs processors', '# Calcul des erreurs de classification ', '# Get the best parameters', '# Recherche de la valeur optimale du paramÃ¨tre de rÃ©gularisation C avec GridSearchCV', '# the grid', '# the classifier', '# Perf a K-fold validation using the accuracy as the performance measure', 'K = 5 # feel free to adapt the value of $K$', '# we will dot it on a grid search using n_jobs processors', '# Calcul des erreurs de classification ', '# Get the best parameters', '# Recherche des valeurs optimales des paramÃ¨tres C et gamma avec GridSearchCV', '# the grid', '# the classifier', '# Perf a K-fold validation using the accuracy as the performance measure', 'K = 3 # feel free to adapt the value of $K$', '# we will do it on a grid search using n_jobs processors', '# Calcul des erreurs de classification', '# Get the best parameters', '# Kppv et GridSearchCV', '# the grid', '# the classifier', '# Perf a K-fold validation using the accuracy as the performance measure', 'K = 5 # feel free to adapt the value of $K$', '# we will dot it on a grid search using n_jobs processors', '# Calcul des erreurs de classification', '# Get the best parameters', '# PrÃ©diction sur les donnÃ©es de validation et matrice de confusion', '# matrices de confusion', '# accuracy: (tp + tn) / (p + n)', '# precision tp / (tp + fp)', '# recall: tp / (tp + fn)', '# f1: 2 tp / (2 tp + fp + fn)']",80
wonne2_titanic-wonne.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', 'import matplotlib.pyplot as plt # import matplotlib to draw graph', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '# Any results you write to the current directory are saved as output.', '# Scewnessë¥¼ ë” ì¤„ì´ê¸° ìœ„í•´ logë¥¼ ë‘ë²ˆì·¨í•´ì¤Œ.', '# histogram', '# í•„ìš”ì—†ëŠ” feature ì\xa0œê±°', '# íŒŒë¼ë¯¸í„° ì¡°í•©', '# Grid Searchë¥¼ ì´ìš©í•œ Hyper Parameter ì„\xa0ì\xa0•ê³¼ì\xa0•.', '# grid search', '    953         # Prepare validation data.', ""    750             check_batch_axis=False,  # Don't enforce the batch size."", '    399             # Python 3 and encoding', '    402             # Python 3 and no explicit encoding']",19
xaxetrov_ml-training-our-first-challenge-titanic.html,"['# Any results you write to the current directory are saved as output.', '# Load test & train data', '# Lets understand  the csv files', '# y histogram', '# Fare histogram -> check split', ""sns.distplot(X['Fare'], kde = False, ax=ax) # sns.distplot(X['Fare'], hist = False, ax=ax)"", ""sns.distplot(train_X['Fare'], kde = False, ax=ax) # sns.distplot(train_X['Fare'], hist = False, ax=ax)"", 'print(""Skewness: %f"" % X[\'Fare\'].skew())  #  third standardized moment', 'print(""Kurtosis: %f"" % X[\'Fare\'].kurt())  #  fourth standardized moment', '# Relation between class and survival']",10
xupanda_titanic-random-forest-82-78.html,[],0
yananhe_titanic-ml.html,"['# sorted(a)', '# sorted(a)']",2
yangsudlor_titanic.html,"['        # linear layer (7 -> hidden_1)', '        # linear layer (n_hidden -> hidden_2)', '        # linear layer (n_hidden -> 10)', '        # dropout layer (p=0.5)', '        # dropout prevents overfitting of data', '        # flatten image input', '        # add hidden layer, with relu activation function', '        # add dropout layer', '        # add hidden layer, with relu activation function', '        # add dropout layer', '        # add output layer', '    # initialize tracker for minimum validation loss', '        # initialize variables to monitor training and validation loss', '        # train the model #', '            # move to GPU', '        # validate the model #', '            # move to GPU', '        # calculate average losses', '        # print training/validation statistics ', '    # return trained model']",20
yasermarey_predicting-survival-of-titanic-passengers-with-dnn.html,"[' #   Column       Non-Null Count  Dtype  ', '# Embarked feature', '# construct model', '# ', '#', '# #']",6
yasserabdelsattar_titanic-predictions-using-g-boosting.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', ""# Change type of 'Sex' feature to numeric""]",9
yergou_titanic.html,"['import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Load in the train and test datasets', '    plt.subplots(figsize=(10, 8)) # è®¾ç½®ç”»é¢å¤§å°']",4
yosher_exploratory-analysis-on-titanic.html,"['# Premise', '# NaN detection', '# a. Looking further into the sex and embarked columns', '# b. Further investigation into the Fare column of x_test:', '# Further investigation into Age column with decimal points', '# Cleansing Process', '  # This is added back by InteractiveShellApp.init_path()', '#  KNN method']",8
youngwancha_notebook-cha-titanic.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load"", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only ""../input/"" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" ', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"", ' #   Column       Non-Null Count  Dtype  ']",10
yuanic_bax452-hw1-titanic.html,"['ax = sns.countplot(x=""Sex"", hue=""Survived"", data=train,palette=survival_color) # for Seaborn version 0.7 and more', 'ax = sns.countplot(x=""Pclass"", hue=""Survived"", data=train,palette=survival_color) # for Seaborn version 0.7 and more                                                  ', 'ax = sns.countplot(x=""Embarked"", hue=""Survived"", data=train,palette=survival_color) # for Seaborn version 0.7 and more', 'ax = sns.barplot(x=""Category"", y=""Survival Rate"", data=sr_sex , palette=[\'red\']) # for Seaborn version 0.7 and more', 'ax = sns.barplot(x=""Category"", y=""Survival Rate"", data=sr_pclass , palette=[\'red\']) # for Seaborn version 0.7 and more', 'ax = sns.barplot(x=""Category"", y=""Survival Rate"", data=sr_embarked , palette=[\'red\']) # for Seaborn version 0.7 and more', 'ax = sns.barplot(x=""Category"", y=""Survival Rate"", data=sr_age , palette=[\'red\']) # for Seaborn version 0.7 and more', '#ax = sns.barplot(x=""Category"", y=""Survival Rate"", data=sr_fare , palette=[\'red\']) # for Seaborn version 0.7 and more']",8
yuta12345_titanic-machine-learning-from-disaster.html,"['# This Python 3 environment comes with many helpful analytics libraries installed', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', ""# For example, here's several helpful packages to load in "", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)', '# Input data files are available in the ""../input/"" directory.', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', '# Any results you write to the current directory are saved as output.', '# Break off validation set from training data', '# Save test predictions to file']",10
zgh19971022_logisticregression-titanic.html,"[' #   Column       Non-Null Count  Dtype  ', ' #   Column       Non-Null Count  Dtype  ']",2
zzw922cn_a-journey-through-titanic.html,"['# Imports', '# pandas', '# numpy, matplotlib, seaborn', '# machine learning', '# get titanic & test csv files as a DataFrame', '# preview the data', ""# drop unnecessary columns, these columns won't be useful in analysis and prediction"", '# Embarked', '# only in titanic_df, fill the two missing values with the most occurred value, which is ""S"".', '# plot', ""# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)"", '# sns.factorplot(\'Survived\',hue=""Embarked"",data=titanic_df,kind=\'count\',order=[1,0],ax=axis2)', '# group by embarked, and get the mean for survived passengers for each value in Embarked', '# Either to consider Embarked column in predictions,', '# and remove ""S"" dummy variable, ', '# and leave ""C"" & ""Q"", since they seem to have a good rate for Survival.', ""# OR, don't create dummy variables for Embarked column, just drop it, "", ""# because logically, Embarked doesn't seem to be useful in prediction."", '# Fare', '# only for test_df, since there is a missing ""Fare"" values', '# convert from float to int', ""# get fare for survived & didn't survive passengers "", '# get average and std for fare of survived/not survived passengers', '# plot', '# Age ', ""# axis3.set_title('Original Age values - Test')"", ""# axis4.set_title('New Age values - Test')"", '# get average, std, and number of NaN values in titanic_df', '# get average, std, and number of NaN values in test_df', '# generate random numbers between (mean - std) & (mean + std)', '# plot original Age values', '# NOTE: drop all null values, and convert to int', ""# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)"", '# fill NaN values in Age column with random values generated', '# convert from float to int', '# plot new Age Values', ""# test_df['Age'].hist(bins=70, ax=axis4)"", '# .... continue with plot Age column', '# peaks for survived/not survived passengers by their age', '# average survived passengers by age', '# Cabin', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction"", '# Family', '# Instead of having two columns Parch & SibSp, ', '# we can have only one column represent if the passenger had any family member aboard or not,', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.', '# drop Parch & SibSp', '# plot', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)"", ""# average of survived for those who had/didn't have any family member"", '# Sex', '# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.', '# So, we can classify passengers as males, females, and child', '# No need to use Sex column since we created Person column', '# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers', ""# sns.factorplot('Person',data=titanic_df,kind='count',ax=axis1)"", '# average of survived for each Person(male, female, or child)', '# Pclass', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])"", '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers', '# define training and testing sets', '# Logistic Regression', '# Support Vector Machines', '# svc = SVC()', '# svc.fit(X_train, Y_train)', '# Y_pred = svc.predict(X_test)', '# svc.score(X_train, Y_train)', '# Random Forests', '# knn = KNeighborsClassifier(n_neighbors = 3)', '# knn.fit(X_train, Y_train)', '# Y_pred = knn.predict(X_test)', '# knn.score(X_train, Y_train)', '# Gaussian Naive Bayes', '# gaussian = GaussianNB()', '# gaussian.fit(X_train, Y_train)', '# Y_pred = gaussian.predict(X_test)', '# gaussian.score(X_train, Y_train)', '# get Correlation Coefficient for each feature using Logistic Regression', '# preview']",79